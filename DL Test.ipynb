{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb3228b3-0083-48fe-b21c-7fad118139c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26c53775-fc7f-4133-8ee4-20e7612fcf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "investments = pd.read_csv('syntheticDataGenerators/investment/invest_data.csv', sep=';')\n",
    "baskets = pd.read_csv('data/company_basket.csv', sep=';')\n",
    "\n",
    "# Get all unique users and baskets\n",
    "unique_users = investments['user_id'].unique()\n",
    "unique_baskets = baskets['basket_name'].unique()\n",
    "\n",
    "# Create an empty dataframe with all possible user-basket combinations\n",
    "all_combinations = []\n",
    "for user in unique_users:\n",
    "    for basket in unique_baskets:\n",
    "        all_combinations.append({'user_id': user, 'basket_name': basket})\n",
    "\n",
    "# Convert to DataFrame\n",
    "complete_matrix = pd.DataFrame(all_combinations)\n",
    "\n",
    "# Create a set of (user_id, basket_name) tuples for quick lookup\n",
    "invested_pairs = set(zip(investments['user_id'], investments['basket_name']))\n",
    "\n",
    "# Add binary rating column (1 if user invested in basket, 0 otherwise)\n",
    "complete_matrix['binary_rating'] = complete_matrix.apply(\n",
    "    lambda row: 1 if (row['user_id'], row['basket_name']) in invested_pairs else 0, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Encode user and basket IDs to integers\n",
    "user2idx = {user_id: idx for idx, user_id in enumerate(complete_matrix['user_id'].unique())}\n",
    "basket2idx = {basket: idx for idx, basket in enumerate(complete_matrix['basket_name'].unique())}\n",
    "\n",
    "complete_matrix['user_idx'] = complete_matrix['user_id'].map(user2idx)\n",
    "complete_matrix['basket_idx'] = complete_matrix['basket_name'].map(basket2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2c62a9-f503-4dab-8e4e-340e02832998",
   "metadata": {},
   "source": [
    "# Single model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "033bf36f-9fbb-45f8-966c-e8c4027d6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size=32):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embedding_size*2, embedding_size*8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size*8, embedding_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size, embedding_size//4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_size//4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_vecs = self.user_embedding(user_ids)\n",
    "        item_vecs = self.item_embedding(item_ids)\n",
    "        combined = torch.cat([user_vecs, item_vecs], dim=1)\n",
    "        return self.fc(combined).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa057ab3-e327-4615-975b-0eb1f8aa73b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.0476, Val Loss = 0.0257\n",
      "Epoch 2: Train Loss = 0.0248, Val Loss = 0.0244\n",
      "Epoch 3: Train Loss = 0.0238, Val Loss = 0.0238\n",
      "Epoch 4: Train Loss = 0.0232, Val Loss = 0.0234\n",
      "Epoch 5: Train Loss = 0.0227, Val Loss = 0.0231\n",
      "Epoch 6: Train Loss = 0.0224, Val Loss = 0.0230\n",
      "Epoch 7: Train Loss = 0.0222, Val Loss = 0.0230\n",
      "Epoch 8: Train Loss = 0.0221, Val Loss = 0.0230\n",
      "Epoch 9: Train Loss = 0.0219, Val Loss = 0.0230\n",
      "Epoch 10: Train Loss = 0.0218, Val Loss = 0.0230\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "X = complete_matrix[['user_idx', 'basket_idx']].values\n",
    "y = complete_matrix['binary_rating'].values.astype(np.float32)\n",
    "\n",
    "# Create user and basket index mappings\n",
    "user2idx = {user: idx for idx, user in enumerate(complete_matrix['user_id'].unique())}\n",
    "basket2idx = {basket: idx for idx, basket in enumerate(complete_matrix['basket_name'].unique())}\n",
    "\n",
    "# Add index columns to your dataframe\n",
    "complete_matrix['user_idx'] = complete_matrix['user_id'].map(user2idx)\n",
    "complete_matrix['basket_idx'] = complete_matrix['basket_name'].map(basket2idx)\n",
    "\n",
    "# Get unique list of users and sort them\n",
    "unique_users = complete_matrix['user_id'].unique()\n",
    "sorted_users = np.sort(unique_users)\n",
    "\n",
    "# Calculate the split point (80% of users)\n",
    "split_idx = int(len(sorted_users) * 0.8)\n",
    "\n",
    "# Split users into train and test sets\n",
    "train_users_ids = sorted_users[:split_idx]\n",
    "test_users_ids = sorted_users[split_idx:]\n",
    "\n",
    "# Filter data by user groups\n",
    "train_data = complete_matrix[complete_matrix['user_id'].isin(train_users_ids)]\n",
    "test_data = complete_matrix[complete_matrix['user_id'].isin(test_users_ids)]\n",
    "\n",
    "# Prepare features and targets for PyTorch\n",
    "X_train = train_data[['user_idx', 'basket_idx']].values\n",
    "y_train = train_data['binary_rating'].values.astype(np.float32)\n",
    "\n",
    "X_test = test_data[['user_idx', 'basket_idx']].values\n",
    "y_test = test_data['binary_rating'].values.astype(np.float32)\n",
    "\n",
    "# Convert to tensors\n",
    "train_users = torch.tensor(X_train[:, 0])\n",
    "train_items = torch.tensor(X_train[:, 1])\n",
    "train_ratings = torch.tensor(y_train)\n",
    "\n",
    "test_users = torch.tensor(X_test[:, 0])\n",
    "test_items = torch.tensor(X_test[:, 1])\n",
    "test_ratings = torch.tensor(y_test)\n",
    "\n",
    "# Model\n",
    "model = RecommenderNet(num_users=len(user2idx), num_items=len(basket2idx))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "batch_size = 128  # or any other value you want\n",
    "\n",
    "train_dataset = TensorDataset(train_users, train_items, train_ratings)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_users, test_items, test_ratings)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_users, batch_items, batch_ratings in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_users, batch_items)\n",
    "        loss = criterion(outputs, batch_ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_train_loss = total_loss / num_batches\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_users, batch_items, batch_ratings in test_loader:\n",
    "            val_preds = model(batch_users, batch_items)\n",
    "            val_loss += criterion(val_preds, batch_ratings).item()\n",
    "            val_batches += 1\n",
    "    \n",
    "    avg_val_loss = val_loss / val_batches\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5aaf0cb1-a2cd-4c53-8e4c-641d3c0eda54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>basket_name</th>\n",
       "      <th>binary_rating</th>\n",
       "      <th>user_idx</th>\n",
       "      <th>basket_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>a123456789b123456789c123456789</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1001</td>\n",
       "      <td>Air related companies world</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1001</td>\n",
       "      <td>Aluminium</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001</td>\n",
       "      <td>Aussi Fin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>Aussi Fin extra</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157595</th>\n",
       "      <td>1800</td>\n",
       "      <td>World Software companies</td>\n",
       "      <td>0</td>\n",
       "      <td>799</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157596</th>\n",
       "      <td>1800</td>\n",
       "      <td>World Software companies I</td>\n",
       "      <td>0</td>\n",
       "      <td>799</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157597</th>\n",
       "      <td>1800</td>\n",
       "      <td>Worst investment</td>\n",
       "      <td>0</td>\n",
       "      <td>799</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157598</th>\n",
       "      <td>1800</td>\n",
       "      <td>WTF…</td>\n",
       "      <td>0</td>\n",
       "      <td>799</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157599</th>\n",
       "      <td>1800</td>\n",
       "      <td>WTF II</td>\n",
       "      <td>0</td>\n",
       "      <td>799</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157600 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id                     basket_name  binary_rating  user_idx  \\\n",
       "0          1001  a123456789b123456789c123456789              0         0   \n",
       "1          1001     Air related companies world              0         0   \n",
       "2          1001                      Aluminium               0         0   \n",
       "3          1001                       Aussi Fin              0         0   \n",
       "4          1001                 Aussi Fin extra              0         0   \n",
       "...         ...                             ...            ...       ...   \n",
       "157595     1800        World Software companies              0       799   \n",
       "157596     1800      World Software companies I              0       799   \n",
       "157597     1800                Worst investment              0       799   \n",
       "157598     1800                            WTF…              0       799   \n",
       "157599     1800                          WTF II              0       799   \n",
       "\n",
       "        basket_idx  \n",
       "0                0  \n",
       "1                1  \n",
       "2                2  \n",
       "3                3  \n",
       "4                4  \n",
       "...            ...  \n",
       "157595         192  \n",
       "157596         193  \n",
       "157597         194  \n",
       "157598         195  \n",
       "157599         196  \n",
       "\n",
       "[157600 rows x 5 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94037500-011c-4ce6-a211-1badaa557dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_for_users(user_ids, top_n=5):\n",
    "    model.eval()\n",
    "    all_baskets = list(basket2idx.keys())\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        user_idx = user2idx[user_id]\n",
    "        \n",
    "        already_invested = investments[investments['user_id'] == user_id]['basket_name'].unique()\n",
    "        unseen_baskets = [b for b in all_baskets if b not in already_invested]\n",
    "        basket_indices = torch.tensor([basket2idx[b] for b in unseen_baskets])\n",
    "        user_tensor = torch.tensor([user_idx] * len(basket_indices))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(user_tensor, basket_indices)\n",
    "        \n",
    "        top_indices = torch.topk(preds, top_n).indices\n",
    "        top_baskets = [(unseen_baskets[i], preds[i].item()) for i in top_indices]\n",
    "\n",
    "        print(f\"\\n🔍 Top-{top_n} Recommendations for User {user_id}:\")\n",
    "        for basket, score in top_baskets:\n",
    "            print(f\"→ {basket} (score: {score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76127221-9feb-46dc-bb07-f6c1d38302a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Top-5 Recommendations for User 1001:\n",
      "→ Healtcare Europe top valuers (score: 0.05)\n",
      "→ Nordic Green Energy (score: 0.03)\n",
      "→ Growth Rockets (score: 0.03)\n",
      "→ Global electric utilities (score: 0.03)\n",
      "→ Casino fun (score: 0.03)\n",
      "\n",
      "🔍 Top-5 Recommendations for User 1002:\n",
      "→ Growth Rockets (score: 0.14)\n",
      "→ Food producents Finland (score: 0.14)\n",
      "→ Technology stars of value (score: 0.13)\n",
      "→ Casino fun (score: 0.13)\n",
      "→ Nordic Green Energy (score: 0.12)\n",
      "\n",
      "🔍 Top-5 Recommendations for User 1003:\n",
      "→ Growth Rockets (score: 0.12)\n",
      "→ Well performed companies (score: 0.08)\n",
      "→ Healtcare Europe top valuers (score: 0.08)\n",
      "→ Financial East (score: 0.08)\n",
      "→ Swedish climbers (score: 0.07)\n",
      "\n",
      "🔍 Top-5 Recommendations for User 1004:\n",
      "→ Basic materials World (score: 0.19)\n",
      "→ Global utilities (score: 0.16)\n",
      "→ Oceania valuecreators (score: 0.12)\n",
      "→ Healtcare Europe top valuers (score: 0.12)\n",
      "→ Techs going upward (score: 0.11)\n",
      "\n",
      "🔍 Top-5 Recommendations for User 1005:\n",
      "→ Global utilities (score: 0.22)\n",
      "→ Growth Rockets (score: 0.22)\n",
      "→ Basic materials World (score: 0.20)\n",
      "→ Nordic Green Energy (score: 0.18)\n",
      "→ Food producents Sweden (score: 0.17)\n"
     ]
    }
   ],
   "source": [
    "random_user_ids = np.random.choice(list(user2idx.keys()), size=10, replace=False)\n",
    "recommend_for_users([1001, 1002, 1003, 1004, 1005])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "65b6060e-fc4a-4ef8-be20-5f11b9b6cf9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- EVALUATION RESULTS -----\n",
      "\n",
      "Metrics at k=2:\n",
      "Precision@2: 0.3850\n",
      "Recall@2: 0.1485\n",
      "F1@2: 0.2123\n",
      "\n",
      "Metrics at k=5:\n",
      "Precision@5: 0.2930\n",
      "Recall@5: 0.2796\n",
      "F1@5: 0.2820\n",
      "\n",
      "Metrics at k=10:\n",
      "Precision@10: 0.2320\n",
      "Recall@10: 0.4388\n",
      "F1@10: 0.2996\n"
     ]
    }
   ],
   "source": [
    "def evaluate_recommendations(user_ids=None, top_n_values=[2, 5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate the model using precision, recall, and F1 score\n",
    "    for multiple values of top_n recommendations\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_baskets = list(basket2idx.keys())\n",
    "    \n",
    "    # Use all test users if no specific users are provided\n",
    "    if user_ids is None:\n",
    "        user_ids = test_data['user_id'].unique()\n",
    "    \n",
    "    # Dictionaries to store metrics\n",
    "    precision_at_k = collections.defaultdict(list)\n",
    "    recall_at_k = collections.defaultdict(list)\n",
    "    f1_at_k = collections.defaultdict(list)\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        # Skip if user is not in test set\n",
    "        if user_id not in test_data['user_id'].unique():\n",
    "            continue\n",
    "            \n",
    "        # Get user index\n",
    "        user_idx = user2idx[user_id]\n",
    "        \n",
    "        # Get baskets from training set (to exclude)\n",
    "        if user_id in train_data['user_id'].unique():\n",
    "            train_baskets = train_data[train_data['user_id'] == user_id]['basket_name'].unique()\n",
    "        else:\n",
    "            train_baskets = []\n",
    "            \n",
    "        # Get ground truth baskets from test set\n",
    "        test_baskets = test_data[(test_data['user_id'] == user_id) & (test_data['binary_rating'] == 1)]['basket_name'].unique() \n",
    "        \n",
    "        # Skip if no test baskets\n",
    "        if len(test_baskets) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get unseen baskets (all baskets minus training baskets)\n",
    "        basket_indices = torch.tensor([basket2idx[b]for b in all_baskets])\n",
    "        user_tensor = torch.tensor([user_idx] * len(basket_indices))\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            preds = model(user_tensor, basket_indices)\n",
    "        \n",
    "        # Evaluate for different k values\n",
    "        for k in top_n_values:\n",
    "                \n",
    "            # Get top-k recommendations\n",
    "            top_indices = torch.topk(preds, k).indices.numpy()\n",
    "            recommended_baskets = [all_baskets[i] for i in top_indices]\n",
    "            \n",
    "            # Calculate true positives (recommendations that are in test set)\n",
    "            true_positives = len(set(recommended_baskets) & set(test_baskets))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = true_positives / k\n",
    "            recall = true_positives / len(test_baskets)\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "            \n",
    "            # Store metrics\n",
    "            precision_at_k[k].append(precision)\n",
    "            recall_at_k[k].append(recall)\n",
    "            f1_at_k[k].append(f1)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    print(\"\\n----- EVALUATION RESULTS -----\")\n",
    "    for k in top_n_values:\n",
    "        if not precision_at_k[k]:\n",
    "            print(f\"No data for k={k}\")\n",
    "            continue\n",
    "            \n",
    "        avg_precision = np.mean(precision_at_k[k])\n",
    "        avg_recall = np.mean(recall_at_k[k])\n",
    "        avg_f1 = np.mean(f1_at_k[k])\n",
    "        \n",
    "        print(f\"\\nMetrics at k={k}:\")\n",
    "        print(f\"Precision@{k}: {avg_precision:.4f}\")\n",
    "        print(f\"Recall@{k}: {avg_recall:.4f}\")\n",
    "        print(f\"F1@{k}: {avg_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'precision': precision_at_k,\n",
    "        'recall': recall_at_k,\n",
    "        'f1': f1_at_k\n",
    "    }\n",
    "\n",
    "# You can call this function to evaluate all test users\n",
    "metrics = evaluate_recommendations(top_n_values=[2, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71b1aa-3b02-4189-bb94-638678eb8432",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e52aaed-fe62-4903-b615-1f81d5ebe903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameter combinations to test: 32\n",
      "With 3-fold CV, total models to train: 96\n",
      "Estimated time: 48-192 minutes\n",
      "\n",
      "Starting grid search...\n",
      "==================================================\n",
      "Grid search started at: 2025-05-04 17:15:26\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "\n",
      "[17:15:26] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.81s, Avg Loss: 0.190380\n",
      "  Epoch 2/10 completed in 0.96s, Avg Loss: 0.072547\n",
      "  Epoch 3/10 completed in 0.78s, Avg Loss: 0.036061\n",
      "  Epoch 4/10 completed in 0.77s, Avg Loss: 0.028889\n",
      "  Epoch 5/10 completed in 0.79s, Avg Loss: 0.026887\n",
      "  Epoch 6/10 completed in 0.87s, Avg Loss: 0.026122\n",
      "  Epoch 7/10 completed in 0.77s, Avg Loss: 0.025627\n",
      "  Epoch 8/10 completed in 0.87s, Avg Loss: 0.025291\n",
      "  Epoch 9/10 completed in 0.87s, Avg Loss: 0.025063\n",
      "  Epoch 10/10 completed in 0.77s, Avg Loss: 0.024686\n",
      "[17:15:35] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.3s\n",
      "\n",
      "[17:15:35] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.77s, Avg Loss: 0.189684\n",
      "  Epoch 2/10 completed in 0.77s, Avg Loss: 0.062826\n",
      "  Epoch 3/10 completed in 0.86s, Avg Loss: 0.032930\n",
      "  Epoch 4/10 completed in 0.77s, Avg Loss: 0.028105\n",
      "  Epoch 5/10 completed in 0.77s, Avg Loss: 0.026830\n",
      "  Epoch 6/10 completed in 0.87s, Avg Loss: 0.026091\n",
      "  Epoch 7/10 completed in 0.86s, Avg Loss: 0.025807\n",
      "  Epoch 8/10 completed in 0.77s, Avg Loss: 0.025517\n",
      "  Epoch 9/10 completed in 0.76s, Avg Loss: 0.025328\n",
      "  Epoch 10/10 completed in 0.78s, Avg Loss: 0.025033\n",
      "[17:15:43] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   8.0s\n",
      "\n",
      "[17:15:43] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.87s, Avg Loss: 0.129043\n",
      "  Epoch 2/10 completed in 0.78s, Avg Loss: 0.045206\n",
      "  Epoch 3/10 completed in 0.88s, Avg Loss: 0.029397\n",
      "  Epoch 4/10 completed in 0.88s, Avg Loss: 0.026656\n",
      "  Epoch 5/10 completed in 0.77s, Avg Loss: 0.025633\n",
      "  Epoch 6/10 completed in 0.82s, Avg Loss: 0.025270\n",
      "  Epoch 7/10 completed in 0.77s, Avg Loss: 0.024879\n",
      "  Epoch 8/10 completed in 0.78s, Avg Loss: 0.024641\n",
      "  Epoch 9/10 completed in 0.88s, Avg Loss: 0.024321\n",
      "  Epoch 10/10 completed in 0.77s, Avg Loss: 0.024023\n",
      "[17:15:51] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.2s\n",
      "\n",
      "[17:15:51] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 1.00s, Avg Loss: 0.143668\n",
      "  Epoch 2/10 completed in 0.98s, Avg Loss: 0.029211\n",
      "  Epoch 3/10 completed in 0.88s, Avg Loss: 0.026212\n",
      "  Epoch 4/10 completed in 0.86s, Avg Loss: 0.025807\n",
      "  Epoch 5/10 completed in 0.90s, Avg Loss: 0.025520\n",
      "  Epoch 6/10 completed in 0.98s, Avg Loss: 0.025200\n",
      "  Epoch 7/10 completed in 0.88s, Avg Loss: 0.024853\n",
      "  Epoch 8/10 completed in 0.99s, Avg Loss: 0.024475\n",
      "  Epoch 9/10 completed in 0.98s, Avg Loss: 0.024132\n",
      "  Epoch 10/10 completed in 0.88s, Avg Loss: 0.023834\n",
      "[17:16:00] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.3s\n",
      "\n",
      "[17:16:00] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.89s, Avg Loss: 0.131445\n",
      "  Epoch 2/10 completed in 0.90s, Avg Loss: 0.028281\n",
      "  Epoch 3/10 completed in 0.90s, Avg Loss: 0.026012\n",
      "  Epoch 4/10 completed in 1.06s, Avg Loss: 0.025432\n",
      "  Epoch 5/10 completed in 0.90s, Avg Loss: 0.025056\n",
      "  Epoch 6/10 completed in 1.00s, Avg Loss: 0.024466\n",
      "  Epoch 7/10 completed in 0.99s, Avg Loss: 0.024104\n",
      "  Epoch 8/10 completed in 0.90s, Avg Loss: 0.023750\n",
      "  Epoch 9/10 completed in 0.90s, Avg Loss: 0.023578\n",
      "  Epoch 10/10 completed in 0.90s, Avg Loss: 0.023454\n",
      "[17:16:10] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   9.4s\n",
      "\n",
      "[17:16:10] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.01s, Avg Loss: 0.156545\n",
      "  Epoch 2/10 completed in 0.91s, Avg Loss: 0.029162\n",
      "  Epoch 3/10 completed in 1.01s, Avg Loss: 0.025908\n",
      "  Epoch 4/10 completed in 0.92s, Avg Loss: 0.025339\n",
      "  Epoch 5/10 completed in 1.00s, Avg Loss: 0.025225\n",
      "  Epoch 6/10 completed in 0.90s, Avg Loss: 0.025008\n",
      "  Epoch 7/10 completed in 0.89s, Avg Loss: 0.024570\n",
      "  Epoch 8/10 completed in 0.93s, Avg Loss: 0.024195\n",
      "  Epoch 9/10 completed in 0.98s, Avg Loss: 0.023919\n",
      "  Epoch 10/10 completed in 0.91s, Avg Loss: 0.023751\n",
      "[17:16:19] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.5s\n",
      "\n",
      "[17:16:19] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.87s, Avg Loss: 0.196050\n",
      "  Epoch 2/15 completed in 0.88s, Avg Loss: 0.065050\n",
      "  Epoch 3/15 completed in 0.77s, Avg Loss: 0.033245\n",
      "  Epoch 4/15 completed in 0.77s, Avg Loss: 0.027856\n",
      "  Epoch 5/15 completed in 0.77s, Avg Loss: 0.026362\n",
      "  Epoch 6/15 completed in 0.78s, Avg Loss: 0.025526\n",
      "  Epoch 7/15 completed in 0.87s, Avg Loss: 0.025140\n",
      "  Epoch 8/15 completed in 0.78s, Avg Loss: 0.024927\n",
      "  Epoch 9/15 completed in 0.87s, Avg Loss: 0.024543\n",
      "  Epoch 10/15 completed in 0.92s, Avg Loss: 0.024264\n",
      "  Epoch 11/15 completed in 0.78s, Avg Loss: 0.023843\n",
      "  Epoch 12/15 completed in 0.77s, Avg Loss: 0.023669\n",
      "  Epoch 13/15 completed in 0.77s, Avg Loss: 0.023368\n",
      "  Epoch 14/15 completed in 0.87s, Avg Loss: 0.023277\n",
      "  Epoch 15/15 completed in 0.77s, Avg Loss: 0.023004\n",
      "[17:16:31] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  12.3s\n",
      "\n",
      "[17:16:31] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.88s, Avg Loss: 0.151842\n",
      "  Epoch 2/15 completed in 0.86s, Avg Loss: 0.054204\n",
      "  Epoch 3/15 completed in 0.76s, Avg Loss: 0.031773\n",
      "  Epoch 4/15 completed in 0.76s, Avg Loss: 0.027443\n",
      "  Epoch 5/15 completed in 0.77s, Avg Loss: 0.026239\n",
      "  Epoch 6/15 completed in 0.82s, Avg Loss: 0.025759\n",
      "  Epoch 7/15 completed in 0.87s, Avg Loss: 0.025197\n",
      "  Epoch 8/15 completed in 0.76s, Avg Loss: 0.024912\n",
      "  Epoch 9/15 completed in 0.86s, Avg Loss: 0.024704\n",
      "  Epoch 10/15 completed in 0.86s, Avg Loss: 0.024224\n",
      "  Epoch 11/15 completed in 0.76s, Avg Loss: 0.024157\n",
      "  Epoch 12/15 completed in 0.76s, Avg Loss: 0.023833\n",
      "  Epoch 13/15 completed in 0.77s, Avg Loss: 0.023653\n",
      "  Epoch 14/15 completed in 0.86s, Avg Loss: 0.023540\n",
      "  Epoch 15/15 completed in 0.77s, Avg Loss: 0.023387\n",
      "[17:16:44] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  12.1s\n",
      "\n",
      "[17:16:44] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.91s, Avg Loss: 0.181354\n",
      "  Epoch 2/15 completed in 0.78s, Avg Loss: 0.061803\n",
      "  Epoch 3/15 completed in 0.86s, Avg Loss: 0.032893\n",
      "  Epoch 4/15 completed in 0.76s, Avg Loss: 0.027734\n",
      "  Epoch 5/15 completed in 0.76s, Avg Loss: 0.026277\n",
      "  Epoch 6/15 completed in 0.76s, Avg Loss: 0.025475\n",
      "  Epoch 7/15 completed in 0.86s, Avg Loss: 0.025051\n",
      "  Epoch 8/15 completed in 0.76s, Avg Loss: 0.024543\n",
      "  Epoch 9/15 completed in 0.86s, Avg Loss: 0.024211\n",
      "  Epoch 10/15 completed in 0.87s, Avg Loss: 0.023858\n",
      "  Epoch 11/15 completed in 0.76s, Avg Loss: 0.023589\n",
      "  Epoch 12/15 completed in 0.82s, Avg Loss: 0.023436\n",
      "  Epoch 13/15 completed in 0.76s, Avg Loss: 0.023204\n",
      "  Epoch 14/15 completed in 0.86s, Avg Loss: 0.022940\n",
      "  Epoch 15/15 completed in 0.77s, Avg Loss: 0.022760\n",
      "[17:16:56] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  12.2s\n",
      "\n",
      "[17:16:56] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.90s, Avg Loss: 0.114824\n",
      "  Epoch 2/15 completed in 1.01s, Avg Loss: 0.027347\n",
      "  Epoch 3/15 completed in 0.99s, Avg Loss: 0.025920\n",
      "  Epoch 4/15 completed in 0.90s, Avg Loss: 0.025492\n",
      "  Epoch 5/15 completed in 0.90s, Avg Loss: 0.025109\n",
      "  Epoch 6/15 completed in 0.90s, Avg Loss: 0.024599\n",
      "  Epoch 7/15 completed in 1.04s, Avg Loss: 0.024075\n",
      "  Epoch 8/15 completed in 0.90s, Avg Loss: 0.023669\n",
      "  Epoch 9/15 completed in 0.98s, Avg Loss: 0.023389\n",
      "  Epoch 10/15 completed in 1.00s, Avg Loss: 0.023164\n",
      "  Epoch 11/15 completed in 0.90s, Avg Loss: 0.022899\n",
      "  Epoch 12/15 completed in 0.89s, Avg Loss: 0.022753\n",
      "  Epoch 13/15 completed in 0.90s, Avg Loss: 0.022509\n",
      "  Epoch 14/15 completed in 0.90s, Avg Loss: 0.022427\n",
      "  Epoch 15/15 completed in 1.00s, Avg Loss: 0.022353\n",
      "[17:17:10] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  14.1s\n",
      "\n",
      "[17:17:10] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.95s, Avg Loss: 0.139178\n",
      "  Epoch 2/15 completed in 1.01s, Avg Loss: 0.027936\n",
      "  Epoch 3/15 completed in 1.00s, Avg Loss: 0.025926\n",
      "  Epoch 4/15 completed in 0.90s, Avg Loss: 0.025545\n",
      "  Epoch 5/15 completed in 0.90s, Avg Loss: 0.025321\n",
      "  Epoch 6/15 completed in 0.90s, Avg Loss: 0.024888\n",
      "  Epoch 7/15 completed in 1.00s, Avg Loss: 0.024509\n",
      "  Epoch 8/15 completed in 0.90s, Avg Loss: 0.024184\n",
      "  Epoch 9/15 completed in 1.04s, Avg Loss: 0.023967\n",
      "  Epoch 10/15 completed in 0.91s, Avg Loss: 0.023706\n",
      "  Epoch 11/15 completed in 0.99s, Avg Loss: 0.023500\n",
      "  Epoch 12/15 completed in 0.90s, Avg Loss: 0.023339\n",
      "  Epoch 13/15 completed in 0.89s, Avg Loss: 0.023095\n",
      "  Epoch 14/15 completed in 0.90s, Avg Loss: 0.022953\n",
      "  Epoch 15/15 completed in 0.98s, Avg Loss: 0.022794\n",
      "[17:17:24] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  14.2s\n",
      "\n",
      "[17:17:24] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.90s, Avg Loss: 0.135683\n",
      "  Epoch 2/15 completed in 1.01s, Avg Loss: 0.028881\n",
      "  Epoch 3/15 completed in 1.06s, Avg Loss: 0.025726\n",
      "  Epoch 4/15 completed in 0.90s, Avg Loss: 0.025080\n",
      "  Epoch 5/15 completed in 0.89s, Avg Loss: 0.024558\n",
      "  Epoch 6/15 completed in 0.90s, Avg Loss: 0.024217\n",
      "  Epoch 7/15 completed in 1.01s, Avg Loss: 0.023849\n",
      "  Epoch 8/15 completed in 0.91s, Avg Loss: 0.023617\n",
      "  Epoch 9/15 completed in 0.92s, Avg Loss: 0.023430\n",
      "  Epoch 10/15 completed in 1.01s, Avg Loss: 0.023194\n",
      "  Epoch 11/15 completed in 1.00s, Avg Loss: 0.022886\n",
      "  Epoch 12/15 completed in 0.89s, Avg Loss: 0.022724\n",
      "  Epoch 13/15 completed in 0.96s, Avg Loss: 0.022519\n",
      "  Epoch 14/15 completed in 0.91s, Avg Loss: 0.022347\n",
      "  Epoch 15/15 completed in 1.00s, Avg Loss: 0.022312\n",
      "[17:17:38] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  14.3s\n",
      "\n",
      "[17:17:38] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.92s, Avg Loss: 0.184381\n",
      "  Epoch 2/10 completed in 1.03s, Avg Loss: 0.053413\n",
      "  Epoch 3/10 completed in 1.02s, Avg Loss: 0.030188\n",
      "  Epoch 4/10 completed in 0.92s, Avg Loss: 0.026672\n",
      "  Epoch 5/10 completed in 0.92s, Avg Loss: 0.025648\n",
      "  Epoch 6/10 completed in 0.92s, Avg Loss: 0.024938\n",
      "  Epoch 7/10 completed in 0.97s, Avg Loss: 0.024472\n",
      "  Epoch 8/10 completed in 0.99s, Avg Loss: 0.023947\n",
      "  Epoch 9/10 completed in 0.93s, Avg Loss: 0.023507\n",
      "  Epoch 10/10 completed in 1.02s, Avg Loss: 0.023237\n",
      "[17:17:48] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.7s\n",
      "\n",
      "[17:17:48] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.98s, Avg Loss: 0.173168\n",
      "  Epoch 2/10 completed in 0.87s, Avg Loss: 0.062924\n",
      "  Epoch 3/10 completed in 0.87s, Avg Loss: 0.032919\n",
      "  Epoch 4/10 completed in 0.86s, Avg Loss: 0.027542\n",
      "  Epoch 5/10 completed in 0.97s, Avg Loss: 0.026035\n",
      "  Epoch 6/10 completed in 0.87s, Avg Loss: 0.025221\n",
      "  Epoch 7/10 completed in 1.03s, Avg Loss: 0.024861\n",
      "  Epoch 8/10 completed in 0.87s, Avg Loss: 0.024343\n",
      "  Epoch 9/10 completed in 0.97s, Avg Loss: 0.024000\n",
      "  Epoch 10/10 completed in 0.87s, Avg Loss: 0.023673\n",
      "[17:17:57] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   9.2s\n",
      "\n",
      "[17:17:57] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.89s, Avg Loss: 0.169509\n",
      "  Epoch 2/10 completed in 0.89s, Avg Loss: 0.053364\n",
      "  Epoch 3/10 completed in 0.99s, Avg Loss: 0.030601\n",
      "  Epoch 4/10 completed in 0.91s, Avg Loss: 0.026749\n",
      "  Epoch 5/10 completed in 1.01s, Avg Loss: 0.025625\n",
      "  Epoch 6/10 completed in 0.99s, Avg Loss: 0.024986\n",
      "  Epoch 7/10 completed in 0.88s, Avg Loss: 0.024367\n",
      "  Epoch 8/10 completed in 0.96s, Avg Loss: 0.023959\n",
      "  Epoch 9/10 completed in 0.91s, Avg Loss: 0.023565\n",
      "  Epoch 10/10 completed in 0.99s, Avg Loss: 0.023155\n",
      "[17:18:07] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.4s\n",
      "\n",
      "[17:18:07] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 1.02s, Avg Loss: 0.148155\n",
      "  Epoch 2/10 completed in 1.06s, Avg Loss: 0.029737\n",
      "  Epoch 3/10 completed in 1.16s, Avg Loss: 0.025665\n",
      "  Epoch 4/10 completed in 1.10s, Avg Loss: 0.024840\n",
      "  Epoch 5/10 completed in 1.06s, Avg Loss: 0.024253\n",
      "  Epoch 6/10 completed in 1.06s, Avg Loss: 0.023799\n",
      "  Epoch 7/10 completed in 0.99s, Avg Loss: 0.023484\n",
      "  Epoch 8/10 completed in 1.22s, Avg Loss: 0.023133\n",
      "  Epoch 9/10 completed in 1.05s, Avg Loss: 0.022920\n",
      "  Epoch 10/10 completed in 1.16s, Avg Loss: 0.022610\n",
      "[17:18:18] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  10.9s\n",
      "\n",
      "[17:18:18] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.09s, Avg Loss: 0.136217\n",
      "  Epoch 2/10 completed in 0.99s, Avg Loss: 0.029464\n",
      "  Epoch 3/10 completed in 0.99s, Avg Loss: 0.025890\n",
      "  Epoch 4/10 completed in 0.99s, Avg Loss: 0.025055\n",
      "  Epoch 5/10 completed in 0.99s, Avg Loss: 0.024330\n",
      "  Epoch 6/10 completed in 1.15s, Avg Loss: 0.023873\n",
      "  Epoch 7/10 completed in 0.99s, Avg Loss: 0.023565\n",
      "  Epoch 8/10 completed in 1.07s, Avg Loss: 0.023422\n",
      "  Epoch 9/10 completed in 1.09s, Avg Loss: 0.023234\n",
      "  Epoch 10/10 completed in 0.99s, Avg Loss: 0.022967\n",
      "[17:18:28] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  10.3s\n",
      "\n",
      "[17:18:28] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.02s, Avg Loss: 0.108451\n",
      "  Epoch 2/10 completed in 1.03s, Avg Loss: 0.026778\n",
      "  Epoch 3/10 completed in 1.13s, Avg Loss: 0.024787\n",
      "  Epoch 4/10 completed in 1.03s, Avg Loss: 0.023910\n",
      "  Epoch 5/10 completed in 1.19s, Avg Loss: 0.023490\n",
      "  Epoch 6/10 completed in 1.00s, Avg Loss: 0.022976\n",
      "  Epoch 7/10 completed in 1.13s, Avg Loss: 0.022677\n",
      "  Epoch 8/10 completed in 1.03s, Avg Loss: 0.022494\n",
      "  Epoch 9/10 completed in 1.03s, Avg Loss: 0.022304\n",
      "  Epoch 10/10 completed in 1.02s, Avg Loss: 0.022132\n",
      "[17:18:39] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  10.6s\n",
      "\n",
      "[17:18:39] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.97s, Avg Loss: 0.170261\n",
      "  Epoch 2/15 completed in 0.86s, Avg Loss: 0.052611\n",
      "  Epoch 3/15 completed in 0.97s, Avg Loss: 0.030251\n",
      "  Epoch 4/15 completed in 1.02s, Avg Loss: 0.026411\n",
      "  Epoch 5/15 completed in 0.86s, Avg Loss: 0.025164\n",
      "  Epoch 6/15 completed in 0.97s, Avg Loss: 0.024384\n",
      "  Epoch 7/15 completed in 0.86s, Avg Loss: 0.023993\n",
      "  Epoch 8/15 completed in 0.96s, Avg Loss: 0.023662\n",
      "  Epoch 9/15 completed in 0.87s, Avg Loss: 0.023295\n",
      "  Epoch 10/15 completed in 0.86s, Avg Loss: 0.023044\n",
      "  Epoch 11/15 completed in 0.97s, Avg Loss: 0.022928\n",
      "  Epoch 12/15 completed in 0.97s, Avg Loss: 0.022748\n",
      "  Epoch 13/15 completed in 0.86s, Avg Loss: 0.022559\n",
      "  Epoch 14/15 completed in 0.86s, Avg Loss: 0.022485\n",
      "  Epoch 15/15 completed in 0.86s, Avg Loss: 0.022394\n",
      "[17:18:52] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.7s\n",
      "\n",
      "[17:18:52] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.02s, Avg Loss: 0.155323\n",
      "  Epoch 2/15 completed in 0.87s, Avg Loss: 0.050738\n",
      "  Epoch 3/15 completed in 0.98s, Avg Loss: 0.030229\n",
      "  Epoch 4/15 completed in 0.97s, Avg Loss: 0.026763\n",
      "  Epoch 5/15 completed in 0.87s, Avg Loss: 0.025685\n",
      "  Epoch 6/15 completed in 0.87s, Avg Loss: 0.025024\n",
      "  Epoch 7/15 completed in 0.87s, Avg Loss: 0.024545\n",
      "  Epoch 8/15 completed in 0.87s, Avg Loss: 0.024242\n",
      "  Epoch 9/15 completed in 0.97s, Avg Loss: 0.023899\n",
      "  Epoch 10/15 completed in 0.93s, Avg Loss: 0.023620\n",
      "  Epoch 11/15 completed in 0.98s, Avg Loss: 0.023456\n",
      "  Epoch 12/15 completed in 0.97s, Avg Loss: 0.023123\n",
      "  Epoch 13/15 completed in 0.87s, Avg Loss: 0.022973\n",
      "  Epoch 14/15 completed in 0.87s, Avg Loss: 0.022747\n",
      "  Epoch 15/15 completed in 0.87s, Avg Loss: 0.022599\n",
      "[17:19:06] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  13.8s\n",
      "\n",
      "[17:19:06] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.98s, Avg Loss: 0.154299\n",
      "  Epoch 2/15 completed in 0.88s, Avg Loss: 0.053162\n",
      "  Epoch 3/15 completed in 0.98s, Avg Loss: 0.030554\n",
      "  Epoch 4/15 completed in 0.89s, Avg Loss: 0.026288\n",
      "  Epoch 5/15 completed in 0.98s, Avg Loss: 0.025030\n",
      "  Epoch 6/15 completed in 0.93s, Avg Loss: 0.024154\n",
      "  Epoch 7/15 completed in 0.86s, Avg Loss: 0.023712\n",
      "  Epoch 8/15 completed in 0.88s, Avg Loss: 0.023253\n",
      "  Epoch 9/15 completed in 0.98s, Avg Loss: 0.023053\n",
      "  Epoch 10/15 completed in 0.88s, Avg Loss: 0.022850\n",
      "  Epoch 11/15 completed in 0.98s, Avg Loss: 0.022590\n",
      "  Epoch 12/15 completed in 0.98s, Avg Loss: 0.022425\n",
      "  Epoch 13/15 completed in 0.88s, Avg Loss: 0.022334\n",
      "  Epoch 14/15 completed in 0.88s, Avg Loss: 0.022165\n",
      "  Epoch 15/15 completed in 0.88s, Avg Loss: 0.022146\n",
      "[17:19:20] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.9s\n",
      "\n",
      "[17:19:20] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 1.05s, Avg Loss: 0.162587\n",
      "  Epoch 2/15 completed in 1.05s, Avg Loss: 0.028805\n",
      "  Epoch 3/15 completed in 1.01s, Avg Loss: 0.025172\n",
      "  Epoch 4/15 completed in 1.10s, Avg Loss: 0.024365\n",
      "  Epoch 5/15 completed in 1.09s, Avg Loss: 0.023825\n",
      "  Epoch 6/15 completed in 0.99s, Avg Loss: 0.023348\n",
      "  Epoch 7/15 completed in 0.99s, Avg Loss: 0.023043\n",
      "  Epoch 8/15 completed in 0.99s, Avg Loss: 0.022797\n",
      "  Epoch 9/15 completed in 1.09s, Avg Loss: 0.022553\n",
      "  Epoch 10/15 completed in 0.99s, Avg Loss: 0.022405\n",
      "  Epoch 11/15 completed in 1.09s, Avg Loss: 0.022249\n",
      "  Epoch 12/15 completed in 1.15s, Avg Loss: 0.022252\n",
      "  Epoch 13/15 completed in 0.99s, Avg Loss: 0.022043\n",
      "  Epoch 14/15 completed in 0.99s, Avg Loss: 0.021919\n",
      "  Epoch 15/15 completed in 0.99s, Avg Loss: 0.021770\n",
      "[17:19:36] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  15.6s\n",
      "\n",
      "[17:19:36] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.98s, Avg Loss: 0.127412\n",
      "  Epoch 2/15 completed in 1.09s, Avg Loss: 0.027681\n",
      "  Epoch 3/15 completed in 0.98s, Avg Loss: 0.025282\n",
      "  Epoch 4/15 completed in 1.07s, Avg Loss: 0.024456\n",
      "  Epoch 5/15 completed in 1.09s, Avg Loss: 0.023941\n",
      "  Epoch 6/15 completed in 1.04s, Avg Loss: 0.023521\n",
      "  Epoch 7/15 completed in 0.98s, Avg Loss: 0.023349\n",
      "  Epoch 8/15 completed in 0.99s, Avg Loss: 0.023140\n",
      "  Epoch 9/15 completed in 1.09s, Avg Loss: 0.022939\n",
      "  Epoch 10/15 completed in 1.00s, Avg Loss: 0.022785\n",
      "  Epoch 11/15 completed in 1.09s, Avg Loss: 0.022598\n",
      "  Epoch 12/15 completed in 0.96s, Avg Loss: 0.022550\n",
      "  Epoch 13/15 completed in 1.08s, Avg Loss: 0.022348\n",
      "  Epoch 14/15 completed in 0.98s, Avg Loss: 0.022249\n",
      "  Epoch 15/15 completed in 0.98s, Avg Loss: 0.022188\n",
      "[17:19:51] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  15.4s\n",
      "\n",
      "[17:19:51] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.05s, Avg Loss: 0.149345\n",
      "  Epoch 2/15 completed in 1.10s, Avg Loss: 0.027858\n",
      "  Epoch 3/15 completed in 1.00s, Avg Loss: 0.024807\n",
      "  Epoch 4/15 completed in 1.09s, Avg Loss: 0.023948\n",
      "  Epoch 5/15 completed in 1.10s, Avg Loss: 0.023260\n",
      "  Epoch 6/15 completed in 1.02s, Avg Loss: 0.023017\n",
      "  Epoch 7/15 completed in 0.99s, Avg Loss: 0.022719\n",
      "  Epoch 8/15 completed in 1.00s, Avg Loss: 0.022397\n",
      "  Epoch 9/15 completed in 1.09s, Avg Loss: 0.022242\n",
      "  Epoch 10/15 completed in 1.05s, Avg Loss: 0.022046\n",
      "  Epoch 11/15 completed in 0.97s, Avg Loss: 0.021914\n",
      "  Epoch 12/15 completed in 1.09s, Avg Loss: 0.021743\n",
      "  Epoch 13/15 completed in 1.08s, Avg Loss: 0.021601\n",
      "  Epoch 14/15 completed in 0.98s, Avg Loss: 0.021560\n",
      "  Epoch 15/15 completed in 1.00s, Avg Loss: 0.021363\n",
      "[17:20:07] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.026, test=-0.027) total time=  15.6s\n",
      "\n",
      "[17:20:07] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.85s, Avg Loss: 0.130152\n",
      "  Epoch 2/10 completed in 0.93s, Avg Loss: 0.045758\n",
      "  Epoch 3/10 completed in 0.83s, Avg Loss: 0.030182\n",
      "  Epoch 4/10 completed in 0.99s, Avg Loss: 0.027091\n",
      "  Epoch 5/10 completed in 0.93s, Avg Loss: 0.026053\n",
      "  Epoch 6/10 completed in 0.83s, Avg Loss: 0.025465\n",
      "  Epoch 7/10 completed in 0.82s, Avg Loss: 0.025178\n",
      "  Epoch 8/10 completed in 0.83s, Avg Loss: 0.024861\n",
      "  Epoch 9/10 completed in 0.82s, Avg Loss: 0.024606\n",
      "  Epoch 10/10 completed in 0.93s, Avg Loss: 0.024346\n",
      "[17:20:16] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.8s\n",
      "\n",
      "[17:20:16] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.84s, Avg Loss: 0.203419\n",
      "  Epoch 2/10 completed in 0.93s, Avg Loss: 0.073300\n",
      "  Epoch 3/10 completed in 0.93s, Avg Loss: 0.036403\n",
      "  Epoch 4/10 completed in 0.88s, Avg Loss: 0.029403\n",
      "  Epoch 5/10 completed in 0.83s, Avg Loss: 0.027226\n",
      "  Epoch 6/10 completed in 0.83s, Avg Loss: 0.026241\n",
      "  Epoch 7/10 completed in 0.92s, Avg Loss: 0.025729\n",
      "  Epoch 8/10 completed in 0.83s, Avg Loss: 0.025293\n",
      "  Epoch 9/10 completed in 0.93s, Avg Loss: 0.024944\n",
      "  Epoch 10/10 completed in 0.93s, Avg Loss: 0.024611\n",
      "[17:20:24] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   8.9s\n",
      "\n",
      "[17:20:24] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.84s, Avg Loss: 0.224181\n",
      "  Epoch 2/10 completed in 0.83s, Avg Loss: 0.078492\n",
      "  Epoch 3/10 completed in 0.89s, Avg Loss: 0.036793\n",
      "  Epoch 4/10 completed in 0.84s, Avg Loss: 0.029275\n",
      "  Epoch 5/10 completed in 0.94s, Avg Loss: 0.027023\n",
      "  Epoch 6/10 completed in 0.84s, Avg Loss: 0.026126\n",
      "  Epoch 7/10 completed in 0.95s, Avg Loss: 0.025680\n",
      "  Epoch 8/10 completed in 0.94s, Avg Loss: 0.025375\n",
      "  Epoch 9/10 completed in 0.83s, Avg Loss: 0.025046\n",
      "  Epoch 10/10 completed in 0.84s, Avg Loss: 0.024784\n",
      "[17:20:33] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.8s\n",
      "\n",
      "[17:20:33] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 1.13s, Avg Loss: 0.111037\n",
      "  Epoch 2/10 completed in 1.07s, Avg Loss: 0.028190\n",
      "  Epoch 3/10 completed in 1.03s, Avg Loss: 0.026069\n",
      "  Epoch 4/10 completed in 1.03s, Avg Loss: 0.025659\n",
      "  Epoch 5/10 completed in 1.13s, Avg Loss: 0.025426\n",
      "  Epoch 6/10 completed in 1.03s, Avg Loss: 0.024973\n",
      "  Epoch 7/10 completed in 1.13s, Avg Loss: 0.024610\n",
      "  Epoch 8/10 completed in 1.03s, Avg Loss: 0.024311\n",
      "  Epoch 9/10 completed in 1.12s, Avg Loss: 0.024022\n",
      "  Epoch 10/10 completed in 1.07s, Avg Loss: 0.023847\n",
      "[17:20:44] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  10.8s\n",
      "\n",
      "[17:20:44] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.09s, Avg Loss: 0.170060\n",
      "  Epoch 2/10 completed in 1.00s, Avg Loss: 0.028795\n",
      "  Epoch 3/10 completed in 1.00s, Avg Loss: 0.026296\n",
      "  Epoch 4/10 completed in 1.00s, Avg Loss: 0.026021\n",
      "  Epoch 5/10 completed in 1.09s, Avg Loss: 0.025611\n",
      "  Epoch 6/10 completed in 1.00s, Avg Loss: 0.025383\n",
      "  Epoch 7/10 completed in 1.00s, Avg Loss: 0.025012\n",
      "  Epoch 8/10 completed in 1.16s, Avg Loss: 0.024727\n",
      "  Epoch 9/10 completed in 1.10s, Avg Loss: 0.024599\n",
      "  Epoch 10/10 completed in 0.99s, Avg Loss: 0.024361\n",
      "[17:20:54] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  10.4s\n",
      "\n",
      "[17:20:55] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.14s, Avg Loss: 0.116712\n",
      "  Epoch 2/10 completed in 1.03s, Avg Loss: 0.028399\n",
      "  Epoch 3/10 completed in 1.04s, Avg Loss: 0.025935\n",
      "  Epoch 4/10 completed in 1.03s, Avg Loss: 0.025410\n",
      "  Epoch 5/10 completed in 1.18s, Avg Loss: 0.025115\n",
      "  Epoch 6/10 completed in 1.03s, Avg Loss: 0.024790\n",
      "  Epoch 7/10 completed in 1.02s, Avg Loss: 0.024413\n",
      "  Epoch 8/10 completed in 1.15s, Avg Loss: 0.024007\n",
      "  Epoch 9/10 completed in 1.14s, Avg Loss: 0.023822\n",
      "  Epoch 10/10 completed in 1.04s, Avg Loss: 0.023574\n",
      "[17:21:05] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  10.8s\n",
      "\n",
      "[17:21:05] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.82s, Avg Loss: 0.188602\n",
      "  Epoch 2/15 completed in 0.82s, Avg Loss: 0.060456\n",
      "  Epoch 3/15 completed in 1.00s, Avg Loss: 0.032695\n",
      "  Epoch 4/15 completed in 0.85s, Avg Loss: 0.027855\n",
      "  Epoch 5/15 completed in 0.93s, Avg Loss: 0.026457\n",
      "  Epoch 6/15 completed in 0.94s, Avg Loss: 0.025766\n",
      "  Epoch 7/15 completed in 0.83s, Avg Loss: 0.025308\n",
      "  Epoch 8/15 completed in 0.84s, Avg Loss: 0.024939\n",
      "  Epoch 9/15 completed in 0.83s, Avg Loss: 0.024542\n",
      "  Epoch 10/15 completed in 0.84s, Avg Loss: 0.024203\n",
      "  Epoch 11/15 completed in 1.00s, Avg Loss: 0.023919\n",
      "  Epoch 12/15 completed in 0.84s, Avg Loss: 0.023737\n",
      "  Epoch 13/15 completed in 0.95s, Avg Loss: 0.023571\n",
      "  Epoch 14/15 completed in 0.94s, Avg Loss: 0.023357\n",
      "  Epoch 15/15 completed in 0.84s, Avg Loss: 0.023185\n",
      "[17:21:19] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.3s\n",
      "\n",
      "[17:21:19] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.84s, Avg Loss: 0.168576\n",
      "  Epoch 2/15 completed in 0.84s, Avg Loss: 0.057343\n",
      "  Epoch 3/15 completed in 0.93s, Avg Loss: 0.032178\n",
      "  Epoch 4/15 completed in 0.90s, Avg Loss: 0.027990\n",
      "  Epoch 5/15 completed in 0.94s, Avg Loss: 0.026690\n",
      "  Epoch 6/15 completed in 0.85s, Avg Loss: 0.025946\n",
      "  Epoch 7/15 completed in 0.94s, Avg Loss: 0.025679\n",
      "  Epoch 8/15 completed in 0.84s, Avg Loss: 0.025268\n",
      "  Epoch 9/15 completed in 0.84s, Avg Loss: 0.025045\n",
      "  Epoch 10/15 completed in 0.84s, Avg Loss: 0.024715\n",
      "  Epoch 11/15 completed in 0.94s, Avg Loss: 0.024508\n",
      "  Epoch 12/15 completed in 0.90s, Avg Loss: 0.024274\n",
      "  Epoch 13/15 completed in 0.95s, Avg Loss: 0.023977\n",
      "  Epoch 14/15 completed in 0.95s, Avg Loss: 0.023849\n",
      "  Epoch 15/15 completed in 0.84s, Avg Loss: 0.023724\n",
      "[17:21:32] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  13.3s\n",
      "\n",
      "[17:21:32] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.84s, Avg Loss: 0.195874\n",
      "  Epoch 2/15 completed in 0.84s, Avg Loss: 0.069368\n",
      "  Epoch 3/15 completed in 0.94s, Avg Loss: 0.033911\n",
      "  Epoch 4/15 completed in 0.89s, Avg Loss: 0.027983\n",
      "  Epoch 5/15 completed in 0.85s, Avg Loss: 0.026312\n",
      "  Epoch 6/15 completed in 0.96s, Avg Loss: 0.025540\n",
      "  Epoch 7/15 completed in 0.95s, Avg Loss: 0.025115\n",
      "  Epoch 8/15 completed in 0.84s, Avg Loss: 0.024767\n",
      "  Epoch 9/15 completed in 0.85s, Avg Loss: 0.024591\n",
      "  Epoch 10/15 completed in 0.84s, Avg Loss: 0.024285\n",
      "  Epoch 11/15 completed in 0.95s, Avg Loss: 0.024090\n",
      "  Epoch 12/15 completed in 0.91s, Avg Loss: 0.024072\n",
      "  Epoch 13/15 completed in 0.95s, Avg Loss: 0.023829\n",
      "  Epoch 14/15 completed in 0.95s, Avg Loss: 0.023781\n",
      "  Epoch 15/15 completed in 0.84s, Avg Loss: 0.023710\n",
      "[17:21:45] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.4s\n",
      "\n",
      "[17:21:45] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 1.15s, Avg Loss: 0.134040\n",
      "  Epoch 2/15 completed in 1.04s, Avg Loss: 0.028784\n",
      "  Epoch 3/15 completed in 1.00s, Avg Loss: 0.025955\n",
      "  Epoch 4/15 completed in 1.10s, Avg Loss: 0.025429\n",
      "  Epoch 5/15 completed in 1.04s, Avg Loss: 0.025130\n",
      "  Epoch 6/15 completed in 1.15s, Avg Loss: 0.024917\n",
      "  Epoch 7/15 completed in 1.05s, Avg Loss: 0.024530\n",
      "  Epoch 8/15 completed in 1.15s, Avg Loss: 0.024201\n",
      "  Epoch 9/15 completed in 1.14s, Avg Loss: 0.023846\n",
      "  Epoch 10/15 completed in 1.09s, Avg Loss: 0.023575\n",
      "  Epoch 11/15 completed in 1.04s, Avg Loss: 0.023469\n",
      "  Epoch 12/15 completed in 1.04s, Avg Loss: 0.023123\n",
      "  Epoch 13/15 completed in 1.14s, Avg Loss: 0.022985\n",
      "  Epoch 14/15 completed in 1.05s, Avg Loss: 0.022769\n",
      "  Epoch 15/15 completed in 1.15s, Avg Loss: 0.022758\n",
      "[17:22:02] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  16.3s\n",
      "\n",
      "[17:22:02] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.09s, Avg Loss: 0.143204\n",
      "  Epoch 2/15 completed in 1.14s, Avg Loss: 0.028551\n",
      "  Epoch 3/15 completed in 1.03s, Avg Loss: 0.026342\n",
      "  Epoch 4/15 completed in 1.13s, Avg Loss: 0.025981\n",
      "  Epoch 5/15 completed in 1.03s, Avg Loss: 0.025546\n",
      "  Epoch 6/15 completed in 1.04s, Avg Loss: 0.025224\n",
      "  Epoch 7/15 completed in 1.03s, Avg Loss: 0.025040\n",
      "  Epoch 8/15 completed in 1.19s, Avg Loss: 0.024486\n",
      "  Epoch 9/15 completed in 1.04s, Avg Loss: 0.024293\n",
      "  Epoch 10/15 completed in 1.13s, Avg Loss: 0.023868\n",
      "  Epoch 11/15 completed in 1.13s, Avg Loss: 0.023567\n",
      "  Epoch 12/15 completed in 1.03s, Avg Loss: 0.023301\n",
      "  Epoch 13/15 completed in 1.02s, Avg Loss: 0.023133\n",
      "  Epoch 14/15 completed in 1.08s, Avg Loss: 0.023008\n",
      "  Epoch 15/15 completed in 1.13s, Avg Loss: 0.022843\n",
      "[17:22:18] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  16.3s\n",
      "\n",
      "[17:22:18] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.04s, Avg Loss: 0.168139\n",
      "  Epoch 2/15 completed in 1.00s, Avg Loss: 0.030612\n",
      "  Epoch 3/15 completed in 1.00s, Avg Loss: 0.025880\n",
      "  Epoch 4/15 completed in 1.00s, Avg Loss: 0.025343\n",
      "  Epoch 5/15 completed in 1.10s, Avg Loss: 0.024874\n",
      "  Epoch 6/15 completed in 1.06s, Avg Loss: 0.024605\n",
      "  Epoch 7/15 completed in 1.10s, Avg Loss: 0.024216\n",
      "  Epoch 8/15 completed in 1.10s, Avg Loss: 0.023912\n",
      "  Epoch 9/15 completed in 1.00s, Avg Loss: 0.023579\n",
      "  Epoch 10/15 completed in 1.00s, Avg Loss: 0.023348\n",
      "  Epoch 11/15 completed in 1.00s, Avg Loss: 0.023228\n",
      "  Epoch 12/15 completed in 1.15s, Avg Loss: 0.022955\n",
      "  Epoch 13/15 completed in 1.01s, Avg Loss: 0.022859\n",
      "  Epoch 14/15 completed in 1.10s, Avg Loss: 0.022671\n",
      "  Epoch 15/15 completed in 1.00s, Avg Loss: 0.022708\n",
      "[17:22:34] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  15.7s\n",
      "\n",
      "[17:22:34] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 1.02s, Avg Loss: 0.206976\n",
      "  Epoch 2/10 completed in 0.92s, Avg Loss: 0.072525\n",
      "  Epoch 3/10 completed in 0.98s, Avg Loss: 0.035179\n",
      "  Epoch 4/10 completed in 0.92s, Avg Loss: 0.028155\n",
      "  Epoch 5/10 completed in 1.02s, Avg Loss: 0.025914\n",
      "  Epoch 6/10 completed in 0.92s, Avg Loss: 0.024901\n",
      "  Epoch 7/10 completed in 1.02s, Avg Loss: 0.024359\n",
      "  Epoch 8/10 completed in 1.03s, Avg Loss: 0.023838\n",
      "  Epoch 9/10 completed in 0.98s, Avg Loss: 0.023575\n",
      "  Epoch 10/10 completed in 0.93s, Avg Loss: 0.023416\n",
      "[17:22:43] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.7s\n",
      "\n",
      "[17:22:44] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.93s, Avg Loss: 0.200217\n",
      "  Epoch 2/10 completed in 1.00s, Avg Loss: 0.062480\n",
      "  Epoch 3/10 completed in 0.93s, Avg Loss: 0.032692\n",
      "  Epoch 4/10 completed in 0.93s, Avg Loss: 0.027816\n",
      "  Epoch 5/10 completed in 1.08s, Avg Loss: 0.026208\n",
      "  Epoch 6/10 completed in 1.02s, Avg Loss: 0.025287\n",
      "  Epoch 7/10 completed in 0.93s, Avg Loss: 0.024696\n",
      "  Epoch 8/10 completed in 0.91s, Avg Loss: 0.024321\n",
      "  Epoch 9/10 completed in 0.92s, Avg Loss: 0.023929\n",
      "  Epoch 10/10 completed in 1.02s, Avg Loss: 0.023663\n",
      "[17:22:53] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   9.7s\n",
      "\n",
      "[17:22:53] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.00s, Avg Loss: 0.145301\n",
      "  Epoch 2/10 completed in 1.05s, Avg Loss: 0.045984\n",
      "  Epoch 3/10 completed in 1.00s, Avg Loss: 0.029195\n",
      "  Epoch 4/10 completed in 0.90s, Avg Loss: 0.026204\n",
      "  Epoch 5/10 completed in 0.95s, Avg Loss: 0.025140\n",
      "  Epoch 6/10 completed in 0.90s, Avg Loss: 0.024506\n",
      "  Epoch 7/10 completed in 0.90s, Avg Loss: 0.023881\n",
      "  Epoch 8/10 completed in 1.06s, Avg Loss: 0.023644\n",
      "  Epoch 9/10 completed in 0.91s, Avg Loss: 0.023237\n",
      "  Epoch 10/10 completed in 1.06s, Avg Loss: 0.022964\n",
      "[17:23:03] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.7s\n",
      "\n",
      "[17:23:03] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 1.26s, Avg Loss: 0.126380\n",
      "  Epoch 2/10 completed in 1.22s, Avg Loss: 0.028289\n",
      "  Epoch 3/10 completed in 1.21s, Avg Loss: 0.025658\n",
      "  Epoch 4/10 completed in 1.16s, Avg Loss: 0.024857\n",
      "  Epoch 5/10 completed in 1.16s, Avg Loss: 0.024216\n",
      "  Epoch 6/10 completed in 1.15s, Avg Loss: 0.023589\n",
      "  Epoch 7/10 completed in 1.23s, Avg Loss: 0.023298\n",
      "  Epoch 8/10 completed in 1.22s, Avg Loss: 0.023075\n",
      "  Epoch 9/10 completed in 1.27s, Avg Loss: 0.022823\n",
      "  Epoch 10/10 completed in 1.27s, Avg Loss: 0.022718\n",
      "[17:23:15] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  12.2s\n",
      "\n",
      "[17:23:15] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.13s, Avg Loss: 0.119650\n",
      "  Epoch 2/10 completed in 1.28s, Avg Loss: 0.027486\n",
      "  Epoch 3/10 completed in 1.12s, Avg Loss: 0.025603\n",
      "  Epoch 4/10 completed in 1.13s, Avg Loss: 0.024886\n",
      "  Epoch 5/10 completed in 1.13s, Avg Loss: 0.024305\n",
      "  Epoch 6/10 completed in 1.23s, Avg Loss: 0.023870\n",
      "  Epoch 7/10 completed in 1.19s, Avg Loss: 0.023504\n",
      "  Epoch 8/10 completed in 1.22s, Avg Loss: 0.023244\n",
      "  Epoch 9/10 completed in 1.22s, Avg Loss: 0.022898\n",
      "  Epoch 10/10 completed in 1.12s, Avg Loss: 0.022786\n",
      "[17:23:27] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  11.8s\n",
      "\n",
      "[17:23:27] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.23s, Avg Loss: 0.130336\n",
      "  Epoch 2/10 completed in 1.19s, Avg Loss: 0.027298\n",
      "  Epoch 3/10 completed in 1.14s, Avg Loss: 0.025125\n",
      "  Epoch 4/10 completed in 1.13s, Avg Loss: 0.024213\n",
      "  Epoch 5/10 completed in 1.13s, Avg Loss: 0.023718\n",
      "  Epoch 6/10 completed in 1.23s, Avg Loss: 0.023460\n",
      "  Epoch 7/10 completed in 1.19s, Avg Loss: 0.023134\n",
      "  Epoch 8/10 completed in 1.23s, Avg Loss: 0.022894\n",
      "  Epoch 9/10 completed in 1.24s, Avg Loss: 0.022633\n",
      "  Epoch 10/10 completed in 1.13s, Avg Loss: 0.022423\n",
      "[17:23:39] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  11.9s\n",
      "\n",
      "[17:23:39] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.93s, Avg Loss: 0.207751\n",
      "  Epoch 2/15 completed in 1.02s, Avg Loss: 0.064170\n",
      "  Epoch 3/15 completed in 1.05s, Avg Loss: 0.032269\n",
      "  Epoch 4/15 completed in 0.95s, Avg Loss: 0.026893\n",
      "  Epoch 5/15 completed in 1.05s, Avg Loss: 0.025239\n",
      "  Epoch 6/15 completed in 1.06s, Avg Loss: 0.024477\n",
      "  Epoch 7/15 completed in 1.00s, Avg Loss: 0.024091\n",
      "  Epoch 8/15 completed in 0.96s, Avg Loss: 0.023729\n",
      "  Epoch 9/15 completed in 0.94s, Avg Loss: 0.023291\n",
      "  Epoch 10/15 completed in 0.95s, Avg Loss: 0.023158\n",
      "  Epoch 11/15 completed in 1.05s, Avg Loss: 0.022974\n",
      "  Epoch 12/15 completed in 0.96s, Avg Loss: 0.022902\n",
      "  Epoch 13/15 completed in 1.12s, Avg Loss: 0.022685\n",
      "  Epoch 14/15 completed in 1.03s, Avg Loss: 0.022482\n",
      "  Epoch 15/15 completed in 0.96s, Avg Loss: 0.022613\n",
      "[17:23:54] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  15.0s\n",
      "\n",
      "[17:23:54] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.96s, Avg Loss: 0.135284\n",
      "  Epoch 2/15 completed in 0.97s, Avg Loss: 0.045733\n",
      "  Epoch 3/15 completed in 1.12s, Avg Loss: 0.029419\n",
      "  Epoch 4/15 completed in 0.93s, Avg Loss: 0.026507\n",
      "  Epoch 5/15 completed in 1.03s, Avg Loss: 0.025345\n",
      "  Epoch 6/15 completed in 0.96s, Avg Loss: 0.024720\n",
      "  Epoch 7/15 completed in 1.06s, Avg Loss: 0.024317\n",
      "  Epoch 8/15 completed in 1.01s, Avg Loss: 0.023926\n",
      "  Epoch 9/15 completed in 0.96s, Avg Loss: 0.023754\n",
      "  Epoch 10/15 completed in 0.95s, Avg Loss: 0.023455\n",
      "  Epoch 11/15 completed in 1.06s, Avg Loss: 0.023237\n",
      "  Epoch 12/15 completed in 0.92s, Avg Loss: 0.023187\n",
      "  Epoch 13/15 completed in 1.06s, Avg Loss: 0.022940\n",
      "  Epoch 14/15 completed in 1.12s, Avg Loss: 0.022806\n",
      "  Epoch 15/15 completed in 0.92s, Avg Loss: 0.022716\n",
      "[17:24:09] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  15.0s\n",
      "\n",
      "[17:24:09] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.95s, Avg Loss: 0.191529\n",
      "  Epoch 2/15 completed in 0.95s, Avg Loss: 0.062312\n",
      "  Epoch 3/15 completed in 0.94s, Avg Loss: 0.032693\n",
      "  Epoch 4/15 completed in 1.10s, Avg Loss: 0.027167\n",
      "  Epoch 5/15 completed in 0.94s, Avg Loss: 0.025580\n",
      "  Epoch 6/15 completed in 1.05s, Avg Loss: 0.024779\n",
      "  Epoch 7/15 completed in 1.05s, Avg Loss: 0.024341\n",
      "  Epoch 8/15 completed in 0.95s, Avg Loss: 0.023854\n",
      "  Epoch 9/15 completed in 0.98s, Avg Loss: 0.023497\n",
      "  Epoch 10/15 completed in 0.94s, Avg Loss: 0.023233\n",
      "  Epoch 11/15 completed in 1.04s, Avg Loss: 0.023032\n",
      "  Epoch 12/15 completed in 0.95s, Avg Loss: 0.022881\n",
      "  Epoch 13/15 completed in 1.05s, Avg Loss: 0.022696\n",
      "  Epoch 14/15 completed in 1.06s, Avg Loss: 0.022574\n",
      "  Epoch 15/15 completed in 0.99s, Avg Loss: 0.022363\n",
      "[17:24:24] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  14.9s\n",
      "\n",
      "[17:24:24] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 1.19s, Avg Loss: 0.122656\n",
      "  Epoch 2/15 completed in 1.09s, Avg Loss: 0.028571\n",
      "  Epoch 3/15 completed in 1.10s, Avg Loss: 0.025906\n",
      "  Epoch 4/15 completed in 1.16s, Avg Loss: 0.025222\n",
      "  Epoch 5/15 completed in 1.10s, Avg Loss: 0.024611\n",
      "  Epoch 6/15 completed in 1.20s, Avg Loss: 0.024125\n",
      "  Epoch 7/15 completed in 1.10s, Avg Loss: 0.023686\n",
      "  Epoch 8/15 completed in 1.26s, Avg Loss: 0.023240\n",
      "  Epoch 9/15 completed in 1.20s, Avg Loss: 0.022935\n",
      "  Epoch 10/15 completed in 1.10s, Avg Loss: 0.022709\n",
      "  Epoch 11/15 completed in 1.09s, Avg Loss: 0.022469\n",
      "  Epoch 12/15 completed in 1.10s, Avg Loss: 0.022297\n",
      "  Epoch 13/15 completed in 1.26s, Avg Loss: 0.022264\n",
      "  Epoch 14/15 completed in 1.10s, Avg Loss: 0.022167\n",
      "  Epoch 15/15 completed in 1.20s, Avg Loss: 0.021931\n",
      "[17:24:41] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  17.3s\n",
      "\n",
      "[17:24:41] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.12s, Avg Loss: 0.106627\n",
      "  Epoch 2/15 completed in 1.27s, Avg Loss: 0.027198\n",
      "  Epoch 3/15 completed in 1.12s, Avg Loss: 0.025648\n",
      "  Epoch 4/15 completed in 1.19s, Avg Loss: 0.024856\n",
      "  Epoch 5/15 completed in 1.11s, Avg Loss: 0.024363\n",
      "  Epoch 6/15 completed in 1.18s, Avg Loss: 0.023886\n",
      "  Epoch 7/15 completed in 1.11s, Avg Loss: 0.023503\n",
      "  Epoch 8/15 completed in 1.18s, Avg Loss: 0.023257\n",
      "  Epoch 9/15 completed in 1.12s, Avg Loss: 0.023100\n",
      "  Epoch 10/15 completed in 1.27s, Avg Loss: 0.022846\n",
      "  Epoch 11/15 completed in 1.21s, Avg Loss: 0.022663\n",
      "  Epoch 12/15 completed in 1.10s, Avg Loss: 0.022617\n",
      "  Epoch 13/15 completed in 1.11s, Avg Loss: 0.022454\n",
      "  Epoch 14/15 completed in 1.17s, Avg Loss: 0.022335\n",
      "  Epoch 15/15 completed in 1.22s, Avg Loss: 0.022185\n",
      "[17:24:59] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  17.5s\n",
      "\n",
      "[17:24:59] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.10s, Avg Loss: 0.150454\n",
      "  Epoch 2/15 completed in 1.11s, Avg Loss: 0.028107\n",
      "  Epoch 3/15 completed in 1.16s, Avg Loss: 0.025252\n",
      "  Epoch 4/15 completed in 1.10s, Avg Loss: 0.024736\n",
      "  Epoch 5/15 completed in 1.20s, Avg Loss: 0.024250\n",
      "  Epoch 6/15 completed in 1.10s, Avg Loss: 0.023682\n",
      "  Epoch 7/15 completed in 1.27s, Avg Loss: 0.023258\n",
      "  Epoch 8/15 completed in 1.20s, Avg Loss: 0.022930\n",
      "  Epoch 9/15 completed in 1.10s, Avg Loss: 0.022713\n",
      "  Epoch 10/15 completed in 1.10s, Avg Loss: 0.022506\n",
      "  Epoch 11/15 completed in 1.16s, Avg Loss: 0.022349\n",
      "  Epoch 12/15 completed in 1.21s, Avg Loss: 0.022219\n",
      "  Epoch 13/15 completed in 1.11s, Avg Loss: 0.022043\n",
      "  Epoch 14/15 completed in 1.21s, Avg Loss: 0.021873\n",
      "  Epoch 15/15 completed in 1.14s, Avg Loss: 0.021759\n",
      "[17:25:16] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=512, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.026, test=-0.027) total time=  17.3s\n",
      "\n",
      "[17:25:16] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.84s, Avg Loss: 0.210758\n",
      "  Epoch 2/10 completed in 0.84s, Avg Loss: 0.141854\n",
      "  Epoch 3/10 completed in 0.83s, Avg Loss: 0.080567\n",
      "  Epoch 4/10 completed in 0.93s, Avg Loss: 0.048742\n",
      "  Epoch 5/10 completed in 0.73s, Avg Loss: 0.036047\n",
      "  Epoch 6/10 completed in 0.99s, Avg Loss: 0.030861\n",
      "  Epoch 7/10 completed in 0.73s, Avg Loss: 0.028455\n",
      "  Epoch 8/10 completed in 0.94s, Avg Loss: 0.027167\n",
      "  Epoch 9/10 completed in 0.73s, Avg Loss: 0.026439\n",
      "  Epoch 10/10 completed in 0.93s, Avg Loss: 0.025923\n",
      "[17:25:25] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.5s\n",
      "\n",
      "[17:25:25] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.73s, Avg Loss: 0.216111\n",
      "  Epoch 2/10 completed in 0.85s, Avg Loss: 0.145090\n",
      "  Epoch 3/10 completed in 0.73s, Avg Loss: 0.085379\n",
      "  Epoch 4/10 completed in 0.84s, Avg Loss: 0.053048\n",
      "  Epoch 5/10 completed in 0.84s, Avg Loss: 0.039032\n",
      "  Epoch 6/10 completed in 0.74s, Avg Loss: 0.032884\n",
      "  Epoch 7/10 completed in 0.95s, Avg Loss: 0.029841\n",
      "  Epoch 8/10 completed in 0.74s, Avg Loss: 0.028275\n",
      "  Epoch 9/10 completed in 0.95s, Avg Loss: 0.027322\n",
      "  Epoch 10/10 completed in 0.74s, Avg Loss: 0.026669\n",
      "[17:25:33] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   8.1s\n",
      "\n",
      "[17:25:33] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.91s, Avg Loss: 0.244576\n",
      "  Epoch 2/10 completed in 0.82s, Avg Loss: 0.154742\n",
      "  Epoch 3/10 completed in 0.85s, Avg Loss: 0.081026\n",
      "  Epoch 4/10 completed in 0.73s, Avg Loss: 0.047841\n",
      "  Epoch 5/10 completed in 0.74s, Avg Loss: 0.035764\n",
      "  Epoch 6/10 completed in 0.94s, Avg Loss: 0.030827\n",
      "  Epoch 7/10 completed in 0.74s, Avg Loss: 0.028543\n",
      "  Epoch 8/10 completed in 0.95s, Avg Loss: 0.027251\n",
      "  Epoch 9/10 completed in 0.95s, Avg Loss: 0.026530\n",
      "  Epoch 10/10 completed in 0.94s, Avg Loss: 0.026016\n",
      "[17:25:41] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.6s\n",
      "\n",
      "[17:25:41] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.80s, Avg Loss: 0.174902\n",
      "  Epoch 2/10 completed in 0.90s, Avg Loss: 0.051701\n",
      "  Epoch 3/10 completed in 0.79s, Avg Loss: 0.028084\n",
      "  Epoch 4/10 completed in 0.88s, Avg Loss: 0.026245\n",
      "  Epoch 5/10 completed in 0.89s, Avg Loss: 0.025799\n",
      "  Epoch 6/10 completed in 0.79s, Avg Loss: 0.025479\n",
      "  Epoch 7/10 completed in 6.47s, Avg Loss: 0.025258\n",
      "  Epoch 8/10 completed in 0.78s, Avg Loss: 0.024994\n",
      "  Epoch 9/10 completed in 1.00s, Avg Loss: 0.024722\n",
      "  Epoch 10/10 completed in 0.79s, Avg Loss: 0.024521\n",
      "[17:25:55] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  14.1s\n",
      "\n",
      "[17:25:55] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.90s, Avg Loss: 0.167534\n",
      "  Epoch 2/10 completed in 0.88s, Avg Loss: 0.053390\n",
      "  Epoch 3/10 completed in 0.88s, Avg Loss: 0.028711\n",
      "  Epoch 4/10 completed in 0.78s, Avg Loss: 0.026643\n",
      "  Epoch 5/10 completed in 0.79s, Avg Loss: 0.026163\n",
      "  Epoch 6/10 completed in 0.99s, Avg Loss: 0.025913\n",
      "  Epoch 7/10 completed in 0.78s, Avg Loss: 0.025673\n",
      "  Epoch 8/10 completed in 1.01s, Avg Loss: 0.025420\n",
      "  Epoch 9/10 completed in 0.78s, Avg Loss: 0.025234\n",
      "  Epoch 10/10 completed in 0.99s, Avg Loss: 0.025031\n",
      "[17:26:04] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   8.8s\n",
      "\n",
      "[17:26:04] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.78s, Avg Loss: 0.219479\n",
      "  Epoch 2/10 completed in 0.89s, Avg Loss: 0.057935\n",
      "  Epoch 3/10 completed in 0.79s, Avg Loss: 0.028038\n",
      "  Epoch 4/10 completed in 0.89s, Avg Loss: 0.026004\n",
      "  Epoch 5/10 completed in 0.88s, Avg Loss: 0.025612\n",
      "  Epoch 6/10 completed in 0.78s, Avg Loss: 0.025370\n",
      "  Epoch 7/10 completed in 1.01s, Avg Loss: 0.025116\n",
      "  Epoch 8/10 completed in 0.79s, Avg Loss: 0.024921\n",
      "  Epoch 9/10 completed in 1.00s, Avg Loss: 0.024629\n",
      "  Epoch 10/10 completed in 0.80s, Avg Loss: 0.024329\n",
      "[17:26:13] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.6s\n",
      "\n",
      "[17:26:13] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.83s, Avg Loss: 0.246339\n",
      "  Epoch 2/15 completed in 0.81s, Avg Loss: 0.168052\n",
      "  Epoch 3/15 completed in 0.82s, Avg Loss: 0.094413\n",
      "  Epoch 4/15 completed in 0.72s, Avg Loss: 0.053909\n",
      "  Epoch 5/15 completed in 0.73s, Avg Loss: 0.037966\n",
      "  Epoch 6/15 completed in 0.91s, Avg Loss: 0.031685\n",
      "  Epoch 7/15 completed in 0.72s, Avg Loss: 0.028798\n",
      "  Epoch 8/15 completed in 0.92s, Avg Loss: 0.027308\n",
      "  Epoch 9/15 completed in 0.73s, Avg Loss: 0.026481\n",
      "  Epoch 10/15 completed in 0.94s, Avg Loss: 0.025904\n",
      "  Epoch 11/15 completed in 0.73s, Avg Loss: 0.025408\n",
      "  Epoch 12/15 completed in 0.95s, Avg Loss: 0.025149\n",
      "  Epoch 13/15 completed in 0.73s, Avg Loss: 0.024900\n",
      "  Epoch 14/15 completed in 0.93s, Avg Loss: 0.024662\n",
      "  Epoch 15/15 completed in 0.73s, Avg Loss: 0.024464\n",
      "[17:26:25] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  12.2s\n",
      "\n",
      "[17:26:25] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.83s, Avg Loss: 0.225648\n",
      "  Epoch 2/15 completed in 0.82s, Avg Loss: 0.141862\n",
      "  Epoch 3/15 completed in 0.83s, Avg Loss: 0.074618\n",
      "  Epoch 4/15 completed in 0.72s, Avg Loss: 0.045359\n",
      "  Epoch 5/15 completed in 0.72s, Avg Loss: 0.034829\n",
      "  Epoch 6/15 completed in 0.93s, Avg Loss: 0.030542\n",
      "  Epoch 7/15 completed in 0.72s, Avg Loss: 0.028530\n",
      "  Epoch 8/15 completed in 0.92s, Avg Loss: 0.027469\n",
      "  Epoch 9/15 completed in 0.72s, Avg Loss: 0.026820\n",
      "  Epoch 10/15 completed in 0.93s, Avg Loss: 0.026372\n",
      "  Epoch 11/15 completed in 0.72s, Avg Loss: 0.026014\n",
      "  Epoch 12/15 completed in 0.93s, Avg Loss: 0.025774\n",
      "  Epoch 13/15 completed in 0.73s, Avg Loss: 0.025587\n",
      "  Epoch 14/15 completed in 0.93s, Avg Loss: 0.025311\n",
      "  Epoch 15/15 completed in 0.73s, Avg Loss: 0.025131\n",
      "[17:26:37] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  12.2s\n",
      "\n",
      "[17:26:37] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.84s, Avg Loss: 0.215218\n",
      "  Epoch 2/15 completed in 90.52s, Avg Loss: 0.123753\n",
      "  Epoch 3/15 completed in 0.90s, Avg Loss: 0.062820\n",
      "  Epoch 4/15 completed in 0.73s, Avg Loss: 0.039679\n",
      "  Epoch 5/15 completed in 0.74s, Avg Loss: 0.031686\n",
      "  Epoch 6/15 completed in 0.92s, Avg Loss: 0.028540\n",
      "  Epoch 7/15 completed in 0.73s, Avg Loss: 0.026924\n",
      "  Epoch 8/15 completed in 0.93s, Avg Loss: 0.026146\n",
      "  Epoch 9/15 completed in 0.74s, Avg Loss: 0.025595\n",
      "  Epoch 10/15 completed in 0.93s, Avg Loss: 0.025229\n",
      "  Epoch 11/15 completed in 0.73s, Avg Loss: 0.024952\n",
      "  Epoch 12/15 completed in 0.95s, Avg Loss: 0.024673\n",
      "  Epoch 13/15 completed in 0.76s, Avg Loss: 0.024444\n",
      "  Epoch 14/15 completed in 0.94s, Avg Loss: 0.024261\n",
      "  Epoch 15/15 completed in 0.73s, Avg Loss: 0.024069\n",
      "[17:28:19] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time= 1.7min\n",
      "\n",
      "[17:28:19] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.90s, Avg Loss: 0.179798\n",
      "  Epoch 2/15 completed in 0.89s, Avg Loss: 0.057819\n",
      "  Epoch 3/15 completed in 0.89s, Avg Loss: 0.028678\n",
      "  Epoch 4/15 completed in 0.79s, Avg Loss: 0.026337\n",
      "  Epoch 5/15 completed in 0.79s, Avg Loss: 0.025826\n",
      "  Epoch 6/15 completed in 0.99s, Avg Loss: 0.025640\n",
      "  Epoch 7/15 completed in 0.80s, Avg Loss: 0.025440\n",
      "  Epoch 8/15 completed in 1.02s, Avg Loss: 0.025240\n",
      "  Epoch 9/15 completed in 0.80s, Avg Loss: 0.025072\n",
      "  Epoch 10/15 completed in 1.00s, Avg Loss: 0.024869\n",
      "  Epoch 11/15 completed in 0.79s, Avg Loss: 0.024638\n",
      "  Epoch 12/15 completed in 0.99s, Avg Loss: 0.024394\n",
      "  Epoch 13/15 completed in 0.89s, Avg Loss: 0.024260\n",
      "  Epoch 14/15 completed in 1.06s, Avg Loss: 0.024031\n",
      "  Epoch 15/15 completed in 0.78s, Avg Loss: 0.023802\n",
      "[17:28:33] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.4s\n",
      "\n",
      "[17:28:33] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.90s, Avg Loss: 0.221422\n",
      "  Epoch 2/15 completed in 0.88s, Avg Loss: 0.113482\n",
      "  Epoch 3/15 completed in 0.89s, Avg Loss: 0.040913\n",
      "  Epoch 4/15 completed in 0.77s, Avg Loss: 0.028988\n",
      "  Epoch 5/15 completed in 0.79s, Avg Loss: 0.026889\n",
      "  Epoch 6/15 completed in 0.98s, Avg Loss: 0.026250\n",
      "  Epoch 7/15 completed in 0.78s, Avg Loss: 0.025900\n",
      "  Epoch 8/15 completed in 1.05s, Avg Loss: 0.025660\n",
      "  Epoch 9/15 completed in 0.84s, Avg Loss: 0.025408\n",
      "  Epoch 10/15 completed in 1.02s, Avg Loss: 0.025148\n",
      "  Epoch 11/15 completed in 0.79s, Avg Loss: 0.024904\n",
      "  Epoch 12/15 completed in 0.99s, Avg Loss: 0.024689\n",
      "  Epoch 13/15 completed in 0.79s, Avg Loss: 0.024481\n",
      "  Epoch 14/15 completed in 0.99s, Avg Loss: 0.024309\n",
      "  Epoch 15/15 completed in 0.79s, Avg Loss: 0.024165\n",
      "[17:28:46] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  13.3s\n",
      "\n",
      "[17:28:46] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.91s, Avg Loss: 0.208049\n",
      "  Epoch 2/15 completed in 0.90s, Avg Loss: 0.063967\n",
      "  Epoch 3/15 completed in 0.90s, Avg Loss: 0.029849\n",
      "  Epoch 4/15 completed in 0.79s, Avg Loss: 0.026545\n",
      "  Epoch 5/15 completed in 0.81s, Avg Loss: 0.025752\n",
      "  Epoch 6/15 completed in 1.01s, Avg Loss: 0.025393\n",
      "  Epoch 7/15 completed in 0.80s, Avg Loss: 0.025140\n",
      "  Epoch 8/15 completed in 1.00s, Avg Loss: 0.024967\n",
      "  Epoch 9/15 completed in 0.82s, Avg Loss: 0.024774\n",
      "  Epoch 10/15 completed in 1.00s, Avg Loss: 0.024566\n",
      "  Epoch 11/15 completed in 969.07s, Avg Loss: 0.024291\n",
      "  Epoch 12/15 completed in 1.02s, Avg Loss: 0.024090\n",
      "  Epoch 13/15 completed in 0.80s, Avg Loss: 0.023901\n",
      "  Epoch 14/15 completed in 1.00s, Avg Loss: 0.023647\n",
      "  Epoch 15/15 completed in 0.80s, Avg Loss: 0.023491\n",
      "[17:45:08] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.0, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=16.4min\n",
      "\n",
      "[17:45:08] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.90s, Avg Loss: 0.216796\n",
      "  Epoch 2/10 completed in 0.89s, Avg Loss: 0.154629\n",
      "  Epoch 3/10 completed in 0.89s, Avg Loss: 0.094528\n",
      "  Epoch 4/10 completed in 0.79s, Avg Loss: 0.056301\n",
      "  Epoch 5/10 completed in 0.79s, Avg Loss: 0.039227\n",
      "  Epoch 6/10 completed in 1.02s, Avg Loss: 0.032050\n",
      "  Epoch 7/10 completed in 0.80s, Avg Loss: 0.028686\n",
      "  Epoch 8/10 completed in 1.00s, Avg Loss: 0.026905\n",
      "  Epoch 9/10 completed in 0.79s, Avg Loss: 0.025991\n",
      "  Epoch 10/10 completed in 1.00s, Avg Loss: 0.025292\n",
      "[17:45:17] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.9s\n",
      "\n",
      "[17:45:17] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.79s, Avg Loss: 0.191689\n",
      "  Epoch 2/10 completed in 0.90s, Avg Loss: 0.124276\n",
      "  Epoch 3/10 completed in 0.79s, Avg Loss: 0.068941\n",
      "  Epoch 4/10 completed in 0.90s, Avg Loss: 0.042795\n",
      "  Epoch 5/10 completed in 0.89s, Avg Loss: 0.033079\n",
      "  Epoch 6/10 completed in 0.79s, Avg Loss: 0.029169\n",
      "  Epoch 7/10 completed in 0.98s, Avg Loss: 0.027369\n",
      "  Epoch 8/10 completed in 0.80s, Avg Loss: 0.026410\n",
      "  Epoch 9/10 completed in 1.00s, Avg Loss: 0.025783\n",
      "  Epoch 10/10 completed in 0.80s, Avg Loss: 0.025353\n",
      "[17:45:25] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   8.6s\n",
      "\n",
      "[17:45:25] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.90s, Avg Loss: 0.163849\n",
      "  Epoch 2/10 completed in 0.88s, Avg Loss: 0.092297\n",
      "  Epoch 3/10 completed in 0.89s, Avg Loss: 0.050510\n",
      "  Epoch 4/10 completed in 0.79s, Avg Loss: 0.035304\n",
      "  Epoch 5/10 completed in 0.88s, Avg Loss: 0.029873\n",
      "  Epoch 6/10 completed in 0.89s, Avg Loss: 0.027549\n",
      "  Epoch 7/10 completed in 0.79s, Avg Loss: 0.026362\n",
      "  Epoch 8/10 completed in 1.00s, Avg Loss: 0.025651\n",
      "  Epoch 9/10 completed in 0.78s, Avg Loss: 0.025179\n",
      "  Epoch 10/10 completed in 1.00s, Avg Loss: 0.024776\n",
      "[17:45:34] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.8s\n",
      "\n",
      "[17:45:34] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.86s, Avg Loss: 0.233886\n",
      "  Epoch 2/10 completed in 0.97s, Avg Loss: 0.086866\n",
      "  Epoch 3/10 completed in 0.85s, Avg Loss: 0.030706\n",
      "  Epoch 4/10 completed in 0.96s, Avg Loss: 0.026275\n",
      "  Epoch 5/10 completed in 0.95s, Avg Loss: 0.025253\n",
      "  Epoch 6/10 completed in 0.86s, Avg Loss: 0.024726\n",
      "  Epoch 7/10 completed in 1.05s, Avg Loss: 0.024283\n",
      "  Epoch 8/10 completed in 0.86s, Avg Loss: 0.023931\n",
      "  Epoch 9/10 completed in 1.21s, Avg Loss: 0.023593\n",
      "  Epoch 10/10 completed in 0.85s, Avg Loss: 0.023328\n",
      "[17:45:44] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.4s\n",
      "\n",
      "[17:45:44] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.95s, Avg Loss: 0.222762\n",
      "  Epoch 2/10 completed in 0.93s, Avg Loss: 0.069875\n",
      "  Epoch 3/10 completed in 0.94s, Avg Loss: 0.030057\n",
      "  Epoch 4/10 completed in 0.83s, Avg Loss: 0.026707\n",
      "  Epoch 5/10 completed in 0.84s, Avg Loss: 0.025939\n",
      "  Epoch 6/10 completed in 1.04s, Avg Loss: 0.025504\n",
      "  Epoch 7/10 completed in 0.83s, Avg Loss: 0.025011\n",
      "  Epoch 8/10 completed in 1.16s, Avg Loss: 0.024527\n",
      "  Epoch 9/10 completed in 0.84s, Avg Loss: 0.024157\n",
      "  Epoch 10/10 completed in 1.04s, Avg Loss: 0.023849\n",
      "[17:45:53] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   9.4s\n",
      "\n",
      "[17:45:53] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.84s, Avg Loss: 0.180820\n",
      "  Epoch 2/10 completed in 0.95s, Avg Loss: 0.053952\n",
      "  Epoch 3/10 completed in 0.84s, Avg Loss: 0.027613\n",
      "  Epoch 4/10 completed in 0.94s, Avg Loss: 0.025690\n",
      "  Epoch 5/10 completed in 0.94s, Avg Loss: 0.024967\n",
      "  Epoch 6/10 completed in 0.87s, Avg Loss: 0.024526\n",
      "  Epoch 7/10 completed in 1.04s, Avg Loss: 0.024062\n",
      "  Epoch 8/10 completed in 0.84s, Avg Loss: 0.023834\n",
      "  Epoch 9/10 completed in 1.05s, Avg Loss: 0.023513\n",
      "  Epoch 10/10 completed in 0.84s, Avg Loss: 0.023273\n",
      "[17:46:02] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.2s\n",
      "\n",
      "[17:46:02] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.89s, Avg Loss: 0.227065\n",
      "  Epoch 2/15 completed in 0.88s, Avg Loss: 0.142728\n",
      "  Epoch 3/15 completed in 0.89s, Avg Loss: 0.072110\n",
      "  Epoch 4/15 completed in 0.78s, Avg Loss: 0.043006\n",
      "  Epoch 5/15 completed in 0.85s, Avg Loss: 0.033104\n",
      "  Epoch 6/15 completed in 0.98s, Avg Loss: 0.029181\n",
      "  Epoch 7/15 completed in 0.79s, Avg Loss: 0.027259\n",
      "  Epoch 8/15 completed in 0.99s, Avg Loss: 0.026231\n",
      "  Epoch 9/15 completed in 0.78s, Avg Loss: 0.025531\n",
      "  Epoch 10/15 completed in 0.99s, Avg Loss: 0.025106\n",
      "  Epoch 11/15 completed in 0.78s, Avg Loss: 0.024701\n",
      "  Epoch 12/15 completed in 0.99s, Avg Loss: 0.024406\n",
      "  Epoch 13/15 completed in 0.78s, Avg Loss: 0.024177\n",
      "  Epoch 14/15 completed in 0.99s, Avg Loss: 0.023933\n",
      "  Epoch 15/15 completed in 0.78s, Avg Loss: 0.023770\n",
      "[17:46:15] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.1s\n",
      "\n",
      "[17:46:15] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.90s, Avg Loss: 0.147781\n",
      "  Epoch 2/15 completed in 0.89s, Avg Loss: 0.085299\n",
      "  Epoch 3/15 completed in 0.89s, Avg Loss: 0.049106\n",
      "  Epoch 4/15 completed in 0.79s, Avg Loss: 0.035284\n",
      "  Epoch 5/15 completed in 0.79s, Avg Loss: 0.030063\n",
      "  Epoch 6/15 completed in 0.99s, Avg Loss: 0.027672\n",
      "  Epoch 7/15 completed in 0.79s, Avg Loss: 0.026457\n",
      "  Epoch 8/15 completed in 0.99s, Avg Loss: 0.025693\n",
      "  Epoch 9/15 completed in 0.79s, Avg Loss: 0.025150\n",
      "  Epoch 10/15 completed in 0.99s, Avg Loss: 0.024796\n",
      "  Epoch 11/15 completed in 0.79s, Avg Loss: 0.024434\n",
      "  Epoch 12/15 completed in 1.00s, Avg Loss: 0.024176\n",
      "  Epoch 13/15 completed in 0.81s, Avg Loss: 0.023964\n",
      "  Epoch 14/15 completed in 0.99s, Avg Loss: 0.023788\n",
      "  Epoch 15/15 completed in 0.79s, Avg Loss: 0.023542\n",
      "[17:46:29] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  13.2s\n",
      "\n",
      "[17:46:29] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.89s, Avg Loss: 0.179870\n",
      "  Epoch 2/15 completed in 0.87s, Avg Loss: 0.113665\n",
      "  Epoch 3/15 completed in 0.88s, Avg Loss: 0.062740\n",
      "  Epoch 4/15 completed in 0.78s, Avg Loss: 0.039767\n",
      "  Epoch 5/15 completed in 0.78s, Avg Loss: 0.031432\n",
      "  Epoch 6/15 completed in 0.98s, Avg Loss: 0.028070\n",
      "  Epoch 7/15 completed in 0.78s, Avg Loss: 0.026426\n",
      "  Epoch 8/15 completed in 0.99s, Avg Loss: 0.025499\n",
      "  Epoch 9/15 completed in 0.78s, Avg Loss: 0.024971\n",
      "  Epoch 10/15 completed in 0.98s, Avg Loss: 0.024546\n",
      "  Epoch 11/15 completed in 0.78s, Avg Loss: 0.024183\n",
      "  Epoch 12/15 completed in 0.99s, Avg Loss: 0.023910\n",
      "  Epoch 13/15 completed in 0.78s, Avg Loss: 0.023591\n",
      "  Epoch 14/15 completed in 0.99s, Avg Loss: 0.023398\n",
      "  Epoch 15/15 completed in 0.78s, Avg Loss: 0.023172\n",
      "[17:46:42] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.1s\n",
      "\n",
      "[17:46:42] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.96s, Avg Loss: 0.249753\n",
      "  Epoch 2/15 completed in 0.96s, Avg Loss: 0.116492\n",
      "  Epoch 3/15 completed in 0.97s, Avg Loss: 0.033670\n",
      "  Epoch 4/15 completed in 0.85s, Avg Loss: 0.026633\n",
      "  Epoch 5/15 completed in 0.86s, Avg Loss: 0.025565\n",
      "  Epoch 6/15 completed in 1.09s, Avg Loss: 0.025075\n",
      "  Epoch 7/15 completed in 0.86s, Avg Loss: 0.024707\n",
      "  Epoch 8/15 completed in 1.07s, Avg Loss: 0.024331\n",
      "  Epoch 9/15 completed in 0.85s, Avg Loss: 0.024034\n",
      "  Epoch 10/15 completed in 1.07s, Avg Loss: 0.023838\n",
      "  Epoch 11/15 completed in 0.86s, Avg Loss: 0.023598\n",
      "  Epoch 12/15 completed in 1.07s, Avg Loss: 0.023421\n",
      "  Epoch 13/15 completed in 0.86s, Avg Loss: 0.023296\n",
      "  Epoch 14/15 completed in 1.08s, Avg Loss: 0.023130\n",
      "  Epoch 15/15 completed in 0.87s, Avg Loss: 0.022979\n",
      "[17:46:56] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  14.3s\n",
      "\n",
      "[17:46:56] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.96s, Avg Loss: 0.233220\n",
      "  Epoch 2/15 completed in 0.93s, Avg Loss: 0.087161\n",
      "  Epoch 3/15 completed in 0.95s, Avg Loss: 0.031926\n",
      "  Epoch 4/15 completed in 0.85s, Avg Loss: 0.026857\n",
      "  Epoch 5/15 completed in 0.85s, Avg Loss: 0.025786\n",
      "  Epoch 6/15 completed in 1.04s, Avg Loss: 0.025270\n",
      "  Epoch 7/15 completed in 0.84s, Avg Loss: 0.024861\n",
      "  Epoch 8/15 completed in 1.05s, Avg Loss: 0.024401\n",
      "  Epoch 9/15 completed in 0.85s, Avg Loss: 0.024061\n",
      "  Epoch 10/15 completed in 1.05s, Avg Loss: 0.023859\n",
      "  Epoch 11/15 completed in 0.85s, Avg Loss: 0.023527\n",
      "  Epoch 12/15 completed in 1.05s, Avg Loss: 0.023318\n",
      "  Epoch 13/15 completed in 0.85s, Avg Loss: 0.023157\n",
      "  Epoch 14/15 completed in 1.05s, Avg Loss: 0.023009\n",
      "  Epoch 15/15 completed in 0.85s, Avg Loss: 0.022891\n",
      "[17:47:10] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  14.0s\n",
      "\n",
      "[17:47:10] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.0\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.95s, Avg Loss: 0.195000\n",
      "  Epoch 2/15 completed in 0.94s, Avg Loss: 0.052477\n",
      "  Epoch 3/15 completed in 0.95s, Avg Loss: 0.027361\n",
      "  Epoch 4/15 completed in 0.84s, Avg Loss: 0.025312\n",
      "  Epoch 5/15 completed in 0.86s, Avg Loss: 0.024623\n",
      "  Epoch 6/15 completed in 1.04s, Avg Loss: 0.024170\n",
      "  Epoch 7/15 completed in 0.85s, Avg Loss: 0.023876\n",
      "  Epoch 8/15 completed in 1.07s, Avg Loss: 0.023550\n",
      "  Epoch 9/15 completed in 0.85s, Avg Loss: 0.023373\n",
      "  Epoch 10/15 completed in 1.04s, Avg Loss: 0.023177\n",
      "  Epoch 11/15 completed in 0.84s, Avg Loss: 0.022985\n",
      "  Epoch 12/15 completed in 1.05s, Avg Loss: 0.022863\n",
      "  Epoch 13/15 completed in 0.85s, Avg Loss: 0.022703\n",
      "  Epoch 14/15 completed in 1.06s, Avg Loss: 0.022574\n",
      "  Epoch 15/15 completed in 0.85s, Avg Loss: 0.022430\n",
      "[17:47:24] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.0, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  14.1s\n",
      "\n",
      "[17:47:24] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.88s, Avg Loss: 0.223241\n",
      "  Epoch 2/10 completed in 0.87s, Avg Loss: 0.142214\n",
      "  Epoch 3/10 completed in 0.88s, Avg Loss: 0.077956\n",
      "  Epoch 4/10 completed in 0.78s, Avg Loss: 0.046999\n",
      "  Epoch 5/10 completed in 0.78s, Avg Loss: 0.035410\n",
      "  Epoch 6/10 completed in 0.98s, Avg Loss: 0.030778\n",
      "  Epoch 7/10 completed in 0.77s, Avg Loss: 0.028638\n",
      "  Epoch 8/10 completed in 0.97s, Avg Loss: 0.027440\n",
      "  Epoch 9/10 completed in 0.78s, Avg Loss: 0.026780\n",
      "  Epoch 10/10 completed in 0.98s, Avg Loss: 0.026380\n",
      "[17:47:33] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.7s\n",
      "\n",
      "[17:47:33] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.78s, Avg Loss: 0.228628\n",
      "  Epoch 2/10 completed in 0.89s, Avg Loss: 0.141130\n",
      "  Epoch 3/10 completed in 0.76s, Avg Loss: 0.074086\n",
      "  Epoch 4/10 completed in 0.90s, Avg Loss: 0.045467\n",
      "  Epoch 5/10 completed in 0.88s, Avg Loss: 0.035032\n",
      "  Epoch 6/10 completed in 0.78s, Avg Loss: 0.030688\n",
      "  Epoch 7/10 completed in 0.98s, Avg Loss: 0.028604\n",
      "  Epoch 8/10 completed in 0.78s, Avg Loss: 0.027552\n",
      "  Epoch 9/10 completed in 0.98s, Avg Loss: 0.026821\n",
      "  Epoch 10/10 completed in 0.78s, Avg Loss: 0.026340\n",
      "[17:47:41] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   8.5s\n",
      "\n",
      "[17:47:41] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.88s, Avg Loss: 0.225317\n",
      "  Epoch 2/10 completed in 0.86s, Avg Loss: 0.138491\n",
      "  Epoch 3/10 completed in 0.87s, Avg Loss: 0.075003\n",
      "  Epoch 4/10 completed in 0.77s, Avg Loss: 0.046606\n",
      "  Epoch 5/10 completed in 0.78s, Avg Loss: 0.035481\n",
      "  Epoch 6/10 completed in 0.98s, Avg Loss: 0.030796\n",
      "  Epoch 7/10 completed in 0.79s, Avg Loss: 0.028499\n",
      "  Epoch 8/10 completed in 0.99s, Avg Loss: 0.027221\n",
      "  Epoch 9/10 completed in 0.78s, Avg Loss: 0.026479\n",
      "  Epoch 10/10 completed in 0.99s, Avg Loss: 0.025977\n",
      "[17:47:50] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   8.7s\n",
      "\n",
      "[17:47:50] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.90s, Avg Loss: 0.201416\n",
      "  Epoch 2/10 completed in 1.01s, Avg Loss: 0.061429\n",
      "  Epoch 3/10 completed in 0.90s, Avg Loss: 0.029699\n",
      "  Epoch 4/10 completed in 1.01s, Avg Loss: 0.026810\n",
      "  Epoch 5/10 completed in 1.00s, Avg Loss: 0.026144\n",
      "  Epoch 6/10 completed in 0.91s, Avg Loss: 0.025796\n",
      "  Epoch 7/10 completed in 1.10s, Avg Loss: 0.025572\n",
      "  Epoch 8/10 completed in 0.90s, Avg Loss: 0.025337\n",
      "  Epoch 9/10 completed in 1.11s, Avg Loss: 0.025147\n",
      "  Epoch 10/10 completed in 0.91s, Avg Loss: 0.024880\n",
      "[17:48:00] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.8s\n",
      "\n",
      "[17:48:00] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.02s, Avg Loss: 0.150967\n",
      "  Epoch 2/10 completed in 1.01s, Avg Loss: 0.045889\n",
      "  Epoch 3/10 completed in 1.05s, Avg Loss: 0.028482\n",
      "  Epoch 4/10 completed in 0.92s, Avg Loss: 0.026954\n",
      "  Epoch 5/10 completed in 0.92s, Avg Loss: 0.026536\n",
      "  Epoch 6/10 completed in 1.12s, Avg Loss: 0.026395\n",
      "  Epoch 7/10 completed in 0.92s, Avg Loss: 0.026206\n",
      "  Epoch 8/10 completed in 1.12s, Avg Loss: 0.026156\n",
      "  Epoch 9/10 completed in 0.91s, Avg Loss: 0.026003\n",
      "  Epoch 10/10 completed in 1.12s, Avg Loss: 0.025766\n",
      "[17:48:10] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  10.1s\n",
      "\n",
      "[17:48:10] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.90s, Avg Loss: 0.193890\n",
      "  Epoch 2/10 completed in 1.01s, Avg Loss: 0.067951\n",
      "  Epoch 3/10 completed in 0.90s, Avg Loss: 0.029995\n",
      "  Epoch 4/10 completed in 1.01s, Avg Loss: 0.026550\n",
      "  Epoch 5/10 completed in 1.00s, Avg Loss: 0.025920\n",
      "  Epoch 6/10 completed in 0.90s, Avg Loss: 0.025743\n",
      "  Epoch 7/10 completed in 1.11s, Avg Loss: 0.025552\n",
      "  Epoch 8/10 completed in 0.91s, Avg Loss: 0.025409\n",
      "  Epoch 9/10 completed in 1.11s, Avg Loss: 0.025311\n",
      "  Epoch 10/10 completed in 0.90s, Avg Loss: 0.025211\n",
      "[17:48:20] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.8s\n",
      "\n",
      "[17:48:20] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.87s, Avg Loss: 0.174580\n",
      "  Epoch 2/15 completed in 0.88s, Avg Loss: 0.117207\n",
      "  Epoch 3/15 completed in 0.88s, Avg Loss: 0.070210\n",
      "  Epoch 4/15 completed in 0.77s, Avg Loss: 0.045215\n",
      "  Epoch 5/15 completed in 0.78s, Avg Loss: 0.034830\n",
      "  Epoch 6/15 completed in 0.98s, Avg Loss: 0.030438\n",
      "  Epoch 7/15 completed in 0.78s, Avg Loss: 0.028375\n",
      "  Epoch 8/15 completed in 0.98s, Avg Loss: 0.027251\n",
      "  Epoch 9/15 completed in 0.78s, Avg Loss: 0.026549\n",
      "  Epoch 10/15 completed in 0.98s, Avg Loss: 0.026116\n",
      "  Epoch 11/15 completed in 0.78s, Avg Loss: 0.025829\n",
      "  Epoch 12/15 completed in 0.98s, Avg Loss: 0.025578\n",
      "  Epoch 13/15 completed in 0.78s, Avg Loss: 0.025381\n",
      "  Epoch 14/15 completed in 0.98s, Avg Loss: 0.025195\n",
      "  Epoch 15/15 completed in 0.78s, Avg Loss: 0.025025\n",
      "[17:48:33] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.0s\n",
      "\n",
      "[17:48:33] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.88s, Avg Loss: 0.195949\n",
      "  Epoch 2/15 completed in 0.87s, Avg Loss: 0.135874\n",
      "  Epoch 3/15 completed in 0.88s, Avg Loss: 0.080771\n",
      "  Epoch 4/15 completed in 0.77s, Avg Loss: 0.050171\n",
      "  Epoch 5/15 completed in 0.78s, Avg Loss: 0.037244\n",
      "  Epoch 6/15 completed in 0.97s, Avg Loss: 0.031752\n",
      "  Epoch 7/15 completed in 0.78s, Avg Loss: 0.029257\n",
      "  Epoch 8/15 completed in 0.98s, Avg Loss: 0.027945\n",
      "  Epoch 9/15 completed in 0.78s, Avg Loss: 0.027194\n",
      "  Epoch 10/15 completed in 0.98s, Avg Loss: 0.026782\n",
      "  Epoch 11/15 completed in 0.77s, Avg Loss: 0.026424\n",
      "  Epoch 12/15 completed in 0.97s, Avg Loss: 0.026214\n",
      "  Epoch 13/15 completed in 0.78s, Avg Loss: 0.026042\n",
      "  Epoch 14/15 completed in 0.98s, Avg Loss: 0.025812\n",
      "  Epoch 15/15 completed in 0.78s, Avg Loss: 0.025688\n",
      "[17:48:46] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  12.9s\n",
      "\n",
      "[17:48:46] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.89s, Avg Loss: 0.197774\n",
      "  Epoch 2/15 completed in 0.87s, Avg Loss: 0.134370\n",
      "  Epoch 3/15 completed in 0.87s, Avg Loss: 0.078833\n",
      "  Epoch 4/15 completed in 0.77s, Avg Loss: 0.048928\n",
      "  Epoch 5/15 completed in 0.78s, Avg Loss: 0.036689\n",
      "  Epoch 6/15 completed in 0.99s, Avg Loss: 0.031410\n",
      "  Epoch 7/15 completed in 0.78s, Avg Loss: 0.029037\n",
      "  Epoch 8/15 completed in 0.98s, Avg Loss: 0.027634\n",
      "  Epoch 9/15 completed in 0.77s, Avg Loss: 0.026813\n",
      "  Epoch 10/15 completed in 0.98s, Avg Loss: 0.026363\n",
      "  Epoch 11/15 completed in 0.78s, Avg Loss: 0.026045\n",
      "  Epoch 12/15 completed in 0.99s, Avg Loss: 0.025717\n",
      "  Epoch 13/15 completed in 0.78s, Avg Loss: 0.025423\n",
      "  Epoch 14/15 completed in 0.98s, Avg Loss: 0.025278\n",
      "  Epoch 15/15 completed in 0.77s, Avg Loss: 0.025057\n",
      "[17:48:59] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.0s\n",
      "\n",
      "[17:48:59] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 1.01s, Avg Loss: 0.254774\n",
      "  Epoch 2/15 completed in 1.01s, Avg Loss: 0.098415\n",
      "  Epoch 3/15 completed in 1.01s, Avg Loss: 0.032803\n",
      "  Epoch 4/15 completed in 0.92s, Avg Loss: 0.027688\n",
      "  Epoch 5/15 completed in 0.92s, Avg Loss: 0.026665\n",
      "  Epoch 6/15 completed in 1.13s, Avg Loss: 0.026256\n",
      "  Epoch 7/15 completed in 0.92s, Avg Loss: 0.026080\n",
      "  Epoch 8/15 completed in 1.11s, Avg Loss: 0.025989\n",
      "  Epoch 9/15 completed in 0.92s, Avg Loss: 0.025868\n",
      "  Epoch 10/15 completed in 1.13s, Avg Loss: 0.025770\n",
      "  Epoch 11/15 completed in 0.91s, Avg Loss: 0.025694\n",
      "  Epoch 12/15 completed in 1.12s, Avg Loss: 0.025587\n",
      "  Epoch 13/15 completed in 0.92s, Avg Loss: 0.025426\n",
      "  Epoch 14/15 completed in 1.13s, Avg Loss: 0.025304\n",
      "  Epoch 15/15 completed in 0.92s, Avg Loss: 0.025062\n",
      "[17:49:14] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  15.1s\n",
      "\n",
      "[17:49:14] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.02s, Avg Loss: 0.169319\n",
      "  Epoch 2/15 completed in 1.01s, Avg Loss: 0.056728\n",
      "  Epoch 3/15 completed in 1.03s, Avg Loss: 0.029963\n",
      "  Epoch 4/15 completed in 0.92s, Avg Loss: 0.027106\n",
      "  Epoch 5/15 completed in 0.92s, Avg Loss: 0.026415\n",
      "  Epoch 6/15 completed in 1.15s, Avg Loss: 0.026067\n",
      "  Epoch 7/15 completed in 0.91s, Avg Loss: 0.025771\n",
      "  Epoch 8/15 completed in 1.11s, Avg Loss: 0.025645\n",
      "  Epoch 9/15 completed in 0.93s, Avg Loss: 0.025430\n",
      "  Epoch 10/15 completed in 1.13s, Avg Loss: 0.025218\n",
      "  Epoch 11/15 completed in 0.92s, Avg Loss: 0.025087\n",
      "  Epoch 12/15 completed in 1.12s, Avg Loss: 0.024884\n",
      "  Epoch 13/15 completed in 0.93s, Avg Loss: 0.024734\n",
      "  Epoch 14/15 completed in 1.13s, Avg Loss: 0.024510\n",
      "  Epoch 15/15 completed in 0.92s, Avg Loss: 0.024383\n",
      "[17:49:29] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  15.2s\n",
      "\n",
      "[17:49:29] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.01s, Avg Loss: 0.180447\n",
      "  Epoch 2/15 completed in 0.99s, Avg Loss: 0.060583\n",
      "  Epoch 3/15 completed in 1.00s, Avg Loss: 0.029851\n",
      "  Epoch 4/15 completed in 0.89s, Avg Loss: 0.026610\n",
      "  Epoch 5/15 completed in 0.91s, Avg Loss: 0.025927\n",
      "  Epoch 6/15 completed in 1.11s, Avg Loss: 0.025691\n",
      "  Epoch 7/15 completed in 0.90s, Avg Loss: 0.025462\n",
      "  Epoch 8/15 completed in 1.11s, Avg Loss: 0.025350\n",
      "  Epoch 9/15 completed in 0.92s, Avg Loss: 0.025159\n",
      "  Epoch 10/15 completed in 1.11s, Avg Loss: 0.024999\n",
      "  Epoch 11/15 completed in 0.89s, Avg Loss: 0.024776\n",
      "  Epoch 12/15 completed in 1.11s, Avg Loss: 0.024547\n",
      "  Epoch 13/15 completed in 0.90s, Avg Loss: 0.024381\n",
      "  Epoch 14/15 completed in 1.11s, Avg Loss: 0.024188\n",
      "  Epoch 15/15 completed in 0.91s, Avg Loss: 0.023985\n",
      "[17:49:44] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.1, embedding_size=32, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  14.9s\n",
      "\n",
      "[17:49:44] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.95s, Avg Loss: 0.238759\n",
      "  Epoch 2/10 completed in 0.94s, Avg Loss: 0.145761\n",
      "  Epoch 3/10 completed in 0.95s, Avg Loss: 0.072096\n",
      "  Epoch 4/10 completed in 0.84s, Avg Loss: 0.042393\n",
      "  Epoch 5/10 completed in 0.84s, Avg Loss: 0.032694\n",
      "  Epoch 6/10 completed in 1.05s, Avg Loss: 0.028848\n",
      "  Epoch 7/10 completed in 0.84s, Avg Loss: 0.027037\n",
      "  Epoch 8/10 completed in 1.04s, Avg Loss: 0.026055\n",
      "  Epoch 9/10 completed in 0.84s, Avg Loss: 0.025405\n",
      "  Epoch 10/10 completed in 1.04s, Avg Loss: 0.024887\n",
      "[17:49:53] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.3s\n",
      "\n",
      "[17:49:53] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.85s, Avg Loss: 0.205811\n",
      "  Epoch 2/10 completed in 0.94s, Avg Loss: 0.116479\n",
      "  Epoch 3/10 completed in 0.83s, Avg Loss: 0.061044\n",
      "  Epoch 4/10 completed in 0.94s, Avg Loss: 0.039592\n",
      "  Epoch 5/10 completed in 0.93s, Avg Loss: 0.031953\n",
      "  Epoch 6/10 completed in 0.83s, Avg Loss: 0.028724\n",
      "  Epoch 7/10 completed in 1.04s, Avg Loss: 0.027172\n",
      "  Epoch 8/10 completed in 0.83s, Avg Loss: 0.026244\n",
      "  Epoch 9/10 completed in 1.04s, Avg Loss: 0.025582\n",
      "  Epoch 10/10 completed in 0.85s, Avg Loss: 0.025194\n",
      "[17:50:02] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=   9.1s\n",
      "\n",
      "[17:50:02] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.97s, Avg Loss: 0.171965\n",
      "  Epoch 2/10 completed in 0.93s, Avg Loss: 0.098679\n",
      "  Epoch 3/10 completed in 0.95s, Avg Loss: 0.052871\n",
      "  Epoch 4/10 completed in 0.84s, Avg Loss: 0.035812\n",
      "  Epoch 5/10 completed in 0.94s, Avg Loss: 0.030043\n",
      "  Epoch 6/10 completed in 0.94s, Avg Loss: 0.027568\n",
      "  Epoch 7/10 completed in 0.83s, Avg Loss: 0.026375\n",
      "  Epoch 8/10 completed in 1.04s, Avg Loss: 0.025584\n",
      "  Epoch 9/10 completed in 0.83s, Avg Loss: 0.025094\n",
      "  Epoch 10/10 completed in 1.04s, Avg Loss: 0.024708\n",
      "[17:50:12] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=   9.3s\n",
      "\n",
      "[17:50:12] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/10 completed in 0.98s, Avg Loss: 0.138719\n",
      "  Epoch 2/10 completed in 1.07s, Avg Loss: 0.040608\n",
      "  Epoch 3/10 completed in 0.99s, Avg Loss: 0.027282\n",
      "  Epoch 4/10 completed in 1.08s, Avg Loss: 0.025828\n",
      "  Epoch 5/10 completed in 1.08s, Avg Loss: 0.025268\n",
      "  Epoch 6/10 completed in 0.98s, Avg Loss: 0.024768\n",
      "  Epoch 7/10 completed in 1.20s, Avg Loss: 0.024414\n",
      "  Epoch 8/10 completed in 1.01s, Avg Loss: 0.024041\n",
      "  Epoch 9/10 completed in 1.19s, Avg Loss: 0.023728\n",
      "  Epoch 10/10 completed in 0.98s, Avg Loss: 0.023464\n",
      "[17:50:22] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  10.6s\n",
      "\n",
      "[17:50:22] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 1.07s, Avg Loss: 0.211168\n",
      "  Epoch 2/10 completed in 1.05s, Avg Loss: 0.061874\n",
      "  Epoch 3/10 completed in 1.05s, Avg Loss: 0.029870\n",
      "  Epoch 4/10 completed in 0.95s, Avg Loss: 0.026817\n",
      "  Epoch 5/10 completed in 0.95s, Avg Loss: 0.025890\n",
      "  Epoch 6/10 completed in 1.16s, Avg Loss: 0.025386\n",
      "  Epoch 7/10 completed in 0.95s, Avg Loss: 0.024984\n",
      "  Epoch 8/10 completed in 1.16s, Avg Loss: 0.024641\n",
      "  Epoch 9/10 completed in 0.95s, Avg Loss: 0.024325\n",
      "  Epoch 10/10 completed in 1.15s, Avg Loss: 0.024169\n",
      "[17:50:33] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  10.5s\n",
      "\n",
      "[17:50:33] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 10\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/10 completed in 0.95s, Avg Loss: 0.200285\n",
      "  Epoch 2/10 completed in 1.07s, Avg Loss: 0.056529\n",
      "  Epoch 3/10 completed in 0.96s, Avg Loss: 0.028070\n",
      "  Epoch 4/10 completed in 1.07s, Avg Loss: 0.025948\n",
      "  Epoch 5/10 completed in 1.06s, Avg Loss: 0.025378\n",
      "  Epoch 6/10 completed in 0.98s, Avg Loss: 0.025074\n",
      "  Epoch 7/10 completed in 1.16s, Avg Loss: 0.024657\n",
      "  Epoch 8/10 completed in 0.97s, Avg Loss: 0.024279\n",
      "  Epoch 9/10 completed in 1.16s, Avg Loss: 0.023842\n",
      "  Epoch 10/10 completed in 0.96s, Avg Loss: 0.023549\n",
      "[17:50:43] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=10, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  10.4s\n",
      "\n",
      "[17:50:43] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 0.95s, Avg Loss: 0.238221\n",
      "  Epoch 2/15 completed in 0.94s, Avg Loss: 0.153388\n",
      "  Epoch 3/15 completed in 0.94s, Avg Loss: 0.081704\n",
      "  Epoch 4/15 completed in 0.83s, Avg Loss: 0.047568\n",
      "  Epoch 5/15 completed in 0.84s, Avg Loss: 0.035042\n",
      "  Epoch 6/15 completed in 1.05s, Avg Loss: 0.030050\n",
      "  Epoch 7/15 completed in 0.84s, Avg Loss: 0.027641\n",
      "  Epoch 8/15 completed in 1.04s, Avg Loss: 0.026339\n",
      "  Epoch 9/15 completed in 0.83s, Avg Loss: 0.025545\n",
      "  Epoch 10/15 completed in 1.04s, Avg Loss: 0.024981\n",
      "  Epoch 11/15 completed in 0.85s, Avg Loss: 0.024581\n",
      "  Epoch 12/15 completed in 1.04s, Avg Loss: 0.024275\n",
      "  Epoch 13/15 completed in 0.84s, Avg Loss: 0.024017\n",
      "  Epoch 14/15 completed in 1.05s, Avg Loss: 0.023732\n",
      "  Epoch 15/15 completed in 0.84s, Avg Loss: 0.023574\n",
      "[17:50:57] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  13.9s\n",
      "\n",
      "[17:50:57] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.94s, Avg Loss: 0.183474\n",
      "  Epoch 2/15 completed in 0.93s, Avg Loss: 0.108546\n",
      "  Epoch 3/15 completed in 0.93s, Avg Loss: 0.058961\n",
      "  Epoch 4/15 completed in 0.82s, Avg Loss: 0.038882\n",
      "  Epoch 5/15 completed in 0.84s, Avg Loss: 0.031657\n",
      "  Epoch 6/15 completed in 1.04s, Avg Loss: 0.028608\n",
      "  Epoch 7/15 completed in 0.83s, Avg Loss: 0.027102\n",
      "  Epoch 8/15 completed in 1.04s, Avg Loss: 0.026202\n",
      "  Epoch 9/15 completed in 0.84s, Avg Loss: 0.025626\n",
      "  Epoch 10/15 completed in 1.05s, Avg Loss: 0.025168\n",
      "  Epoch 11/15 completed in 0.83s, Avg Loss: 0.024869\n",
      "  Epoch 12/15 completed in 1.05s, Avg Loss: 0.024567\n",
      "  Epoch 13/15 completed in 0.83s, Avg Loss: 0.024351\n",
      "  Epoch 14/15 completed in 1.04s, Avg Loss: 0.024176\n",
      "  Epoch 15/15 completed in 0.84s, Avg Loss: 0.023901\n",
      "[17:51:11] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  13.8s\n",
      "\n",
      "[17:51:11] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 0.96s, Avg Loss: 0.191028\n",
      "  Epoch 2/15 completed in 0.94s, Avg Loss: 0.119287\n",
      "  Epoch 3/15 completed in 0.96s, Avg Loss: 0.064423\n",
      "  Epoch 4/15 completed in 0.86s, Avg Loss: 0.040651\n",
      "  Epoch 5/15 completed in 0.87s, Avg Loss: 0.032139\n",
      "  Epoch 6/15 completed in 1.07s, Avg Loss: 0.028583\n",
      "  Epoch 7/15 completed in 0.86s, Avg Loss: 0.026891\n",
      "  Epoch 8/15 completed in 1.07s, Avg Loss: 0.025865\n",
      "  Epoch 9/15 completed in 0.87s, Avg Loss: 0.025188\n",
      "  Epoch 10/15 completed in 1.06s, Avg Loss: 0.024670\n",
      "  Epoch 11/15 completed in 0.85s, Avg Loss: 0.024405\n",
      "  Epoch 12/15 completed in 1.07s, Avg Loss: 0.024010\n",
      "  Epoch 13/15 completed in 0.86s, Avg Loss: 0.023808\n",
      "  Epoch 14/15 completed in 1.07s, Avg Loss: 0.023554\n",
      "  Epoch 15/15 completed in 0.85s, Avg Loss: 0.023332\n",
      "[17:51:25] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  14.2s\n",
      "\n",
      "[17:51:25] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105066\n",
      "  Epoch 1/15 completed in 1.08s, Avg Loss: 0.227926\n",
      "  Epoch 2/15 completed in 1.07s, Avg Loss: 0.079846\n",
      "  Epoch 3/15 completed in 1.08s, Avg Loss: 0.032496\n",
      "  Epoch 4/15 completed in 0.97s, Avg Loss: 0.027701\n",
      "  Epoch 5/15 completed in 0.97s, Avg Loss: 0.026375\n",
      "  Epoch 6/15 completed in 1.18s, Avg Loss: 0.025864\n",
      "  Epoch 7/15 completed in 0.98s, Avg Loss: 0.025552\n",
      "  Epoch 8/15 completed in 1.18s, Avg Loss: 0.025091\n",
      "  Epoch 9/15 completed in 0.97s, Avg Loss: 0.024652\n",
      "  Epoch 10/15 completed in 1.18s, Avg Loss: 0.024239\n",
      "  Epoch 11/15 completed in 0.98s, Avg Loss: 0.023931\n",
      "  Epoch 12/15 completed in 1.18s, Avg Loss: 0.023658\n",
      "  Epoch 13/15 completed in 0.99s, Avg Loss: 0.023456\n",
      "  Epoch 14/15 completed in 1.18s, Avg Loss: 0.023298\n",
      "  Epoch 15/15 completed in 0.97s, Avg Loss: 0.023105\n",
      "[17:51:41] Training completed!\n",
      "\n",
      "[CV 1/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  16.0s\n",
      "\n",
      "[17:51:41] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.07s, Avg Loss: 0.195430\n",
      "  Epoch 2/15 completed in 1.07s, Avg Loss: 0.083158\n",
      "  Epoch 3/15 completed in 1.09s, Avg Loss: 0.032351\n",
      "  Epoch 4/15 completed in 0.98s, Avg Loss: 0.027238\n",
      "  Epoch 5/15 completed in 0.98s, Avg Loss: 0.026192\n",
      "  Epoch 6/15 completed in 1.18s, Avg Loss: 0.025597\n",
      "  Epoch 7/15 completed in 0.97s, Avg Loss: 0.025130\n",
      "  Epoch 8/15 completed in 1.18s, Avg Loss: 0.024848\n",
      "  Epoch 9/15 completed in 0.98s, Avg Loss: 0.024567\n",
      "  Epoch 10/15 completed in 1.18s, Avg Loss: 0.024361\n",
      "  Epoch 11/15 completed in 0.98s, Avg Loss: 0.024092\n",
      "  Epoch 12/15 completed in 1.18s, Avg Loss: 0.023902\n",
      "  Epoch 13/15 completed in 1.00s, Avg Loss: 0.023640\n",
      "  Epoch 14/15 completed in 1.19s, Avg Loss: 0.023465\n",
      "  Epoch 15/15 completed in 0.97s, Avg Loss: 0.023237\n",
      "[17:51:57] Training completed!\n",
      "\n",
      "[CV 2/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.026) total time=  16.0s\n",
      "\n",
      "[17:51:57] Starting training with parameters:\n",
      "  - Embedding size: 64\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 1024\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 105067\n",
      "  Epoch 1/15 completed in 1.09s, Avg Loss: 0.227146\n",
      "  Epoch 2/15 completed in 1.08s, Avg Loss: 0.103307\n",
      "  Epoch 3/15 completed in 1.10s, Avg Loss: 0.035651\n",
      "  Epoch 4/15 completed in 0.98s, Avg Loss: 0.027380\n",
      "  Epoch 5/15 completed in 0.98s, Avg Loss: 0.025874\n",
      "  Epoch 6/15 completed in 1.18s, Avg Loss: 0.025309\n",
      "  Epoch 7/15 completed in 0.99s, Avg Loss: 0.024877\n",
      "  Epoch 8/15 completed in 1.20s, Avg Loss: 0.024658\n",
      "  Epoch 9/15 completed in 0.97s, Avg Loss: 0.024367\n",
      "  Epoch 10/15 completed in 1.19s, Avg Loss: 0.024072\n",
      "  Epoch 11/15 completed in 0.98s, Avg Loss: 0.023818\n",
      "  Epoch 12/15 completed in 1.18s, Avg Loss: 0.023580\n",
      "  Epoch 13/15 completed in 0.99s, Avg Loss: 0.023341\n",
      "  Epoch 14/15 completed in 1.18s, Avg Loss: 0.023169\n",
      "  Epoch 15/15 completed in 0.99s, Avg Loss: 0.022955\n",
      "[17:52:14] Training completed!\n",
      "\n",
      "[CV 3/3] END batch_size=1024, dropout_rate=0.1, embedding_size=64, epochs=15, hidden_layers=[128, 64, 32], learning_rate=0.0001, optimizer_name=AdamW;, score=(train=-0.027, test=-0.027) total time=  16.1s\n",
      "\n",
      "[17:52:14] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 157600\n",
      "  Epoch 1/15 completed in 1.61s, Avg Loss: 0.092024\n",
      "  Epoch 2/15 completed in 1.62s, Avg Loss: 0.026271\n",
      "  Epoch 3/15 completed in 1.53s, Avg Loss: 0.025723\n",
      "  Epoch 4/15 completed in 1.62s, Avg Loss: 0.025429\n",
      "  Epoch 5/15 completed in 1.58s, Avg Loss: 0.025018\n",
      "  Epoch 6/15 completed in 1.62s, Avg Loss: 0.024575\n",
      "  Epoch 7/15 completed in 1.52s, Avg Loss: 0.024107\n",
      "  Epoch 8/15 completed in 1.64s, Avg Loss: 0.023695\n",
      "  Epoch 9/15 completed in 1.53s, Avg Loss: 0.023308\n",
      "  Epoch 10/15 completed in 1.62s, Avg Loss: 0.023008\n",
      "  Epoch 11/15 completed in 1.62s, Avg Loss: 0.022800\n",
      "  Epoch 12/15 completed in 1.52s, Avg Loss: 0.022669\n",
      "  Epoch 13/15 completed in 1.62s, Avg Loss: 0.022574\n",
      "  Epoch 14/15 completed in 1.62s, Avg Loss: 0.022471\n",
      "  Epoch 15/15 completed in 1.52s, Avg Loss: 0.022380\n",
      "[17:52:37] Training completed!\n",
      "\n",
      "\n",
      "Grid search completed in 37.18 minutes\n",
      "==================================================\n",
      "\n",
      "Best parameters found:\n",
      "  batch_size: 512\n",
      "  dropout_rate: 0.1\n",
      "  embedding_size: 32\n",
      "  epochs: 15\n",
      "  hidden_layers: [128, 64, 32]\n",
      "  learning_rate: 0.0001\n",
      "  optimizer_name: AdamW\n",
      "\n",
      "Best cross-validation score: 0.026764\n",
      "\n",
      "Top 5 parameter combinations:\n",
      "\n",
      "1. Score: 0.026764\n",
      "   batch_size: 512\n",
      "   dropout_rate: 0.1\n",
      "   embedding_size: 32\n",
      "   epochs: 15\n",
      "   hidden_layers: [128, 64, 32]\n",
      "   learning_rate: 0.0001\n",
      "   optimizer_name: AdamW\n",
      "\n",
      "2. Score: 0.026770\n",
      "   batch_size: 512\n",
      "   dropout_rate: 0.0\n",
      "   embedding_size: 64\n",
      "   epochs: 15\n",
      "   hidden_layers: [128, 64, 32]\n",
      "   learning_rate: 0.0001\n",
      "   optimizer_name: AdamW\n",
      "\n",
      "3. Score: 0.026783\n",
      "   batch_size: 512\n",
      "   dropout_rate: 0.0\n",
      "   embedding_size: 32\n",
      "   epochs: 10\n",
      "   hidden_layers: [64, 32]\n",
      "   learning_rate: 0.0001\n",
      "   optimizer_name: AdamW\n",
      "\n",
      "4. Score: 0.026783\n",
      "   batch_size: 512\n",
      "   dropout_rate: 0.0\n",
      "   embedding_size: 32\n",
      "   epochs: 10\n",
      "   hidden_layers: [128, 64, 32]\n",
      "   learning_rate: 0.0001\n",
      "   optimizer_name: AdamW\n",
      "\n",
      "5. Score: 0.026783\n",
      "   batch_size: 512\n",
      "   dropout_rate: 0.0\n",
      "   embedding_size: 32\n",
      "   epochs: 15\n",
      "   hidden_layers: [64, 32]\n",
      "   learning_rate: 0.0001\n",
      "   optimizer_name: AdamW\n",
      "\n",
      "Evaluating best model on test set...\n",
      "Test MSE: 0.026904\n",
      "\n",
      "Retraining best model with verbose output...\n",
      "\n",
      "[17:52:37] Starting training with parameters:\n",
      "  - Embedding size: 32\n",
      "  - Hidden layers: [128, 64, 32]\n",
      "  - Learning rate: 0.0001\n",
      "  - Batch size: 512\n",
      "  - Epochs: 15\n",
      "  - Dropout rate: 0.1\n",
      "  - Optimizer: AdamW\n",
      "  - Weight decay: 0.01\n",
      "  - Training samples: 157600\n",
      "  Epoch 1/15 completed in 1.58s, Avg Loss: 0.123976\n",
      "  Epoch 2/15 completed in 1.59s, Avg Loss: 0.026585\n",
      "  Epoch 3/15 completed in 1.57s, Avg Loss: 0.025555\n",
      "  Epoch 4/15 completed in 1.47s, Avg Loss: 0.024828\n",
      "  Epoch 5/15 completed in 1.57s, Avg Loss: 0.024229\n",
      "  Epoch 6/15 completed in 1.58s, Avg Loss: 0.023805\n",
      "  Epoch 7/15 completed in 1.58s, Avg Loss: 0.023492\n",
      "  Epoch 8/15 completed in 1.48s, Avg Loss: 0.023278\n",
      "  Epoch 9/15 completed in 1.58s, Avg Loss: 0.023130\n",
      "  Epoch 10/15 completed in 1.50s, Avg Loss: 0.022953\n",
      "  Epoch 11/15 completed in 1.58s, Avg Loss: 0.022832\n",
      "  Epoch 12/15 completed in 1.57s, Avg Loss: 0.022642\n",
      "  Epoch 13/15 completed in 1.47s, Avg Loss: 0.022548\n",
      "  Epoch 14/15 completed in 1.58s, Avg Loss: 0.022420\n",
      "  Epoch 15/15 completed in 1.58s, Avg Loss: 0.022325\n",
      "[17:53:01] Training completed!\n",
      "\n",
      "\n",
      "Making predictions on test set...\n",
      "\n",
      "All tasks completed at: 2025-05-04 17:53:01\n",
      "Total runtime: 37.57 minutes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import warnings\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# First, create a flexible neural network that can accept different architectures\n",
    "class FlexibleRecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_size=32, hidden_layers=[64, 32, 16], dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        \n",
    "        # Build flexible architecture based on hidden_layers parameter\n",
    "        layers = []\n",
    "        input_size = embedding_size * 2\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout_rate > 0:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.fc = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_vecs = self.user_embedding(user_ids)\n",
    "        item_vecs = self.item_embedding(item_ids)\n",
    "        combined = torch.cat([user_vecs, item_vecs], dim=1)\n",
    "        return self.fc(combined).squeeze()\n",
    "\n",
    "# Create a scikit-learn compatible wrapper with progress output\n",
    "class PyTorchRecommender(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, num_users, num_items, embedding_size=32, hidden_layers=[64, 32, 16], \n",
    "                 learning_rate=0.01, batch_size=1024, epochs=20, dropout_rate=0.0,\n",
    "                 optimizer_name='AdamW', weight_decay=0.01, verbose=True):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.optimizer_name = optimizer_name\n",
    "        self.weight_decay = weight_decay\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.verbose:\n",
    "            print(f\"\\n[{datetime.now().strftime('%H:%M:%S')}] Starting training with parameters:\")\n",
    "            print(f\"  - Embedding size: {self.embedding_size}\")\n",
    "            print(f\"  - Hidden layers: {self.hidden_layers}\")\n",
    "            print(f\"  - Learning rate: {self.learning_rate}\")\n",
    "            print(f\"  - Batch size: {self.batch_size}\")\n",
    "            print(f\"  - Epochs: {self.epochs}\")\n",
    "            print(f\"  - Dropout rate: {self.dropout_rate}\")\n",
    "            print(f\"  - Optimizer: {self.optimizer_name}\")\n",
    "            print(f\"  - Weight decay: {self.weight_decay}\")\n",
    "            print(f\"  - Training samples: {len(X)}\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        # Create data loader\n",
    "        dataset = TensorDataset(X_tensor[:, 0], X_tensor[:, 1], y_tensor)\n",
    "        train_loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model_ = FlexibleRecommenderNet(\n",
    "            self.num_users, \n",
    "            self.num_items, \n",
    "            self.embedding_size, \n",
    "            self.hidden_layers,\n",
    "            self.dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Choose optimizer\n",
    "        if self.optimizer_name == 'AdamW':\n",
    "            optimizer = optim.AdamW(self.model_.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        elif self.optimizer_name == 'Adam':\n",
    "            optimizer = optim.Adam(self.model_.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_name == 'SGD':\n",
    "            optimizer = optim.SGD(self.model_.parameters(), lr=self.learning_rate, momentum=0.9)\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training loop with progress output\n",
    "        self.model_.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_start_time = time.time()\n",
    "            total_loss = 0\n",
    "            num_batches = len(train_loader)\n",
    "            \n",
    "            for batch_idx, (user_batch, item_batch, rating_batch) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model_(user_batch, item_batch)\n",
    "                loss = criterion(outputs, rating_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Progress output every 10 batches or last batch\n",
    "                if self.verbose and (batch_idx % 10 == 0 or batch_idx == num_batches - 1):\n",
    "                    print(f\"  Epoch {epoch+1}/{self.epochs}, Batch {batch_idx+1}/{num_batches}, \"\n",
    "                          f\"Loss: {loss.item():.6f}\", end='\\r')\n",
    "            \n",
    "            epoch_time = time.time() - epoch_start_time\n",
    "            avg_loss = total_loss / num_batches\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"  Epoch {epoch+1}/{self.epochs} completed in {epoch_time:.2f}s, \"\n",
    "                      f\"Avg Loss: {avg_loss:.6f}\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"[{datetime.now().strftime('%H:%M:%S')}] Training completed!\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "            outputs = self.model_(X_tensor[:, 0], X_tensor[:, 1])\n",
    "            return (outputs > 0.5).numpy().astype(int)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "            outputs = self.model_(X_tensor[:, 0], X_tensor[:, 1]).numpy()\n",
    "            # Return probabilities for both classes\n",
    "            return np.vstack([1 - outputs, outputs]).T\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        # Return negative MSE as score (sklearn expects higher is better)\n",
    "        self.model_.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.long)\n",
    "            y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "            outputs = self.model_(X_tensor[:, 0], X_tensor[:, 1])\n",
    "            mse = nn.MSELoss()(outputs, y_tensor).item()\n",
    "            return -mse  # Negative MSE so higher is better\n",
    "\n",
    "# Calculate total number of parameter combinations\n",
    "def calculate_total_combinations(param_grid):\n",
    "    total = 1\n",
    "    for values in param_grid.values():\n",
    "        total *= len(values)\n",
    "    return total\n",
    "\n",
    "# Define parameter grid for hyperparameter search\n",
    "param_grid = {\n",
    "    'embedding_size': [32, 64],\n",
    "    'hidden_layers': [\n",
    "        [64, 32],           # 2 layers\n",
    "        [128, 64, 32],      # 3 layers  \n",
    "    ],\n",
    "    'learning_rate': [0.0001],\n",
    "    'batch_size': [512, 1024],\n",
    "    'epochs': [10, 15],\n",
    "    'dropout_rate': [0.0, 0.1],\n",
    "    'optimizer_name': ['AdamW'],\n",
    "}\n",
    "\n",
    "total_combinations = calculate_total_combinations(param_grid)\n",
    "print(f\"Total parameter combinations to test: {total_combinations}\")\n",
    "print(f\"With 3-fold CV, total models to train: {total_combinations * 3}\")\n",
    "print(f\"Estimated time: {total_combinations * 3 * 0.5:.0f}-{total_combinations * 3 * 2:.0f} minutes\")\n",
    "print(\"\\nStarting grid search...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize the model wrapper\n",
    "base_model = PyTorchRecommender(\n",
    "    num_users=len(user2idx),\n",
    "    num_items=len(basket2idx),\n",
    "    verbose=True  # Set to False to reduce output clutter during grid search\n",
    ")\n",
    "\n",
    "# Create GridSearchCV object with more verbose output\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=base_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Use negative MSE\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    n_jobs=1,  # Use all available cores\n",
    "    verbose=3,  # Increased verbosity\n",
    "    refit=True,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Track grid search progress\n",
    "start_time = time.time()\n",
    "print(f\"Grid search started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Fit grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Calculate total time\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nGrid search completed in {total_time/60:.2f} minutes\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Print best parameters\n",
    "print(\"\\nBest parameters found:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "print(f\"\\nBest cross-validation score: {-grid_search.best_score_:.6f}\")\n",
    "\n",
    "# Show top 5 parameter combinations\n",
    "print(\"\\nTop 5 parameter combinations:\")\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "top_5_indices = results_df['mean_test_score'].nlargest(5).index\n",
    "for i, idx in enumerate(top_5_indices):\n",
    "    print(f\"\\n{i+1}. Score: {-results_df.loc[idx, 'mean_test_score']:.6f}\")\n",
    "    params = results_df.loc[idx, 'params']\n",
    "    for param, value in params.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating best model on test set...\")\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(f\"Test MSE: {-test_score:.6f}\")\n",
    "\n",
    "# Train final model with verbose output\n",
    "print(\"\\nRetraining best model with verbose output...\")\n",
    "best_params = grid_search.best_params_.copy()\n",
    "best_params['verbose'] = True  # Enable verbose output for final training\n",
    "\n",
    "final_model = PyTorchRecommender(\n",
    "    num_users=len(user2idx),\n",
    "    num_items=len(basket2idx),\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with progress\n",
    "print(\"\\nMaking predictions on test set...\")\n",
    "predictions = final_model.predict(X_test)\n",
    "probabilities = final_model.predict_proba(X_test)\n",
    "\n",
    "print(f\"\\nAll tasks completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Total runtime: {(time.time() - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9175b0a6-3880-4ed8-a43d-7f3c8063b3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating best model from grid search...\n",
      "\n",
      "----- EVALUATION RESULTS -----\n",
      "\n",
      "Metrics at k=2:\n",
      "Precision@2: 0.3875\n",
      "Recall@2: 0.1502\n",
      "F1@2: 0.2143\n",
      "\n",
      "Metrics at k=5:\n",
      "Precision@5: 0.2900\n",
      "Recall@5: 0.2789\n",
      "F1@5: 0.2803\n",
      "\n",
      "Metrics at k=10:\n",
      "Precision@10: 0.2360\n",
      "Recall@10: 0.4451\n",
      "F1@10: 0.3044\n",
      "\n",
      "----- EVALUATION RESULTS -----\n",
      "\n",
      "Metrics at k=2:\n",
      "Precision@2: 0.3875\n",
      "Recall@2: 0.1502\n",
      "F1@2: 0.2143\n",
      "\n",
      "Metrics at k=5:\n",
      "Precision@5: 0.2900\n",
      "Recall@5: 0.2789\n",
      "F1@5: 0.2803\n",
      "\n",
      "Metrics at k=10:\n",
      "Precision@10: 0.2360\n",
      "Recall@10: 0.4451\n",
      "F1@10: 0.3044\n"
     ]
    }
   ],
   "source": [
    "def evaluate_recommendations(model, user_ids=None, top_n_values=[2, 5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate the model using precision, recall, and F1 score\n",
    "    for multiple values of top_n recommendations\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_baskets = list(basket2idx.keys())\n",
    "    \n",
    "    # Use all test users if no specific users are provided\n",
    "    if user_ids is None:\n",
    "        user_ids = test_data['user_id'].unique()\n",
    "    \n",
    "    # Dictionaries to store metrics\n",
    "    precision_at_k = collections.defaultdict(list)\n",
    "    recall_at_k = collections.defaultdict(list)\n",
    "    f1_at_k = collections.defaultdict(list)\n",
    "    \n",
    "    for user_id in user_ids:\n",
    "        # Skip if user is not in test set\n",
    "        if user_id not in test_data['user_id'].unique():\n",
    "            continue\n",
    "            \n",
    "        # Get user index\n",
    "        user_idx = user2idx[user_id]\n",
    "        \n",
    "        # Get baskets from training set (to exclude)\n",
    "        if user_id in train_data['user_id'].unique():\n",
    "            train_baskets = train_data[train_data['user_id'] == user_id]['basket_name'].unique()\n",
    "        else:\n",
    "            train_baskets = []\n",
    "            \n",
    "        # Get ground truth baskets from test set\n",
    "        test_baskets = test_data[(test_data['user_id'] == user_id) & (test_data['binary_rating'] == 1)]['basket_name'].unique() \n",
    "        \n",
    "        # Skip if no test baskets\n",
    "        if len(test_baskets) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get unseen baskets (all baskets minus training baskets)\n",
    "        basket_indices = torch.tensor([basket2idx[b] for b in all_baskets])\n",
    "        user_tensor = torch.tensor([user_idx] * len(basket_indices))\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            preds = model(user_tensor, basket_indices)\n",
    "        \n",
    "        # Evaluate for different k values\n",
    "        for k in top_n_values:\n",
    "            # Get top-k recommendations\n",
    "            top_indices = torch.topk(preds, k).indices.numpy()\n",
    "            recommended_baskets = [all_baskets[i] for i in top_indices]\n",
    "            \n",
    "            # Calculate true positives (recommendations that are in test set)\n",
    "            true_positives = len(set(recommended_baskets) & set(test_baskets))\n",
    "            \n",
    "            # Calculate metrics\n",
    "            precision = true_positives / k\n",
    "            recall = true_positives / len(test_baskets)\n",
    "            f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "            \n",
    "            # Store metrics\n",
    "            precision_at_k[k].append(precision)\n",
    "            recall_at_k[k].append(recall)\n",
    "            f1_at_k[k].append(f1)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    print(\"\\n----- EVALUATION RESULTS -----\")\n",
    "    for k in top_n_values:\n",
    "        if not precision_at_k[k]:\n",
    "            print(f\"No data for k={k}\")\n",
    "            continue\n",
    "            \n",
    "        avg_precision = np.mean(precision_at_k[k])\n",
    "        avg_recall = np.mean(recall_at_k[k])\n",
    "        avg_f1 = np.mean(f1_at_k[k])\n",
    "        \n",
    "        print(f\"\\nMetrics at k={k}:\")\n",
    "        print(f\"Precision@{k}: {avg_precision:.4f}\")\n",
    "        print(f\"Recall@{k}: {avg_recall:.4f}\")\n",
    "        print(f\"F1@{k}: {avg_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'precision': precision_at_k,\n",
    "        'recall': recall_at_k,\n",
    "        'f1': f1_at_k\n",
    "    }\n",
    "\n",
    "# After grid search is complete, get the best model\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_pytorch_model = best_estimator.model_  # Access the PyTorch model from the sklearn wrapper\n",
    "\n",
    "# Evaluate using the best model\n",
    "print(\"\\nEvaluating best model from grid search...\")\n",
    "metrics = evaluate_recommendations(best_pytorch_model, top_n_values=[2, 5, 10])\n",
    "\n",
    "# Or alternatively, you can create a wrapper function\n",
    "def evaluate_best_model(grid_search_results, user_ids=None, top_n_values=[2, 5, 10]):\n",
    "    \"\"\"\n",
    "    Evaluate the best model from grid search\n",
    "    \"\"\"\n",
    "    # Get the best PyTorch model from the grid search\n",
    "    best_estimator = grid_search_results.best_estimator_\n",
    "    best_pytorch_model = best_estimator.model_\n",
    "    \n",
    "    # Evaluate\n",
    "    return evaluate_recommendations(best_pytorch_model, user_ids, top_n_values)\n",
    "\n",
    "# Use it like this:\n",
    "metrics = evaluate_best_model(grid_search, top_n_values=[2, 5, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "05f3e1cf-90aa-4ffa-9f4a-c87b4dea521a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.3181, Val Loss: 0.3044\n",
      "Epoch [10/10], Train Loss: 0.2721, Val Loss: 0.3509\n",
      "\n",
      "Recommendations for user user_123:\n",
      "1. German and French companies (Score: 0.500)\n",
      "   Top features:\n",
      "2. Techs seeking value upgrade (Score: 0.500)\n",
      "   Top features:\n",
      "3. Most traded stocks (Score: 0.500)\n",
      "   Top features:\n",
      "4. Spanish companies (Score: 0.500)\n",
      "   Top features:\n",
      "5. medical equipment medium (Score: 0.500)\n",
      "   Top features:\n",
      "6. Software in Germany (Score: 0.500)\n",
      "   Top features:\n",
      "7. Financial World  nu funds (Score: 0.500)\n",
      "   Top features:\n",
      "8. Pers Choice Europé (Score: 0.500)\n",
      "   Top features:\n",
      "9. Swedish climbers (Score: 0.500)\n",
      "   Top features:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HybridRecommenderNet(nn.Module):\n",
    "    def __init__(self, num_users, num_items, num_content_features, \n",
    "                 embedding_size=32, hidden_layers=[128, 64, 32], dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # User and item embeddings\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_size)\n",
    "        \n",
    "        # Content feature processing - reduce from 148 features to embedding_size\n",
    "        self.content_fc = nn.Sequential(\n",
    "            nn.Linear(num_content_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, embedding_size)\n",
    "        )\n",
    "        \n",
    "        # Combined feature size: user + item + content embeddings\n",
    "        input_size = embedding_size * 3\n",
    "        \n",
    "        # Build the main network\n",
    "        layers = []\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.fc = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                nn.init.normal_(module.weight, std=0.1)\n",
    "    \n",
    "    def forward(self, user_ids, item_ids, content_features):\n",
    "        user_vecs = self.user_embedding(user_ids)\n",
    "        item_vecs = self.item_embedding(item_ids)\n",
    "        content_vecs = self.content_fc(content_features)\n",
    "        \n",
    "        combined = torch.cat([user_vecs, item_vecs, content_vecs], dim=1)\n",
    "        return self.fc(combined).squeeze()\n",
    "\n",
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, user_ids, item_ids, content_features, labels):\n",
    "        self.user_ids = torch.tensor(user_ids, dtype=torch.long)\n",
    "        self.item_ids = torch.tensor(item_ids, dtype=torch.long)\n",
    "        self.content_features = torch.tensor(content_features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.user_ids[idx], \n",
    "                self.item_ids[idx], \n",
    "                self.content_features[idx], \n",
    "                self.labels[idx])\n",
    "\n",
    "def prepare_hybrid_data(investments_df, basket_features_df):\n",
    "    \"\"\"\n",
    "    Prepare data for the hybrid model\n",
    "    \"\"\"\n",
    "    # Create user and basket index mappings\n",
    "    user2idx = {user: idx for idx, user in enumerate(investments_df['user_id'].unique())}\n",
    "    basket2idx = {basket: idx for idx, basket in enumerate(basket_features_df['basket_name'].unique())}\n",
    "    \n",
    "    # Create feature matrix for baskets (excluding basket_name column)\n",
    "    feature_cols = basket_features_df.columns[1:]  # Skip basket_name\n",
    "    basket_feature_matrix = basket_features_df[feature_cols].values\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    basket_feature_matrix = scaler.fit_transform(basket_feature_matrix)\n",
    "    \n",
    "    # Create basket name to feature vector mapping\n",
    "    basket_to_features = {}\n",
    "    for idx, basket_name in enumerate(basket_features_df['basket_name']):\n",
    "        basket_to_features[basket_name] = basket_feature_matrix[idx]\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_users = []\n",
    "    X_items = []\n",
    "    X_content = []\n",
    "    y = []\n",
    "    \n",
    "    # Add positive examples (user-basket pairs from investments)\n",
    "    for _, row in investments_df.iterrows():\n",
    "        if row['basket_name'] in basket2idx:  # Only if basket exists in features\n",
    "            user_idx = user2idx[row['user_id']]\n",
    "            basket_idx = basket2idx[row['basket_name']]\n",
    "            content_features = basket_to_features[row['basket_name']]\n",
    "            \n",
    "            X_users.append(user_idx)\n",
    "            X_items.append(basket_idx)\n",
    "            X_content.append(content_features)\n",
    "            y.append(1)  # Positive example\n",
    "    \n",
    "    # Add negative examples\n",
    "    all_baskets = list(basket2idx.keys())\n",
    "    for _, row in investments_df.iterrows():\n",
    "        if row['basket_name'] in basket2idx:\n",
    "            user_idx = user2idx[row['user_id']]\n",
    "            \n",
    "            # Sample negative baskets for this user\n",
    "            invested_baskets = investments_df[investments_df['user_id'] == row['user_id']]['basket_name'].tolist()\n",
    "            eligible_baskets = [b for b in all_baskets if b not in invested_baskets]\n",
    "            \n",
    "            if len(eligible_baskets) > 0:\n",
    "                # Sample 3 negative examples for each positive\n",
    "                neg_samples = np.random.choice(eligible_baskets, size=min(3, len(eligible_baskets)), replace=False)\n",
    "                \n",
    "                for neg_basket in neg_samples:\n",
    "                    neg_idx = basket2idx[neg_basket]\n",
    "                    content_features = basket_to_features[neg_basket]\n",
    "                    \n",
    "                    X_users.append(user_idx)\n",
    "                    X_items.append(neg_idx)\n",
    "                    X_content.append(content_features)\n",
    "                    y.append(0)  # Negative example\n",
    "    \n",
    "    return (np.array(X_users), np.array(X_items), np.array(X_content), np.array(y), \n",
    "            user2idx, basket2idx, basket_to_features, scaler)\n",
    "\n",
    "def train_hybrid_model(model, train_loader, val_loader, epochs=10, lr=0.0005):\n",
    "    \"\"\"\n",
    "    Train the hybrid model\n",
    "    \"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            user_ids, item_ids, content_features, labels = batch\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(user_ids, item_ids, content_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                user_ids, item_ids, content_features, labels = batch\n",
    "                outputs = model(user_ids, item_ids, content_features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "class HybridRecommender:\n",
    "    def __init__(self, model, user2idx, basket2idx, basket_to_features, scaler):\n",
    "        self.model = model\n",
    "        self.user2idx = user2idx\n",
    "        self.basket2idx = basket2idx\n",
    "        self.idx2basket = {v: k for k, v in basket2idx.items()}\n",
    "        self.basket_to_features = basket_to_features\n",
    "        self.scaler = scaler\n",
    "    \n",
    "    def recommend(self, user_id, investments_df, n_recommendations=10):\n",
    "        \"\"\"\n",
    "        Generate recommendations for a user\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Handle new users\n",
    "        if user_id not in self.user2idx:\n",
    "            return self.recommend_for_new_user(n_recommendations)\n",
    "        \n",
    "        user_idx = self.user2idx[user_id]\n",
    "        \n",
    "        # Get user's existing investments\n",
    "        user_investments = investments_df[investments_df['user_id'] == user_id]['basket_name'].values\n",
    "        \n",
    "        # Get all baskets that user hasn't invested in\n",
    "        all_baskets = list(self.basket2idx.keys())\n",
    "        candidate_baskets = [b for b in all_baskets if b not in user_investments]\n",
    "        \n",
    "        if not candidate_baskets:\n",
    "            return []\n",
    "        \n",
    "        # Prepare data for prediction\n",
    "        user_ids = np.array([user_idx] * len(candidate_baskets))\n",
    "        item_ids = np.array([self.basket2idx[b] for b in candidate_baskets])\n",
    "        content_features = np.array([self.basket_to_features[b] for b in candidate_baskets])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        user_ids_tensor = torch.tensor(user_ids, dtype=torch.long)\n",
    "        item_ids_tensor = torch.tensor(item_ids, dtype=torch.long)\n",
    "        content_features_tensor = torch.tensor(content_features, dtype=torch.float32)\n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            scores = self.model(user_ids_tensor, item_ids_tensor, content_features_tensor).numpy()\n",
    "        \n",
    "        # Create recommendations with scores\n",
    "        recommendations = []\n",
    "        for i, basket in enumerate(candidate_baskets):\n",
    "            recommendations.append({\n",
    "                'basket': basket,\n",
    "                'score': float(scores[i]),\n",
    "                'features': self._get_top_features(basket)\n",
    "            })\n",
    "        \n",
    "        # Sort by score\n",
    "        recommendations.sort(key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        return recommendations[:n_recommendations]\n",
    "    \n",
    "    def recommend_for_new_user(self, n_recommendations=10):\n",
    "        \"\"\"\n",
    "        Recommend baskets for new users based on content popularity\n",
    "        \"\"\"\n",
    "        # Simple popularity-based approach for cold start\n",
    "        popular_baskets = []\n",
    "        \n",
    "        # You could implement a more sophisticated approach here\n",
    "        # For example, diversify based on different feature categories\n",
    "        \n",
    "        # Get baskets with diverse characteristics\n",
    "        baskets_by_volatility = {}\n",
    "        for basket, features in self.basket_to_features.items():\n",
    "            # Get volatility features (last 3 columns in your case)\n",
    "            volatility_features = features[-3:]\n",
    "            volatility_type = np.argmax(volatility_features)\n",
    "            \n",
    "            if volatility_type not in baskets_by_volatility:\n",
    "                baskets_by_volatility[volatility_type] = []\n",
    "            baskets_by_volatility[volatility_type].append(basket)\n",
    "        \n",
    "        # Sample from each volatility type\n",
    "        recommendations = []\n",
    "        for volatility_type, baskets in baskets_by_volatility.items():\n",
    "            sample_size = min(n_recommendations // 3, len(baskets))\n",
    "            sampled = np.random.choice(baskets, size=sample_size, replace=False)\n",
    "            for basket in sampled:\n",
    "                recommendations.append({\n",
    "                    'basket': basket,\n",
    "                    'score': 0.5,  # Default score for new users\n",
    "                    'features': self._get_top_features(basket)\n",
    "                })\n",
    "        \n",
    "        return recommendations[:n_recommendations]\n",
    "    \n",
    "    def _get_top_features(self, basket_name):\n",
    "        \"\"\"\n",
    "        Get the top features for a basket for interpretability\n",
    "        \"\"\"\n",
    "        features = self.basket_to_features[basket_name]\n",
    "        \n",
    "        # Get the original feature names from basket_features columns\n",
    "        feature_names = ['industry_name_Advanced Medical Equipment & Technology', \n",
    "                        'industry_name_Advertising & Marketing', ...]  # Use actual column names\n",
    "        \n",
    "        # Find top 5 features\n",
    "        top_indices = np.argsort(np.abs(features))[-5:]\n",
    "        \n",
    "        top_features = []\n",
    "        for idx in top_indices:\n",
    "            if idx < len(feature_names):\n",
    "                top_features.append({\n",
    "                    'feature': feature_names[idx],\n",
    "                    'value': float(features[idx])\n",
    "                })\n",
    "        \n",
    "        return top_features\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    investments_df = pd.read_csv('syntheticDataGenerators/investment/invest_data.csv', sep=';')    \n",
    "    basket_features_df = pd.read_csv('basket_features.csv')\n",
    "    \n",
    "    # Prepare data\n",
    "    (X_users, X_items, X_content, y, \n",
    "     user2idx, basket2idx, basket_to_features, scaler) = prepare_hybrid_data(investments_df, basket_features_df)\n",
    "    \n",
    "    # Split data\n",
    "    train_indices, val_indices = train_test_split(range(len(y)), test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = HybridDataset(X_users[train_indices], X_items[train_indices], \n",
    "                                  X_content[train_indices], y[train_indices])\n",
    "    val_dataset = HybridDataset(X_users[val_indices], X_items[val_indices], \n",
    "                                X_content[val_indices], y[val_indices])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_users = len(user2idx)\n",
    "    num_items = len(basket2idx)\n",
    "    num_content_features = basket_features_df.shape[1] - 1  # Exclude basket_name column\n",
    "    \n",
    "    model = HybridRecommenderNet(\n",
    "        num_users=num_users,\n",
    "        num_items=num_items,\n",
    "        num_content_features=num_content_features,\n",
    "        embedding_size=32,\n",
    "        hidden_layers=[128, 64, 32],\n",
    "        dropout_rate=0.2\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    train_losses, val_losses = train_hybrid_model(model, train_loader, val_loader, epochs=10)\n",
    "    \n",
    "    # Create recommender\n",
    "    recommender = HybridRecommender(model, user2idx, basket2idx, basket_to_features, scaler)\n",
    "    \n",
    "    # Example usage\n",
    "    user_id = 'user_123'\n",
    "    recommendations = recommender.recommend(user_id, investments_df, n_recommendations=10)\n",
    "    \n",
    "    print(f\"\\nRecommendations for user {user_id}:\")\n",
    "    for i, rec in enumerate(recommendations):\n",
    "        print(f\"{i+1}. {rec['basket']} (Score: {rec['score']:.3f})\")\n",
    "        print(\"   Top features:\")\n",
    "        for feat in rec['features']:\n",
    "            print(f\"   - {feat['feature']}: {feat['value']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cebcfd-96dd-416d-a6a3-0f2606964797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
