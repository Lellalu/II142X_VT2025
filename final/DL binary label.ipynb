{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c241089-ca0a-46c3-a3a0-74ef288c6af5",
   "metadata": {},
   "source": [
    "# All user in training, but only 80% of purchase history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f93599a-92d8-4e3b-b5bd-90852eab0326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Grid Search...\n",
      "Total combinations to try: 9\n",
      "\n",
      "Combination 1/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32], 'learning_rate': 0.01, 'num_epochs': 30}\n",
      "Architecture: 1 hidden layers with sizes [32]\n",
      "Epoch [5/30], Train Loss: 3.5096, Val Loss: 3.5388, Accuracy: 0.1229\n",
      "Epoch [10/30], Train Loss: 3.4063, Val Loss: 3.4425, Accuracy: 0.1211\n",
      "Epoch [15/30], Train Loss: 3.3627, Val Loss: 3.4209, Accuracy: 0.1101\n",
      "Epoch [20/30], Train Loss: 3.3388, Val Loss: 3.4049, Accuracy: 0.1216\n",
      "Epoch [25/30], Train Loss: 3.3190, Val Loss: 3.4542, Accuracy: 0.1154\n",
      "Epoch [30/30], Train Loss: 3.3151, Val Loss: 3.3889, Accuracy: 0.1250\n",
      "New best model found! Validation Loss: 3.3889, Accuracy: 0.1250\n",
      "\n",
      "Combination 2/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32], 'learning_rate': 0.005, 'num_epochs': 30}\n",
      "Architecture: 1 hidden layers with sizes [32]\n",
      "Epoch [5/30], Train Loss: 3.4964, Val Loss: 3.5738, Accuracy: 0.1242\n",
      "Epoch [10/30], Train Loss: 3.3819, Val Loss: 3.4924, Accuracy: 0.1133\n",
      "Epoch [15/30], Train Loss: 3.3276, Val Loss: 3.4411, Accuracy: 0.1169\n",
      "Epoch [20/30], Train Loss: 3.2917, Val Loss: 3.4143, Accuracy: 0.1114\n",
      "Epoch [25/30], Train Loss: 3.2713, Val Loss: 3.3875, Accuracy: 0.1201\n",
      "Epoch [30/30], Train Loss: 3.2528, Val Loss: 3.3793, Accuracy: 0.1109\n",
      "New best model found! Validation Loss: 3.3793, Accuracy: 0.1109\n",
      "\n",
      "Combination 3/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32], 'learning_rate': 0.002, 'num_epochs': 30}\n",
      "Architecture: 1 hidden layers with sizes [32]\n",
      "Epoch [5/30], Train Loss: 3.6153, Val Loss: 3.6801, Accuracy: 0.1195\n",
      "Epoch [10/30], Train Loss: 3.5052, Val Loss: 3.6005, Accuracy: 0.1248\n",
      "Epoch [15/30], Train Loss: 3.4363, Val Loss: 3.5387, Accuracy: 0.1331\n",
      "Epoch [20/30], Train Loss: 3.3874, Val Loss: 3.5167, Accuracy: 0.1263\n",
      "Epoch [25/30], Train Loss: 3.3574, Val Loss: 3.4845, Accuracy: 0.1282\n",
      "Epoch [30/30], Train Loss: 3.3344, Val Loss: 3.4739, Accuracy: 0.1261\n",
      "\n",
      "Combination 4/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 32], 'learning_rate': 0.01, 'num_epochs': 30}\n",
      "Architecture: 2 hidden layers with sizes [32, 32]\n",
      "Epoch [5/30], Train Loss: 3.5253, Val Loss: 3.5251, Accuracy: 0.1140\n",
      "Epoch [10/30], Train Loss: 3.4100, Val Loss: 3.4519, Accuracy: 0.1201\n",
      "Epoch [15/30], Train Loss: 3.3660, Val Loss: 3.4006, Accuracy: 0.1180\n",
      "Epoch [20/30], Train Loss: 3.3366, Val Loss: 3.3788, Accuracy: 0.1203\n",
      "Epoch [25/30], Train Loss: 3.3182, Val Loss: 3.4142, Accuracy: 0.1154\n",
      "Epoch [30/30], Train Loss: 3.3180, Val Loss: 3.3478, Accuracy: 0.1120\n",
      "New best model found! Validation Loss: 3.3478, Accuracy: 0.1120\n",
      "\n",
      "Combination 5/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 32], 'learning_rate': 0.005, 'num_epochs': 30}\n",
      "Architecture: 2 hidden layers with sizes [32, 32]\n",
      "Epoch [5/30], Train Loss: 3.5106, Val Loss: 3.5174, Accuracy: 0.1208\n",
      "Epoch [10/30], Train Loss: 3.3807, Val Loss: 3.4192, Accuracy: 0.1248\n",
      "Epoch [15/30], Train Loss: 3.3189, Val Loss: 3.3638, Accuracy: 0.1164\n",
      "Epoch [20/30], Train Loss: 3.2779, Val Loss: 3.3410, Accuracy: 0.1177\n",
      "Epoch [25/30], Train Loss: 3.2496, Val Loss: 3.3147, Accuracy: 0.1195\n",
      "Epoch [30/30], Train Loss: 3.2277, Val Loss: 3.3093, Accuracy: 0.1138\n",
      "New best model found! Validation Loss: 3.3093, Accuracy: 0.1138\n",
      "\n",
      "Combination 6/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 32], 'learning_rate': 0.002, 'num_epochs': 30}\n",
      "Architecture: 2 hidden layers with sizes [32, 32]\n",
      "Epoch [5/30], Train Loss: 3.6205, Val Loss: 3.6241, Accuracy: 0.1154\n",
      "Epoch [10/30], Train Loss: 3.4587, Val Loss: 3.4926, Accuracy: 0.1198\n",
      "Epoch [15/30], Train Loss: 3.3641, Val Loss: 3.4306, Accuracy: 0.1185\n",
      "Epoch [20/30], Train Loss: 3.3050, Val Loss: 3.3897, Accuracy: 0.1112\n",
      "Epoch [25/30], Train Loss: 3.2585, Val Loss: 3.3499, Accuracy: 0.1256\n",
      "Epoch [30/30], Train Loss: 3.2242, Val Loss: 3.3295, Accuracy: 0.1138\n",
      "\n",
      "Combination 7/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.01, 'num_epochs': 30}\n",
      "Architecture: 3 hidden layers with sizes [32, 64, 32]\n",
      "Epoch [5/30], Train Loss: 3.5500, Val Loss: 3.6142, Accuracy: 0.1151\n",
      "Epoch [10/30], Train Loss: 3.4639, Val Loss: 3.5159, Accuracy: 0.1148\n",
      "Epoch [15/30], Train Loss: 3.4181, Val Loss: 3.4741, Accuracy: 0.1075\n",
      "Epoch [20/30], Train Loss: 3.3894, Val Loss: 3.4793, Accuracy: 0.1164\n",
      "Epoch [25/30], Train Loss: 3.3707, Val Loss: 3.4657, Accuracy: 0.1167\n",
      "Epoch [30/30], Train Loss: 3.3670, Val Loss: 3.4615, Accuracy: 0.1154\n",
      "\n",
      "Combination 8/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.005, 'num_epochs': 30}\n",
      "Architecture: 3 hidden layers with sizes [32, 64, 32]\n",
      "Epoch [5/30], Train Loss: 3.5485, Val Loss: 3.5574, Accuracy: 0.1258\n",
      "Epoch [10/30], Train Loss: 3.4394, Val Loss: 3.4147, Accuracy: 0.1222\n",
      "Epoch [15/30], Train Loss: 3.3681, Val Loss: 3.3900, Accuracy: 0.1258\n",
      "Epoch [20/30], Train Loss: 3.3224, Val Loss: 3.3606, Accuracy: 0.1222\n",
      "Epoch [25/30], Train Loss: 3.2856, Val Loss: 3.3143, Accuracy: 0.1331\n",
      "Epoch [30/30], Train Loss: 3.2599, Val Loss: 3.3100, Accuracy: 0.1222\n",
      "\n",
      "Combination 9/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.002, 'num_epochs': 30}\n",
      "Architecture: 3 hidden layers with sizes [32, 64, 32]\n",
      "Epoch [5/30], Train Loss: 3.5631, Val Loss: 3.5930, Accuracy: 0.1072\n",
      "Epoch [10/30], Train Loss: 3.4276, Val Loss: 3.4884, Accuracy: 0.1180\n",
      "Epoch [15/30], Train Loss: 3.3310, Val Loss: 3.4061, Accuracy: 0.1269\n",
      "Epoch [20/30], Train Loss: 3.2612, Val Loss: 3.3717, Accuracy: 0.1300\n",
      "Epoch [25/30], Train Loss: 3.2093, Val Loss: 3.3390, Accuracy: 0.1248\n",
      "Epoch [30/30], Train Loss: 3.1743, Val Loss: 3.3019, Accuracy: 0.1271\n",
      "New best model found! Validation Loss: 3.3019, Accuracy: 0.1271\n",
      "\n",
      "==================================================\n",
      "Grid Search Complete!\n",
      "Best Validation Loss: 3.3019\n",
      "Best Parameters:\n",
      "  batch_size: 32\n",
      "  hidden_sizes: [32, 64, 32]\n",
      "  learning_rate: 0.002\n",
      "  num_epochs: 30\n",
      "\n",
      "Training final model with best parameters:\n",
      "Architecture: 3 hidden layers with sizes [32, 64, 32]\n",
      "Epoch [2/30], Loss: 3.7583\n",
      "Epoch [4/30], Loss: 3.5810\n",
      "Epoch [6/30], Loss: 3.5097\n",
      "Epoch [8/30], Loss: 3.4548\n",
      "Epoch [10/30], Loss: 3.4033\n",
      "Epoch [12/30], Loss: 3.3623\n",
      "Epoch [14/30], Loss: 3.3290\n",
      "Epoch [16/30], Loss: 3.3020\n",
      "Epoch [18/30], Loss: 3.2737\n",
      "Epoch [20/30], Loss: 3.2495\n",
      "Epoch [22/30], Loss: 3.2294\n",
      "Epoch [24/30], Loss: 3.2098\n",
      "Epoch [26/30], Loss: 3.1948\n",
      "Epoch [28/30], Loss: 3.1764\n",
      "Epoch [30/30], Loss: 3.1631\n",
      "\n",
      "Model training complete and saved to 'best_recommendation_model.pth'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# 1. Load the data - reusing your existing code\n",
    "x_train = pd.read_csv('X_train.csv', delimiter=',')\n",
    "y_train = pd.read_csv('y_train.csv', delimiter=',')\n",
    "basket_features = pd.read_csv('../basket_features.csv')\n",
    "\n",
    "# Initialize encoders dictionary\n",
    "encoders = {}\n",
    "\n",
    "# Merge the data\n",
    "merged_data = pd.merge(x_train, y_train, on='user_id')\n",
    "\n",
    "# Encode features directly in the merged DataFrame\n",
    "for col in ['location', 'gender', 'education', 'invest_goal', 'age_group']:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(merged_data[col])\n",
    "    merged_data[f'{col}_encoded'] = encoder.transform(merged_data[col])\n",
    "    encoders[col] = encoder\n",
    "\n",
    "# Encode basket names\n",
    "all_basket_names = basket_features['basket_name'].unique()\n",
    "basket_encoder = LabelEncoder()\n",
    "basket_encoder.fit(all_basket_names)\n",
    "num_baskets = len(basket_encoder.classes_)\n",
    "merged_data['basket_encoded'] = basket_encoder.transform(merged_data['basket_name'])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = merged_data[[f'{col}_encoded' for col in encoders.keys()]].values\n",
    "y = merged_data['basket_encoded'].values\n",
    "\n",
    "# Dataset class - reusing your existing code\n",
    "class RecommendationDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), self.y[idx]\n",
    "\n",
    "# Updated MLP model to support variable hidden layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Create a list of layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer to first hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Create additional hidden layers\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        # Create a sequential container\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Updated function to train and validate model with specific parameters\n",
    "def train_and_validate(X, y, hidden_sizes, learning_rate, batch_size, num_epochs):\n",
    "    # Create dataset\n",
    "    dataset = RecommendationDataset(X, y)\n",
    "    \n",
    "    # Split into train/validation\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = X.shape[1]\n",
    "    output_size = num_baskets\n",
    "    model = MLP(input_size, hidden_sizes, output_size)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # Print progress every few epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return model, avg_val_loss, accuracy\n",
    "\n",
    "# Updated Grid Search to support neural network architectures\n",
    "def grid_search(param_grid):\n",
    "    # Create all parameter combinations\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    print(f\"Total combinations to try: {len(grid)}\")\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    # Try each combination\n",
    "    for i, params in enumerate(grid):\n",
    "        print(f\"\\nCombination {i+1}/{len(grid)}:\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        print(f\"Architecture: {len(params['hidden_sizes'])} hidden layers with sizes {params['hidden_sizes']}\")\n",
    "        \n",
    "        # Train and validate with these parameters\n",
    "        model, val_loss, accuracy = train_and_validate(\n",
    "            X, y, \n",
    "            hidden_sizes=params['hidden_sizes'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            batch_size=params['batch_size'],\n",
    "            num_epochs=params['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'val_loss': val_loss,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_params = params\n",
    "            print(f\"New best model found! Validation Loss: {best_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Print final best results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Grid Search Complete!\")\n",
    "    print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return best_params, results\n",
    "\n",
    "# Updated train_final_model function to support variable hidden layers\n",
    "def train_final_model(best_params):\n",
    "    # Parameters\n",
    "    input_size = X.shape[1]\n",
    "    hidden_sizes = best_params['hidden_sizes']\n",
    "    output_size = num_baskets\n",
    "    batch_size = best_params['batch_size']\n",
    "    num_epochs = best_params['num_epochs']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = RecommendationDataset(X, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model with variable hidden layers\n",
    "    model = MLP(input_size, hidden_sizes, output_size)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nTraining final model with best parameters:\")\n",
    "    print(f\"Architecture: {len(hidden_sizes)} hidden layers with sizes {hidden_sizes}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_x, batch_y in dataloader:\n",
    "            # Move data to device\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model, basket_encoder\n",
    "\n",
    "# 1. Perform grid search\n",
    "print(\"Starting Grid Search...\")\n",
    "# Define parameter grid with different network architectures\n",
    "param_grid = {\n",
    "    'hidden_sizes': [\n",
    "        [32],\n",
    "        [32, 32],\n",
    "        [32, 64, 32],\n",
    "    ],\n",
    "    'learning_rate': [0.01, 0.005, 0.002],\n",
    "    'batch_size': [32],\n",
    "    'num_epochs': [30]\n",
    "}\n",
    "best_params, all_results = grid_search(param_grid)\n",
    "\n",
    "# 2. Train the final model with the best parameters\n",
    "final_model, basket_encoder = train_final_model(best_params)\n",
    "\n",
    "# 3. Save the model and encoders\n",
    "torch.save({\n",
    "    'model_state_dict': final_model.state_dict(),\n",
    "    'best_params': best_params,\n",
    "    'encoders': encoders,\n",
    "    'basket_encoder': basket_encoder\n",
    "}, 'best_recommendation_model.pth')\n",
    "\n",
    "print(\"\\nModel training complete and saved to 'best_recommendation_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "21b8b912-d54d-4634-b9af-4875b9a9db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Metrics for Top-K Recommendations:\n",
      "\n",
      "Metrics for k=1:\n",
      "Precision@1: 0.2257\n",
      "Recall@1: 0.1675\n",
      "F1@1: 0.1869\n",
      "Number of users evaluated: 988\n",
      "\n",
      "Metrics for k=2:\n",
      "Precision@2: 0.1842\n",
      "Recall@2: 0.2647\n",
      "F1@2: 0.2110\n",
      "Number of users evaluated: 988\n",
      "\n",
      "Metrics for k=3:\n",
      "Precision@3: 0.1488\n",
      "Recall@3: 0.3214\n",
      "F1@3: 0.1982\n",
      "Number of users evaluated: 988\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load test data\n",
    "x_test = pd.read_csv('X_test.csv', delimiter=',')  # Contains user features\n",
    "y_test = pd.read_csv('y_test.csv', delimiter=',')  # Contains user_id and basket_name\n",
    "\n",
    "# Create merged train data for finding baskets users have already bought\n",
    "train_data = pd.merge(x_train, y_train, on='user_id')\n",
    "\n",
    "# Get a list of all unique baskets\n",
    "unique_baskets = list(basket_encoder.classes_)\n",
    "\n",
    "# Function to predict basket probabilities for a user\n",
    "def predict_user_baskets(user_id, model, exclude_baskets=None):\n",
    "    # Find the user in the original data\n",
    "    user_data = x_train[x_train['user_id'] == user_id]\n",
    "    if len(user_data) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Encode features on-the-fly for this user\n",
    "    feature_vector = []\n",
    "    for col in encoders.keys():\n",
    "        encoded_value = encoders[col].transform([user_data[col].iloc[0]])[0]\n",
    "        feature_vector.append(encoded_value)\n",
    "    \n",
    "    # Convert to tensor and predict\n",
    "    x = torch.tensor(feature_vector, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = final_model(x)\n",
    "        probabilities = torch.softmax(logits, dim=1).squeeze(0)\n",
    "    \n",
    "    # Convert to numpy for easier manipulation\n",
    "    probs = probabilities.cpu().numpy()\n",
    "    \n",
    "    # Create a list of (basket_idx, probability) tuples\n",
    "    basket_probs = [(i, probs[i]) for i in range(len(probs))]\n",
    "    \n",
    "    # If exclude_baskets is provided, filter them out\n",
    "    if exclude_baskets is not None:\n",
    "        basket_probs = [(idx, prob) for idx, prob in basket_probs if idx not in exclude_baskets]\n",
    "    \n",
    "    # Sort by probability (descending)\n",
    "    basket_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return basket_probs\n",
    "\n",
    "# Evaluation function - logic remains the same\n",
    "def evaluate_model(model, x_test, y_test, train_data, top_k_values=[1, 2, 3]):\n",
    "    # Dictionary to store precision, recall, and F1 values\n",
    "    precision_at_k = collections.defaultdict(list)\n",
    "    recall_at_k = collections.defaultdict(list)\n",
    "    f1_at_k = collections.defaultdict(list)\n",
    "    \n",
    "    # Get all unique test users\n",
    "    test_user_ids = x_test['user_id'].unique()\n",
    "    \n",
    "    # For each user in the test set\n",
    "    for user_id in test_user_ids:\n",
    "        # Find baskets this user has invested in from test data (ground truth)\n",
    "        user_test_data = y_test[y_test['user_id'] == user_id]\n",
    "        \n",
    "        # Skip if user has no test data\n",
    "        if len(user_test_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        user_positive_test_baskets = set(basket_encoder.transform(user_test_data['basket_name']))\n",
    "        \n",
    "        # If no positive test baskets, skip this user\n",
    "        if len(user_positive_test_baskets) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find baskets the user has already invested in from train data\n",
    "        user_train_data = train_data[train_data['user_id'] == user_id]\n",
    "        user_invested_train_baskets = set()\n",
    "        \n",
    "        if len(user_train_data) > 0:\n",
    "            user_invested_train_baskets = set(basket_encoder.transform(user_train_data['basket_name']))\n",
    "        \n",
    "        # Get predictions for this user (excluding already purchased baskets)\n",
    "        basket_probs = predict_user_baskets(user_id, model, exclude_baskets=user_invested_train_baskets)\n",
    "        \n",
    "        # Skip if no predictions\n",
    "        if len(basket_probs) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate precision and recall at different k values\n",
    "        for k in top_k_values:\n",
    "            # Ensure k doesn't exceed number of predictions\n",
    "            effective_k = min(k, len(basket_probs))\n",
    "            \n",
    "            # Skip if no predictions\n",
    "            if effective_k == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get top-k recommended baskets\n",
    "            top_k_recs = [idx for idx, _ in basket_probs[:effective_k]]\n",
    "            \n",
    "            # Calculate relevant items among top-k recommendations\n",
    "            true_positives = len(set(top_k_recs) & user_positive_test_baskets)\n",
    "            \n",
    "            # Precision = relevant recommended / all recommended\n",
    "            precision = true_positives / effective_k\n",
    "            \n",
    "            # Recall = relevant recommended / all relevant\n",
    "            recall = true_positives / len(user_positive_test_baskets)\n",
    "            \n",
    "            # F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            precision_at_k[k].append(precision)\n",
    "            recall_at_k[k].append(recall)\n",
    "            f1_at_k[k].append(f1)\n",
    "    \n",
    "    # Calculate average precision, recall, and F1 for each k\n",
    "    results = {}\n",
    "    print(\"\\nEvaluation Metrics for Top-K Recommendations:\")\n",
    "    for k in top_k_values:\n",
    "        avg_precision = np.mean(precision_at_k[k]) if precision_at_k[k] else 0\n",
    "        avg_recall = np.mean(recall_at_k[k]) if recall_at_k[k] else 0\n",
    "        avg_f1 = np.mean(f1_at_k[k]) if f1_at_k[k] else 0\n",
    "        \n",
    "        print(f\"\\nMetrics for k={k}:\")\n",
    "        print(f\"Precision@{k}: {avg_precision:.4f}\")\n",
    "        print(f\"Recall@{k}: {avg_recall:.4f}\")\n",
    "        print(f\"F1@{k}: {avg_f1:.4f}\")\n",
    "        \n",
    "\n",
    "        print(f\"Number of users evaluated: {len(precision_at_k[k])}\")\n",
    "        \n",
    "        results[k] = {\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall,\n",
    "            'f1': avg_f1,\n",
    "            'num_users': len(precision_at_k[k])\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the evaluation\n",
    "evaluation_results = evaluate_model(model, x_test, y_test, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d98073-5746-4d12-b4fb-a77621c3cf95",
   "metadata": {},
   "source": [
    "# 80% user in training with full purchase history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa7f479c-9611-4548-8d7a-b41af11002b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique users: 994\n",
      "Users in training set: 795\n",
      "Users in test set: 199\n",
      "Overlap between train and test users: 0\n",
      "Training set size: 27616\n",
      "Test set size: 6605\n",
      "Starting Grid Search...\n",
      "Total combinations to try: 9\n",
      "\n",
      "Combination 1/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32], 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Architecture: 1 hidden layers with sizes [32]\n",
      "Epoch [2/10], Train Loss: 3.8987, Test Loss: 4.0850, Accuracy: 0.0902\n",
      "Epoch [4/10], Train Loss: 3.7046, Test Loss: 4.0740, Accuracy: 0.0969\n",
      "Epoch [6/10], Train Loss: 3.6469, Test Loss: 4.1286, Accuracy: 0.0948\n",
      "Epoch [8/10], Train Loss: 3.6095, Test Loss: 4.1975, Accuracy: 0.0955\n",
      "Epoch [10/10], Train Loss: 3.5825, Test Loss: 4.2566, Accuracy: 0.0975\n",
      "New best model found! Test Loss: 4.2566, Accuracy: 0.0975\n",
      "\n",
      "Combination 2/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32], 'learning_rate': 0.0005, 'num_epochs': 10}\n",
      "Architecture: 1 hidden layers with sizes [32]\n",
      "Epoch [2/10], Train Loss: 4.0557, Test Loss: 4.2035, Accuracy: 0.0789\n",
      "Epoch [4/10], Train Loss: 3.8348, Test Loss: 4.0922, Accuracy: 0.0886\n",
      "Epoch [6/10], Train Loss: 3.7342, Test Loss: 4.0643, Accuracy: 0.0942\n",
      "Epoch [8/10], Train Loss: 3.6846, Test Loss: 4.0822, Accuracy: 0.0930\n",
      "Epoch [10/10], Train Loss: 3.6532, Test Loss: 4.1122, Accuracy: 0.0954\n",
      "New best model found! Test Loss: 4.1122, Accuracy: 0.0954\n",
      "\n",
      "Combination 3/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32], 'learning_rate': 0.0002, 'num_epochs': 10}\n",
      "Architecture: 1 hidden layers with sizes [32]\n",
      "Epoch [2/10], Train Loss: 4.2878, Test Loss: 4.3736, Accuracy: 0.0839\n",
      "Epoch [4/10], Train Loss: 4.1003, Test Loss: 4.2473, Accuracy: 0.0898\n",
      "Epoch [6/10], Train Loss: 3.9798, Test Loss: 4.1714, Accuracy: 0.0904\n",
      "Epoch [8/10], Train Loss: 3.8946, Test Loss: 4.1071, Accuracy: 0.0910\n",
      "Epoch [10/10], Train Loss: 3.8302, Test Loss: 4.0709, Accuracy: 0.0949\n",
      "New best model found! Test Loss: 4.0709, Accuracy: 0.0949\n",
      "\n",
      "Combination 4/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 32], 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Architecture: 2 hidden layers with sizes [32, 32]\n",
      "Epoch [2/10], Train Loss: 3.7975, Test Loss: 3.9961, Accuracy: 0.0933\n",
      "Epoch [4/10], Train Loss: 3.6766, Test Loss: 4.0259, Accuracy: 0.0948\n",
      "Epoch [6/10], Train Loss: 3.6125, Test Loss: 4.0662, Accuracy: 0.0925\n",
      "Epoch [8/10], Train Loss: 3.5623, Test Loss: 4.1154, Accuracy: 0.1057\n",
      "Epoch [10/10], Train Loss: 3.5181, Test Loss: 4.2088, Accuracy: 0.0984\n",
      "\n",
      "Combination 5/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 32], 'learning_rate': 0.0005, 'num_epochs': 10}\n",
      "Architecture: 2 hidden layers with sizes [32, 32]\n",
      "Epoch [2/10], Train Loss: 3.9577, Test Loss: 4.0440, Accuracy: 0.0902\n",
      "Epoch [4/10], Train Loss: 3.7237, Test Loss: 4.0066, Accuracy: 0.0908\n",
      "Epoch [6/10], Train Loss: 3.6556, Test Loss: 4.0110, Accuracy: 0.1017\n",
      "Epoch [8/10], Train Loss: 3.6169, Test Loss: 4.0405, Accuracy: 0.1001\n",
      "Epoch [10/10], Train Loss: 3.5890, Test Loss: 4.0691, Accuracy: 0.1023\n",
      "New best model found! Test Loss: 4.0691, Accuracy: 0.1023\n",
      "\n",
      "Combination 6/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 32], 'learning_rate': 0.0002, 'num_epochs': 10}\n",
      "Architecture: 2 hidden layers with sizes [32, 32]\n",
      "Epoch [2/10], Train Loss: 4.2220, Test Loss: 4.3042, Accuracy: 0.0769\n",
      "Epoch [4/10], Train Loss: 3.9879, Test Loss: 4.1322, Accuracy: 0.0827\n",
      "Epoch [6/10], Train Loss: 3.8575, Test Loss: 4.0350, Accuracy: 0.0921\n",
      "Epoch [8/10], Train Loss: 3.7759, Test Loss: 4.0050, Accuracy: 0.0919\n",
      "Epoch [10/10], Train Loss: 3.7289, Test Loss: 4.0083, Accuracy: 0.0940\n",
      "New best model found! Test Loss: 4.0083, Accuracy: 0.0940\n",
      "\n",
      "Combination 7/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.001, 'num_epochs': 10}\n",
      "Architecture: 3 hidden layers with sizes [32, 64, 32]\n",
      "Epoch [2/10], Train Loss: 3.7592, Test Loss: 3.9713, Accuracy: 0.0881\n",
      "Epoch [4/10], Train Loss: 3.6274, Test Loss: 4.0117, Accuracy: 0.0946\n",
      "Epoch [6/10], Train Loss: 3.5571, Test Loss: 4.0378, Accuracy: 0.0933\n",
      "Epoch [8/10], Train Loss: 3.4988, Test Loss: 4.1334, Accuracy: 0.0987\n",
      "Epoch [10/10], Train Loss: 3.4423, Test Loss: 4.2238, Accuracy: 0.0943\n",
      "\n",
      "Combination 8/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.0005, 'num_epochs': 10}\n",
      "Architecture: 3 hidden layers with sizes [32, 64, 32]\n",
      "Epoch [2/10], Train Loss: 4.0145, Test Loss: 4.1215, Accuracy: 0.0804\n",
      "Epoch [4/10], Train Loss: 3.7735, Test Loss: 3.9860, Accuracy: 0.0945\n",
      "Epoch [6/10], Train Loss: 3.6766, Test Loss: 3.9790, Accuracy: 0.1063\n",
      "Epoch [8/10], Train Loss: 3.6306, Test Loss: 3.9876, Accuracy: 0.1005\n",
      "Epoch [10/10], Train Loss: 3.5920, Test Loss: 4.0357, Accuracy: 0.0960\n",
      "\n",
      "Combination 9/9:\n",
      "Parameters: {'batch_size': 32, 'hidden_sizes': [32, 64, 32], 'learning_rate': 0.0002, 'num_epochs': 10}\n",
      "Architecture: 3 hidden layers with sizes [32, 64, 32]\n",
      "Epoch [2/10], Train Loss: 4.2072, Test Loss: 4.2267, Accuracy: 0.0771\n",
      "Epoch [4/10], Train Loss: 3.8937, Test Loss: 4.0298, Accuracy: 0.0908\n",
      "Epoch [6/10], Train Loss: 3.7698, Test Loss: 3.9785, Accuracy: 0.0990\n",
      "Epoch [8/10], Train Loss: 3.7207, Test Loss: 3.9681, Accuracy: 0.0978\n",
      "Epoch [10/10], Train Loss: 3.6922, Test Loss: 3.9708, Accuracy: 0.0984\n",
      "New best model found! Test Loss: 3.9708, Accuracy: 0.0984\n",
      "\n",
      "==================================================\n",
      "Grid Search Complete!\n",
      "Best Test Loss: 3.9708\n",
      "Best Parameters:\n",
      "  batch_size: 32\n",
      "  hidden_sizes: [32, 64, 32]\n",
      "  learning_rate: 0.0002\n",
      "  num_epochs: 10\n",
      "\n",
      "Training final model with best parameters:\n",
      "Architecture: 3 hidden layers with sizes [32, 64, 32]\n",
      "Epoch [2/10], Loss: 4.2209\n",
      "Epoch [4/10], Loss: 3.9310\n",
      "Epoch [6/10], Loss: 3.7890\n",
      "Epoch [8/10], Loss: 3.7148\n",
      "Epoch [10/10], Loss: 3.6766\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import ParameterGrid, GroupShuffleSplit\n",
    "\n",
    "# 1. Load the data\n",
    "x_train = pd.read_csv('X_train.csv', delimiter=',')\n",
    "y_train = pd.read_csv('y_train.csv', delimiter=',')\n",
    "x_test = pd.read_csv('X_test.csv', delimiter=',')\n",
    "y_test = pd.read_csv('y_test.csv', delimiter=',')\n",
    "basket_features = pd.read_csv('../basket_features.csv')\n",
    "\n",
    "# 2. Merge training and test data\n",
    "x_combined = pd.concat([x_train, x_test], ignore_index=True)\n",
    "y_combined = pd.concat([y_train, y_test], ignore_index=True)\n",
    "\n",
    "# 3. Merge features and targets\n",
    "merged_data = pd.merge(x_combined, y_combined, on='user_id')\n",
    "\n",
    "# 4. Initialize encoders dictionary\n",
    "encoders = {}\n",
    "\n",
    "# 5. Encode categorical features\n",
    "for col in ['location', 'gender', 'education', 'invest_goal', 'age_group']:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(merged_data[col])\n",
    "    merged_data[f'{col}_encoded'] = encoder.transform(merged_data[col])\n",
    "    encoders[col] = encoder\n",
    "\n",
    "# 6. Encode basket names\n",
    "all_basket_names = basket_features['basket_name'].unique()\n",
    "basket_encoder = LabelEncoder()\n",
    "basket_encoder.fit(all_basket_names)\n",
    "num_baskets = len(basket_encoder.classes_)\n",
    "merged_data['basket_encoded'] = basket_encoder.transform(merged_data['basket_name'])\n",
    "\n",
    "# 7. Split data by user_id: 80% of users in training, 20% in test\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_indices, test_indices = next(splitter.split(merged_data, groups=merged_data['user_id']))\n",
    "\n",
    "# Create new training and test sets\n",
    "new_x_train = merged_data.iloc[train_indices]\n",
    "new_x_test = merged_data.iloc[test_indices]\n",
    "\n",
    "# Verify the split worked correctly\n",
    "train_users = set(new_x_train['user_id'])\n",
    "test_users = set(new_x_test['user_id'])\n",
    "print(f\"Total unique users: {len(set(merged_data['user_id']))}\")\n",
    "print(f\"Users in training set: {len(train_users)}\")\n",
    "print(f\"Users in test set: {len(test_users)}\")\n",
    "print(f\"Overlap between train and test users: {len(train_users.intersection(test_users))}\")\n",
    "print(f\"Training set size: {len(new_x_train)}\")\n",
    "print(f\"Test set size: {len(new_x_test)}\")\n",
    "\n",
    "# 8. Prepare features and targets for model training\n",
    "X_train = new_x_train[[f'{col}_encoded' for col in encoders.keys()]].values\n",
    "y_train = new_x_train['basket_encoded'].values\n",
    "X_test = new_x_test[[f'{col}_encoded' for col in encoders.keys()]].values\n",
    "y_test = new_x_test['basket_encoded'].values\n",
    "\n",
    "# Dataset class\n",
    "class RecommendationDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), self.y[idx]\n",
    "\n",
    "# MLP model with variable hidden layers\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # Create a list of layers\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer to first hidden layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        # Create additional hidden layers\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "        # Create a sequential container\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_and_evaluate(X_train, y_train, X_test, y_test, hidden_sizes, learning_rate, batch_size, num_epochs):\n",
    "    # Create datasets\n",
    "    train_dataset = RecommendationDataset(X_train, y_train)\n",
    "    test_dataset = RecommendationDataset(X_test, y_test)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    input_size = X_train.shape[1]\n",
    "    output_size = num_baskets\n",
    "    model = MLP(input_size, hidden_sizes, output_size)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        # Test evaluation\n",
    "        model.eval()\n",
    "        total_test_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in test_loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                \n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                \n",
    "                total_test_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += batch_y.size(0)\n",
    "                correct += (predicted == batch_y).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_test_loss = total_test_loss / len(test_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # Print progress every few epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return model, avg_test_loss, accuracy\n",
    "\n",
    "# Grid Search for hyperparameter optimization\n",
    "def grid_search(param_grid):\n",
    "    # Create all parameter combinations\n",
    "    grid = list(ParameterGrid(param_grid))\n",
    "    print(f\"Total combinations to try: {len(grid)}\")\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    best_test_loss = float('inf')\n",
    "    best_params = None\n",
    "    \n",
    "    # Try each combination\n",
    "    for i, params in enumerate(grid):\n",
    "        print(f\"\\nCombination {i+1}/{len(grid)}:\")\n",
    "        print(f\"Parameters: {params}\")\n",
    "        print(f\"Architecture: {len(params['hidden_sizes'])} hidden layers with sizes {params['hidden_sizes']}\")\n",
    "        \n",
    "        # Train and evaluate with these parameters\n",
    "        model, test_loss, accuracy = train_and_evaluate(\n",
    "            X_train, y_train, X_test, y_test,\n",
    "            hidden_sizes=params['hidden_sizes'],\n",
    "            learning_rate=params['learning_rate'],\n",
    "            batch_size=params['batch_size'],\n",
    "            num_epochs=params['num_epochs']\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'test_loss': test_loss,\n",
    "            'accuracy': accuracy\n",
    "        })\n",
    "        \n",
    "        # Check if this is the best model so far\n",
    "        if test_loss < best_test_loss:\n",
    "            best_test_loss = test_loss\n",
    "            best_params = params\n",
    "            print(f\"New best model found! Test Loss: {best_test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Print final best results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Grid Search Complete!\")\n",
    "    print(f\"Best Test Loss: {best_test_loss:.4f}\")\n",
    "    print(\"Best Parameters:\")\n",
    "    for key, value in best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return best_params, results\n",
    "\n",
    "# Function to train final model with best parameters\n",
    "def train_final_model(best_params):\n",
    "    # Parameters\n",
    "    input_size = X_train.shape[1]\n",
    "    hidden_sizes = best_params['hidden_sizes']\n",
    "    output_size = num_baskets\n",
    "    batch_size = best_params['batch_size']\n",
    "    num_epochs = best_params['num_epochs']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "    \n",
    "    # Create combined dataset for final training\n",
    "    X_all = np.vstack((X_train, X_test))\n",
    "    y_all = np.concatenate((y_train, y_test))\n",
    "    \n",
    "    dataset = RecommendationDataset(X_all, y_all)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model with best parameters\n",
    "    model = MLP(input_size, hidden_sizes, output_size)\n",
    "    \n",
    "    # Use GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nTraining final model with best parameters:\")\n",
    "    print(f\"Architecture: {len(hidden_sizes)} hidden layers with sizes {hidden_sizes}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_x, batch_y in dataloader:\n",
    "            # Move data to device\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\")\n",
    "    \n",
    "    return model, basket_encoder\n",
    "\n",
    "# Define parameter grid with different network architectures\n",
    "param_grid = {\n",
    "    'hidden_sizes': [\n",
    "        [32],\n",
    "        [32, 32],\n",
    "        [32, 64, 32],\n",
    "    ],\n",
    "    'learning_rate': [0.001, 0.0005, 0.0002],\n",
    "    'batch_size': [32],\n",
    "    'num_epochs': [10]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "print(\"Starting Grid Search...\")\n",
    "best_params, all_results = grid_search(param_grid)\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "final_model, basket_encoder = train_final_model(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4700e612-dd18-4020-8653-761147dc0f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation on the test set...\n",
      "\n",
      "Evaluation Metrics for Top-K Recommendations:\n",
      "\n",
      "Metrics for k=1:\n",
      "Precision@1: 0.6080\n",
      "Recall@1: 0.1149\n",
      "F1@1: 0.1909\n",
      "Number of users evaluated: 199\n",
      "\n",
      "Metrics for k=2:\n",
      "Precision@2: 0.5829\n",
      "Recall@2: 0.2229\n",
      "F1@2: 0.3157\n",
      "Number of users evaluated: 199\n",
      "\n",
      "Metrics for k=3:\n",
      "Precision@3: 0.5159\n",
      "Recall@3: 0.2911\n",
      "F1@3: 0.3644\n",
      "Number of users evaluated: 199\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to predict basket probabilities for a user\n",
    "def predict_user_baskets(user_features, model, basket_encoder, encoders, exclude_baskets=None):\n",
    "    # Encode features for this user\n",
    "    feature_vector = []\n",
    "    for col in encoders.keys():\n",
    "        col_name = f'{col}_encoded'\n",
    "        if col_name in user_features:\n",
    "            # If already encoded\n",
    "            feature_vector.append(user_features[col_name])\n",
    "        else:\n",
    "            # If needs encoding on-the-fly\n",
    "            encoded_value = encoders[col].transform([user_features[col]])[0]\n",
    "            feature_vector.append(encoded_value)\n",
    "    \n",
    "    # Convert to tensor and predict\n",
    "    x = torch.tensor(feature_vector, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probabilities = torch.softmax(logits, dim=1).squeeze(0)\n",
    "    \n",
    "    # Convert to numpy for easier manipulation\n",
    "    probs = probabilities.cpu().numpy()\n",
    "    \n",
    "    # Create a list of (basket_idx, probability) tuples\n",
    "    basket_probs = [(i, probs[i]) for i in range(len(probs))]\n",
    "    \n",
    "    # If exclude_baskets is provided, filter them out\n",
    "    if exclude_baskets is not None:\n",
    "        basket_probs = [(idx, prob) for idx, prob in basket_probs if idx not in exclude_baskets]\n",
    "    \n",
    "    # Sort by probability (descending)\n",
    "    basket_probs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return basket_probs\n",
    "\n",
    "# Evaluation function for the new data split\n",
    "def evaluate_model(model, new_x_test, new_x_train, basket_encoder, encoders, top_k_values=[1, 2, 3, 5, 10]):\n",
    "    # Dictionary to store precision, recall, and F1 values\n",
    "    precision_at_k = collections.defaultdict(list)\n",
    "    recall_at_k = collections.defaultdict(list)\n",
    "    f1_at_k = collections.defaultdict(list)\n",
    "    \n",
    "    # Get all unique test users\n",
    "    test_user_ids = new_x_test['user_id'].unique()\n",
    "    \n",
    "    # For each user in the test set\n",
    "    for user_id in test_user_ids:\n",
    "        # Get all test data for this user\n",
    "        user_test_data = new_x_test[new_x_test['user_id'] == user_id]\n",
    "        \n",
    "        # Skip if user has no test data\n",
    "        if len(user_test_data) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Get the baskets this user has invested in (ground truth)\n",
    "        user_positive_test_baskets = set(user_test_data['basket_encoded'])\n",
    "        \n",
    "        # If no positive test baskets, skip this user\n",
    "        if len(user_positive_test_baskets) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Find baskets the user has already invested in from train data (if any)\n",
    "        user_train_data = new_x_train[new_x_train['user_id'] == user_id]\n",
    "        user_invested_train_baskets = set()\n",
    "        \n",
    "        if len(user_train_data) > 0:\n",
    "            # This should be empty since we split by user, but included for completeness\n",
    "            user_invested_train_baskets = set(user_train_data['basket_encoded'])\n",
    "        \n",
    "        # Get the first row of user data (features are the same for all rows of the same user)\n",
    "        user_features = user_test_data.iloc[0]\n",
    "        \n",
    "        # Get predictions for this user (excluding already purchased baskets if any)\n",
    "        basket_probs = predict_user_baskets(\n",
    "            user_features, \n",
    "            model, \n",
    "            basket_encoder, \n",
    "            encoders, \n",
    "            exclude_baskets=user_invested_train_baskets\n",
    "        )\n",
    "        \n",
    "        # Skip if no predictions\n",
    "        if len(basket_probs) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Calculate precision and recall at different k values\n",
    "        for k in top_k_values:\n",
    "            # Ensure k doesn't exceed number of predictions\n",
    "            effective_k = min(k, len(basket_probs))\n",
    "            \n",
    "            # Skip if no predictions\n",
    "            if effective_k == 0:\n",
    "                continue\n",
    "            \n",
    "            # Get top-k recommended baskets\n",
    "            top_k_recs = [idx for idx, _ in basket_probs[:effective_k]]\n",
    "            \n",
    "            # Calculate relevant items among top-k recommendations\n",
    "            true_positives = len(set(top_k_recs) & user_positive_test_baskets)\n",
    "            \n",
    "            # Precision = relevant recommended / all recommended\n",
    "            precision = true_positives / effective_k\n",
    "            \n",
    "            # Recall = relevant recommended / all relevant\n",
    "            recall = true_positives / len(user_positive_test_baskets)\n",
    "            \n",
    "            # F1 score = 2 * (precision * recall) / (precision + recall)\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            precision_at_k[k].append(precision)\n",
    "            recall_at_k[k].append(recall)\n",
    "            f1_at_k[k].append(f1)\n",
    "    \n",
    "    # Calculate average precision, recall, and F1 for each k\n",
    "    results = {}\n",
    "    print(\"\\nEvaluation Metrics for Top-K Recommendations:\")\n",
    "    for k in top_k_values:\n",
    "        if not precision_at_k[k]:\n",
    "            print(f\"\\nNo data for k={k}\")\n",
    "            continue\n",
    "            \n",
    "        avg_precision = np.mean(precision_at_k[k])\n",
    "        avg_recall = np.mean(recall_at_k[k])\n",
    "        avg_f1 = np.mean(f1_at_k[k])\n",
    "        \n",
    "        print(f\"\\nMetrics for k={k}:\")\n",
    "        print(f\"Precision@{k}: {avg_precision:.4f}\")\n",
    "        print(f\"Recall@{k}: {avg_recall:.4f}\")\n",
    "        print(f\"F1@{k}: {avg_f1:.4f}\")\n",
    "        \n",
    "        # Also print number of users evaluated\n",
    "        print(f\"Number of users evaluated: {len(precision_at_k[k])}\")\n",
    "        \n",
    "        results[k] = {\n",
    "            'precision': avg_precision,\n",
    "            'recall': avg_recall,\n",
    "            'f1': avg_f1,\n",
    "            'num_users': len(precision_at_k[k])\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# After training the model, you would call the evaluation like this:\n",
    "def run_evaluation(model, new_x_test, new_x_train, basket_encoder, encoders):\n",
    "    print(\"Running evaluation on the test set...\")\n",
    "    evaluation_results = evaluate_model(\n",
    "        model=model,\n",
    "        new_x_test=new_x_test,\n",
    "        new_x_train=new_x_train,\n",
    "        basket_encoder=basket_encoder,\n",
    "        encoders=encoders,\n",
    "        top_k_values=[1, 2, 3]\n",
    "    )\n",
    "    return evaluation_results\n",
    "\n",
    "\n",
    "# After training your final model\n",
    "evaluation_results = run_evaluation(\n",
    "    model=final_model,\n",
    "    new_x_test=new_x_test,\n",
    "    new_x_train=new_x_train,\n",
    "    basket_encoder=basket_encoder,\n",
    "    encoders=encoders\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c69bc3-b9d4-4c79-b1ec-7695a892b3d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
