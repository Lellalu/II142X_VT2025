{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59053f4c-a40a-480e-a4af-a8e16249e467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5fa093f-88d6-4b14-bf98-4ce6dcfe2aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "investments_pd = pd.read_csv('../syntheticDataGenerators/investment/invest_data_gemini-2.5-flash.csv', sep=';')\n",
    "baskets_pd = pd.read_csv('../data/company_basket.csv', sep=';')\n",
    "users_pd = pd.read_csv('../syntheticDataGenerators/user/swedish_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81adab-8af8-453f-a2d7-9a0c201a0f5f",
   "metadata": {},
   "source": [
    "# User data into numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3329e906-9f45-4ecd-bbc0-53c8711645e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     user_id         location  gender          education invest_goal age_group\n",
      "0       1001      Västmanland  Female    Master's Degree   Long-term     31-40\n",
      "1       1002            Skåne    Male        High School   Long-term     21-30\n",
      "2       1003      Västmanland  Female    Master's Degree   Long-term     61-70\n",
      "3       1004        Stockholm    Male        High School  Short-term     21-30\n",
      "4       1005           Kalmar    Male  Bachelor's Degree  Short-term     21-30\n",
      "..       ...              ...     ...                ...         ...       ...\n",
      "995     1996            Skåne    Male        High School  Short-term     51-60\n",
      "996     1997          Uppsala    Male        High School   Long-term     71-80\n",
      "997     1998        Gävleborg    Male        High School  Short-term     31-40\n",
      "998     1999        Jönköping    Male        High School   Long-term     61-70\n",
      "999     2000  Västra Götaland  Female        High School   Long-term     41-50\n",
      "\n",
      "[1000 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# First, drop user_name column but keep user_id\n",
    "users_pd_processed = users_pd.drop(['user_name'], axis=1)\n",
    "\n",
    "# Create age groups without encoding\n",
    "def create_age_group(age):\n",
    "    if age <= 20:\n",
    "        return '<=20'\n",
    "    elif age <= 30:\n",
    "        return '21-30'\n",
    "    elif age <= 40:\n",
    "        return '31-40'\n",
    "    elif age <= 50:\n",
    "        return '41-50'\n",
    "    elif age <= 60:\n",
    "        return '51-60'\n",
    "    elif age <= 70:\n",
    "        return '61-70'\n",
    "    elif age <= 80:\n",
    "        return '71-80'\n",
    "    else:\n",
    "        return '>80'\n",
    "\n",
    "# Apply age grouping\n",
    "users_pd_processed['age_group'] = users_pd_processed['age'].apply(create_age_group)\n",
    "users_pd_processed = users_pd_processed.drop('age', axis=1)  # Remove original age column\n",
    "\n",
    "# Handle locations that are not in the predefined list\n",
    "all_locations = [\n",
    "    'Stockholm', 'Västra Götaland', 'Skåne', 'Uppsala', 'Östergötland', \n",
    "    'Västmanland', 'Södermanland', 'Jönköping', 'Halland', 'Västerbotten', \n",
    "    'Kalmar', 'Gävleborg', 'Kronoberg', 'Värmland', 'Örebro', \n",
    "    'Blekinge', 'Dalarna', 'Norrbotten', 'Västernorrland', 'Gotland', \n",
    "    'Jämtland', 'Other'\n",
    "]\n",
    "\n",
    "users_pd_processed['location'] = users_pd_processed['location'].apply(\n",
    "    lambda x: x if x in all_locations else 'Other'\n",
    ")\n",
    "\n",
    "# View the processed dataframe\n",
    "print(users_pd_processed)\n",
    "\n",
    "# Step 1: Create a dataset where each row is a single purchase\n",
    "investment_data = investments_pd.copy()\n",
    "\n",
    "# Step 2: Merge with user features to create the final dataset\n",
    "# Each row will be a purchase with the user's features\n",
    "purchase_dataset = investment_data.merge(users_pd_processed, on='user_id', how='inner')\n",
    "\n",
    "# Step 3: Split each user's purchases 80/20 for train/test\n",
    "# Group the purchase dataset by user_id\n",
    "user_groups = purchase_dataset.groupby('user_id')\n",
    "\n",
    "# Initialize empty lists for train and test indices\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# For each user, split their purchases 80/20\n",
    "for user_id, user_data in user_groups:\n",
    "    # Get indices for this user's purchases\n",
    "    user_indices = user_data.index.tolist()\n",
    "    \n",
    "    # Randomly shuffle the indices\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    np.random.shuffle(user_indices)\n",
    "\n",
    "    # Calculate split point (80% for training)\n",
    "    split_idx = int(len(user_indices) * 0.8)\n",
    "\n",
    "    # Add indices to train and test lists\n",
    "    train_indices.extend(user_indices[:split_idx])\n",
    "    test_indices.extend(user_indices[split_idx:])\n",
    "\n",
    "# Create train and test datasets using the indices\n",
    "train_data = purchase_dataset.loc[train_indices]\n",
    "test_data = purchase_dataset.loc[test_indices]\n",
    "\n",
    "# Create X and y for training data - using string values directly\n",
    "X_train = train_data[['user_id', 'location', 'gender', 'education', 'invest_goal', 'age_group']]\n",
    "y_train = train_data[['user_id', 'basket_name']]  # Use basket_name instead of basket_encoded\n",
    "\n",
    "# Create X and y for test data - using string values directly\n",
    "X_test = test_data[['user_id', 'location', 'gender', 'education', 'invest_goal', 'age_group']]\n",
    "y_test = test_data[['user_id', 'basket_name']]  # Use basket_name instead of basket_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9088cbdd-a0f9-451a-8471-8dc27640a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv', index=False)\n",
    "# Save the test features with user_id\n",
    "X_test.to_csv('X_test.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "# Save the training labels\n",
    "#pd.DataFrame(y_train, columns=['basket_encoded']).to_csv('y_train.csv', index=False)\n",
    "# Save the test labels\n",
    "#pd.DataFrame(y_test, columns=['basket_encoded']).to_csv('y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28704a38-9557-4940-bbb3-737a22de352a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'basket_encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../basket_features.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Convert features\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m result \u001b[38;5;241m=\u001b[39m convert_basket_features(csv_path, \u001b[43mbasket_encoder\u001b[49m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Save result of numerical basket features to new CSV file\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'basket_encoder' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def convert_basket_features(csv_path, basket_encoder):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Get the basket names (first column)\n",
    "    baskets = df.iloc[:, 0].tolist()\n",
    "    \n",
    "    # Initialize the result dataframe with basket names\n",
    "    result_df = pd.DataFrame({'basket_name': baskets})\n",
    "    \n",
    "    result_df['basket_encoded'] = basket_encoder.transform(result_df['basket_name'])\n",
    "    \n",
    "    # Create mappings for different feature categories\n",
    "    feature_categories = {\n",
    "        'industry_name': [],\n",
    "        'economic_sector_name': [],\n",
    "        'Size Category': [],\n",
    "        'volatility_category': []\n",
    "    }\n",
    "    \n",
    "    # Identify columns for each category\n",
    "    for col in df.columns[1:]:  # Skip the first column which is basket_name\n",
    "        for category in feature_categories:\n",
    "            if category in col:\n",
    "                feature_categories[category].append(col)\n",
    "    \n",
    "    # Process each basket and assign numerical features\n",
    "    for category, columns in feature_categories.items():\n",
    "        if not columns:\n",
    "            continue\n",
    "            \n",
    "        # Create a new column to store the feature value\n",
    "        result_df[f'{category}_feature'] = 0\n",
    "        \n",
    "        # For each basket, find the column with maximum value\n",
    "        for idx, row in df.iterrows():\n",
    "            if len(columns) > 0:\n",
    "                # Get values for this category\n",
    "                values = row[columns].values\n",
    "                \n",
    "                # Find the maximum value index\n",
    "                max_idx = np.argmax(values)\n",
    "                \n",
    "                if values[max_idx] > 0:  # Only if there's a positive value\n",
    "                    # Get the feature name from the column\n",
    "                    feature_name = columns[max_idx].replace(f'{category}_', '')\n",
    "                    \n",
    "                    # Create a mapping of feature names to numerical values if needed\n",
    "                    # Here we simply use the index as the numerical feature\n",
    "                    result_df.at[idx, f'{category}_feature'] = max_idx + 1  # +1 to avoid 0\n",
    "                    \n",
    "                    # Optionally store the feature name for reference\n",
    "                    result_df.at[idx, f'{category}_name'] = feature_name\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Path to CSV file\n",
    "csv_path = '../basket_features.csv'\n",
    "\n",
    "# Convert features\n",
    "result = convert_basket_features(csv_path, basket_encoder)\n",
    "\n",
    "print(result.head())\n",
    "\n",
    "# Save result of numerical basket features to new CSV file\n",
    "result.to_csv('basket_numerical_features.csv', index=False)\n",
    "print(\"Saved numerical features to 'basket_numerical_features.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69c8dba-d742-40a6-aa29-3cf00b614391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
