{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c2ba94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global healthcare I: 0.8822\n",
      "Pharmaceuticals EU: 0.7212\n",
      "Well traded stocks: 0.6855\n",
      "Healthcare southern Europ√©: 0.6595\n",
      "Healthcare Facilities: 0.6401\n",
      "\n",
      "üìä Hybrid Recommender Evaluation Metrics (CB + DL):\n",
      "\n",
      "‚û°Ô∏è Top-1 metrics:\n",
      "Precision@1: 0.1358\n",
      "Recall@1:    0.0986\n",
      "F1@1:        0.1110\n",
      "Users evaluated: 994\n",
      "\n",
      "‚û°Ô∏è Top-2 metrics:\n",
      "Precision@2: 0.1162\n",
      "Recall@2:    0.1670\n",
      "F1@2:        0.1331\n",
      "Users evaluated: 994\n",
      "\n",
      "‚û°Ô∏è Top-3 metrics:\n",
      "Precision@3: 0.1026\n",
      "Recall@3:    0.2183\n",
      "F1@3:        0.1360\n",
      "Users evaluated: 994\n"
     ]
    }
   ],
   "source": [
    "# --- Setup: Load saved model and rebuild components ---\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import collections\n",
    "import torch.nn as nn\n",
    "\n",
    "# Load saved model checkpoint with allowed globals (due to LabelEncoder)\n",
    "with torch.serialization.safe_globals([LabelEncoder]):\n",
    "    checkpoint = torch.load('best_recommendation_model.pth', weights_only=False)\n",
    "\n",
    "# Extract saved encoders, best model parameters, and basket encoder\n",
    "basket_encoder = checkpoint['basket_encoder']\n",
    "encoders = checkpoint['encoders']\n",
    "best_params = checkpoint['best_params']\n",
    "\n",
    "# Rebuild the MLP model with saved architecture\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        for i in range(1, len(hidden_sizes)):\n",
    "            layers.append(nn.Linear(hidden_sizes[i - 1], hidden_sizes[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate and load model weights\n",
    "input_size = len(encoders)\n",
    "hidden_sizes = best_params['hidden_sizes']\n",
    "output_size = len(basket_encoder.classes_)\n",
    "final_model = MLP(input_size, hidden_sizes, output_size)\n",
    "final_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "final_model.eval()\n",
    "\n",
    "# --- Load investment and basket feature data ---\n",
    "train_data = pd.read_csv('y_train.csv')\n",
    "test_data = pd.read_csv('y_test.csv')\n",
    "basket_features = pd.read_csv('../basket_features.csv')\n",
    "basket_features_indexed = basket_features.set_index('basket_name')\n",
    "unique_baskets = basket_features['basket_name'].unique()\n",
    "\n",
    "# --- Build user content profiles by averaging invested basket features ---\n",
    "def build_user_profile(user_id, user_baskets, basket_features_df):\n",
    "    user_basket_list = user_baskets[user_baskets['user_id'] == user_id]['basket_name'].tolist()\n",
    "    user_basket_features = [\n",
    "        basket_features_df.loc[basket] for basket in user_basket_list \n",
    "        if basket in basket_features_df.index\n",
    "    ]\n",
    "    if not user_basket_features:\n",
    "        return pd.Series(0, index=basket_features_df.columns)\n",
    "    return pd.concat(user_basket_features, axis=1).mean(axis=1)\n",
    "\n",
    "# Precompute user profiles for all training users\n",
    "user_profiles = {\n",
    "    user_id: build_user_profile(user_id, train_data, basket_features_indexed)\n",
    "    for user_id in train_data['user_id'].unique()\n",
    "}\n",
    "user_profiles_df = pd.DataFrame(user_profiles).T\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def normalize_scores(score_dict):\n",
    "    # Normalize score dictionary values to [0, 1]\n",
    "    items = list(score_dict.items())\n",
    "    keys = [k for k, _ in items]\n",
    "    values = np.array([v for _, v in items]).reshape(-1, 1)\n",
    "    if len(values) == 0:\n",
    "        return {}\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized = scaler.fit_transform(values).flatten()\n",
    "    return dict(zip(keys, normalized))\n",
    "\n",
    "def predict_rating(user_id, basket_name, user_profiles_df, basket_features_df):\n",
    "    # Compute cosine similarity between user profile and basket feature vector\n",
    "    if user_id not in user_profiles_df.index or basket_name not in basket_features_df.index:\n",
    "        return 0\n",
    "    user_profile = user_profiles_df.loc[user_id]\n",
    "    basket_vector = basket_features_df.loc[basket_name]\n",
    "    sim = np.dot(user_profile, basket_vector) / (np.linalg.norm(user_profile) * np.linalg.norm(basket_vector) + 1e-8)\n",
    "    return sim\n",
    "\n",
    "def predict_user_baskets(user_id, model, exclude_baskets=None):\n",
    "    # Use deep learning model to predict basket probabilities for a given user\n",
    "    x_train = pd.read_csv('X_train.csv')\n",
    "    user_row = x_train[x_train['user_id'] == user_id]\n",
    "    if user_row.empty:\n",
    "        return []\n",
    "    feature_vector = [\n",
    "        encoders[col].transform([user_row[col].values[0]])[0]\n",
    "        for col in encoders\n",
    "    ]\n",
    "    x_tensor = torch.tensor(feature_vector, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_tensor)\n",
    "        probs = torch.softmax(logits, dim=1).squeeze(0).numpy()\n",
    "    basket_probs = [(i, prob) for i, prob in enumerate(probs)]\n",
    "    if exclude_baskets is not None:\n",
    "        basket_probs = [\n",
    "            (i, p) for i, p in basket_probs\n",
    "            if basket_encoder.inverse_transform([i])[0] not in exclude_baskets\n",
    "        ]\n",
    "    return sorted(basket_probs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# --- Hybrid Recommendation Function ---\n",
    "def get_hybrid_recommendations(\n",
    "    user_id,\n",
    "    top_k=5,\n",
    "    weights=(0.5, 0.5),  # (CB, DL)\n",
    "    basket_encoder=None,\n",
    "    basket_features_indexed=None,\n",
    "    user_profiles_df=None,\n",
    "    final_model=None,\n",
    "    encoders=None,\n",
    "    all_baskets=None,\n",
    "    train_data=None\n",
    "):\n",
    "    # Identify candidate baskets not already purchased\n",
    "    invested_baskets = set(train_data[train_data['user_id'] == user_id]['basket_name'])\n",
    "    candidate_baskets = [b for b in all_baskets if b not in invested_baskets]\n",
    "\n",
    "    # Predict content-based scores\n",
    "    cb_scores = {\n",
    "        basket: predict_rating(user_id, basket, user_profiles_df, basket_features_indexed)\n",
    "        for basket in candidate_baskets\n",
    "    }\n",
    "    cb_scores = normalize_scores(cb_scores)\n",
    "\n",
    "    # Predict deep learning scores\n",
    "    dl_predictions = predict_user_baskets(user_id, final_model, exclude_baskets=invested_baskets)\n",
    "    dl_scores = {\n",
    "        basket_encoder.inverse_transform([idx])[0]: score\n",
    "        for idx, score in dl_predictions if basket_encoder.inverse_transform([idx])[0] in candidate_baskets\n",
    "    }\n",
    "    dl_scores = normalize_scores(dl_scores)\n",
    "\n",
    "    # Weighted score fusion\n",
    "    combined_scores = {}\n",
    "    for basket in candidate_baskets:\n",
    "        cb = cb_scores.get(basket, 0)\n",
    "        dl = dl_scores.get(basket, 0)\n",
    "        combined_scores[basket] = weights[0]*cb + weights[1]*dl\n",
    "\n",
    "    return sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "def evaluate_hybrid_model(\n",
    "    test_user_ids,\n",
    "    test_data,\n",
    "    train_data,\n",
    "    basket_encoder,\n",
    "    basket_features_indexed,\n",
    "    user_profiles_df,\n",
    "    final_model,\n",
    "    encoders,\n",
    "    all_baskets,\n",
    "    top_k_values=[1, 2, 3],\n",
    "    weights=(0.5, 0.5)\n",
    "):\n",
    "    precision_at_k = collections.defaultdict(list)\n",
    "    recall_at_k = collections.defaultdict(list)\n",
    "    f1_at_k = collections.defaultdict(list)\n",
    "\n",
    "    for user_id in test_user_ids:\n",
    "        true_baskets = set(test_data[test_data['user_id'] == user_id]['basket_name'])\n",
    "        if not true_baskets:\n",
    "            continue\n",
    "        top_k_recs = get_hybrid_recommendations(\n",
    "            user_id=user_id,\n",
    "            top_k=max(top_k_values),\n",
    "            weights=weights,\n",
    "            basket_encoder=basket_encoder,\n",
    "            basket_features_indexed=basket_features_indexed,\n",
    "            user_profiles_df=user_profiles_df,\n",
    "            final_model=final_model,\n",
    "            encoders=encoders,\n",
    "            all_baskets=all_baskets,\n",
    "            train_data=train_data\n",
    "        )\n",
    "        rec_baskets = [basket for basket, _ in top_k_recs]\n",
    "        for k in top_k_values:\n",
    "            top_k = rec_baskets[:k]\n",
    "            tp = len(set(top_k) & true_baskets)\n",
    "            precision = tp / k\n",
    "            recall = tp / len(true_baskets)\n",
    "            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "            precision_at_k[k].append(precision)\n",
    "            recall_at_k[k].append(recall)\n",
    "            f1_at_k[k].append(f1)\n",
    "\n",
    "    print(\"\\n Hybrid Recommender Evaluation Metrics (CB + DL):\")\n",
    "    results = {}\n",
    "    for k in top_k_values:\n",
    "        avg_p = np.mean(precision_at_k[k]) if precision_at_k[k] else 0\n",
    "        avg_r = np.mean(recall_at_k[k]) if recall_at_k[k] else 0\n",
    "        avg_f1 = np.mean(f1_at_k[k]) if f1_at_k[k] else 0\n",
    "        print(f\"\\n‚û°Ô∏è Top-{k} metrics:\")\n",
    "        print(f\"Precision@{k}: {avg_p:.4f}\")\n",
    "        print(f\"Recall@{k}:    {avg_r:.4f}\")\n",
    "        print(f\"F1@{k}:        {avg_f1:.4f}\")\n",
    "        print(f\"Users evaluated: {len(precision_at_k[k])}\")\n",
    "        results[k] = {\"precision\": avg_p, \"recall\": avg_r, \"f1\": avg_f1}\n",
    "    return results\n",
    "\n",
    "# --- Run Hybrid Recommender for a single user ---\n",
    "top_recs = get_hybrid_recommendations(\n",
    "    user_id=1001,\n",
    "    top_k=5,\n",
    "    weights=(0.3, 0.7),  # More weight to DL\n",
    "    basket_encoder=basket_encoder,\n",
    "    basket_features_indexed=basket_features_indexed,\n",
    "    user_profiles_df=user_profiles_df,\n",
    "    final_model=final_model,\n",
    "    encoders=encoders,\n",
    "    all_baskets=unique_baskets,\n",
    "    train_data=train_data\n",
    ")\n",
    "\n",
    "for basket, score in top_recs:\n",
    "    print(f\"{basket}: {score:.4f}\")\n",
    "\n",
    "# --- Run Evaluation on all users ---\n",
    "results = evaluate_hybrid_model(\n",
    "    test_user_ids=test_data['user_id'].unique(),\n",
    "    test_data=test_data,\n",
    "    train_data=train_data,\n",
    "    basket_encoder=basket_encoder,\n",
    "    basket_features_indexed=basket_features_indexed,\n",
    "    user_profiles_df=user_profiles_df,\n",
    "    final_model=final_model,\n",
    "    encoders=encoders,\n",
    "    all_baskets=unique_baskets,\n",
    "    top_k_values=[1, 2, 3],\n",
    "    weights=(0.3, 0.7)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38ef0e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def grid_search_weights(\n",
    "    test_user_ids,\n",
    "    test_data,\n",
    "    train_data,\n",
    "    basket_encoder,\n",
    "    basket_features_indexed,\n",
    "    user_profiles_df,\n",
    "    final_model,\n",
    "    encoders,\n",
    "    all_baskets,\n",
    "    top_k=3,  # Evaluate at k=3 by default\n",
    "    step=0.1\n",
    "):\n",
    "    best_f1 = 0\n",
    "    best_weights = None\n",
    "    results_log = []\n",
    "\n",
    "    weight_range = np.arange(0.0, 1.01, step)  # [0.0, 0.1, ..., 1.0]\n",
    "\n",
    "    print(\"Starting grid search over hybrid weights...\\n\")\n",
    "\n",
    "    for cb_weight in weight_range:\n",
    "        dl_weight = 1.0 - cb_weight\n",
    "        weights = (cb_weight, dl_weight)\n",
    "\n",
    "        print(f\"Evaluating weights: CB={cb_weight:.1f}, DL={dl_weight:.1f}\")\n",
    "\n",
    "        result = evaluate_hybrid_model(\n",
    "            test_user_ids=test_user_ids,\n",
    "            test_data=test_data,\n",
    "            train_data=train_data,\n",
    "            basket_encoder=basket_encoder,\n",
    "            basket_features_indexed=basket_features_indexed,\n",
    "            user_profiles_df=user_profiles_df,\n",
    "            final_model=final_model,\n",
    "            encoders=encoders,\n",
    "            all_baskets=all_baskets,\n",
    "            top_k_values=[top_k],\n",
    "            weights=weights\n",
    "        )\n",
    "\n",
    "        f1 = result[top_k]['f1']\n",
    "        results_log.append((cb_weight, dl_weight, f1))\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_weights = weights\n",
    "\n",
    "    print(\"\\nGrid Search Complete\")\n",
    "    print(f\"Best Weights: CB={best_weights[0]:.2f}, DL={best_weights[1]:.2f} ‚Üí F1@{top_k} = {best_f1:.4f}\")\n",
    "\n",
    "    return best_weights, results_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63476ba8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting grid search over hybrid weights...\n",
      "\n",
      "Evaluating weights: CB=0.0, DL=1.0\n",
      "\n",
      "üìä Hybrid Recommender Evaluation Metrics (CB + DL):\n",
      "\n",
      "‚û°Ô∏è Top-1 metrics:\n",
      "Precision@1: 0.2243\n",
      "Recall@1:    0.1665\n",
      "F1@1:        0.1858\n",
      "Users evaluated: 994\n",
      "Evaluating weights: CB=0.1, DL=0.9\n"
     ]
    }
   ],
   "source": [
    "best_weights, logs = grid_search_weights(\n",
    "    test_user_ids=test_data['user_id'].unique(),\n",
    "    test_data=test_data,\n",
    "    train_data=train_data,\n",
    "    basket_encoder=basket_encoder,\n",
    "    basket_features_indexed=basket_features_indexed,\n",
    "    user_profiles_df=user_profiles_df,\n",
    "    final_model=final_model,\n",
    "    encoders=encoders,\n",
    "    all_baskets=unique_baskets,\n",
    "    top_k=1,     # test k=1 ... or k=5\n",
    "    step=0.1 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0edca-7545-40bc-8281-2974b8474f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
